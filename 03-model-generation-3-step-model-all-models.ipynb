{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661eaff7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T08:53:00.325729Z",
     "start_time": "2021-07-12T08:53:00.267621Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Enable autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489a5f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost tqdm shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64d3939",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T08:53:12.097558Z",
     "start_time": "2021-07-12T08:53:05.439047Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, TimeSeriesSplit, cross_validate\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from scipy.special import logit, expit\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, mean_squared_error\n",
    "import itertools\n",
    "import shap\n",
    "shap.initjs()\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "pd.set_option('max_columns',None)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73461b3",
   "metadata": {},
   "source": [
    "# Import pickle file with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78dce65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T08:56:29.006009Z",
     "start_time": "2021-07-12T08:53:12.100117Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./data/export_features_2016_2020.pkl.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34f5f36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T08:56:30.051396Z",
     "start_time": "2021-07-12T08:56:29.009001Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5dff4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T09:11:53.255249Z",
     "start_time": "2021-07-12T09:11:49.627684Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_orig = df.copy() # save all data for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0b7f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_orig # get all data back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c568f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = ['location_count', 'location_mean', 'location_std',\n",
    " 'location_min', 'location_5%', 'location_10%', 'location_15%',\n",
    " 'location_20%', 'location_25%', 'location_30%', 'location_35%',\n",
    " 'location_40%', 'location_45%', 'location_50%', 'location_55%',\n",
    " 'location_60%', 'location_65%', 'location_70%', 'location_75%',\n",
    " 'location_80%', 'location_85%', 'location_90%', 'location_95%',\n",
    " 'location_max', \n",
    "                \n",
    " 'band_count', 'band_mean', 'band_std',\n",
    " 'band_min', 'band_5%', 'band_10%', 'band_15%', 'band_20%',\n",
    " 'band_25%', 'band_30%', 'band_35%', 'band_40%', 'band_45%',\n",
    " 'band_50%', 'band_55%', 'band_60%', 'band_65%', 'band_70%',\n",
    " 'band_75%', 'band_80%', 'band_85%', 'band_90%', 'band_95%',\n",
    " 'band_max', \n",
    "                \n",
    " 'promoter_count', 'promoter_mean', 'promoter_std',\n",
    " 'promoter_min', 'promoter_5%', 'promoter_10%', 'promoter_15%',\n",
    " 'promoter_20%', 'promoter_25%', 'promoter_30%', 'promoter_35%',\n",
    " 'promoter_40%', 'promoter_45%', 'promoter_50%', 'promoter_55%',\n",
    " 'promoter_60%', 'promoter_65%', 'promoter_70%', 'promoter_75%',\n",
    " 'promoter_80%', 'promoter_85%', 'promoter_90%', 'promoter_95%',\n",
    " 'promoter_max', \n",
    " \n",
    " 'vg_datum_year', 'vg_datum_month', 'vg_datum_day_of_week',\n",
    "                \n",
    " 'location_kirche',  'location_hotel', 'location_theater', 'location_cafe',\n",
    " 'location_stadthalle', 'location_buergerhaus', 'location_club', 'location_gaststaette',\n",
    " 'location_halle', 'location_festhalle', 'location_kurhaus', 'location_schloss',\n",
    " 'location_restaurant', 'location_kulturzentrum', 'location_festzelt', 'location_musikschule',\n",
    " 'location_mehrzweckhalle', 'location_pub', 'location_bar', 'location_gasthaus', 'location_turnhalle',\n",
    " 'location_kulturhaus', 'location_gymnasium', 'location_rathaus', 'location_gasthof',\n",
    " 'location_park', 'location_kabarett', 'location_schuetzenhalle', 'location_gemeindehalle',\n",
    " 'location_gemeindehaus', \n",
    "                \n",
    " 'band_musikverein', 'band_band', 'band_mv', 'band_duo', 'band_trio', 'band_musikkapelle',\n",
    " 'band_chor', 'band_blaskapelle', 'band_orchester', 'band_stadtkapelle', 'band_gbr',\n",
    " 'band_jazz', 'band_kurorchester', 'band_amp', 'band_ensemble', 'band_blasorchester',\n",
    " 'band_partyband', 'band_friends', 'band_blues', 'band_original', 'band_live',\n",
    " 'band_swing', 'band_musikzug', 'band_solo', 'band_mgv', 'band_jugendkapelle',\n",
    " 'band_sound', 'band_harmonie', 'band_black', 'band_ev']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a150797b",
   "metadata": {},
   "source": [
    "# Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca0bd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.amount_segment==3].amount.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a458ed9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['amount_segment']).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43921f43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-25T11:24:30.980377Z",
     "start_time": "2021-05-25T11:24:14.267057Z"
    }
   },
   "outputs": [],
   "source": [
    "df.groupby(['amount_segment']).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec0ad02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-25T11:33:12.922961Z",
     "start_time": "2021-05-25T11:32:52.846249Z"
    }
   },
   "outputs": [],
   "source": [
    "df.groupby(['amount_segment']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58ba073",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-25T11:40:47.487554Z",
     "start_time": "2021-05-25T11:40:47.286965Z"
    }
   },
   "outputs": [],
   "source": [
    "df[(df.amount>150) &(df.amount_segment==4)].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4d5528",
   "metadata": {},
   "source": [
    "# Get sample of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83dfccd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T09:12:08.076676Z",
     "start_time": "2021-07-12T09:11:59.509874Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get a sample of the DataFrame \n",
    "splitSample = StratifiedShuffleSplit(n_splits=1, test_size=0.01, random_state=42)\n",
    "\n",
    "for train_idx, test_idx in splitSample.split(df[all_features], df.amount_segment):\n",
    "    df_sample=df.iloc[test_idx]\n",
    "    \n",
    "    plt.figure()\n",
    "    df.amount_segment.astype(int).value_counts().sort_index().plot.bar(color='r')\n",
    "    df_sample.amount_segment.astype(int).value_counts().sort_index().plot.bar(color='g')\n",
    "\n",
    "    plt.title('Inkasso-Segment')\n",
    "    plt.legend(['Full DF', 'Sample DF'])\n",
    "    plt.show()\n",
    "\n",
    "df=df_sample.copy()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0476bc8",
   "metadata": {},
   "source": [
    "# Define Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2ab3df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T09:12:08.257608Z",
     "start_time": "2021-07-12T09:12:08.080525Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_groups = ['location', 'band', 'promoter']\n",
    "\n",
    "feature_group_combinations = []\n",
    "for i in range(1, len(feature_groups) + 1):\n",
    "    comb = itertools.combinations(feature_groups, i)\n",
    "    feature_group_combinations += list(comb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526a5764",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T09:12:08.476767Z",
     "start_time": "2021-07-12T09:12:08.262518Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features={}\n",
    "\n",
    "features['location'] = ['location_count', 'location_mean', 'location_std',\n",
    " 'location_min', 'location_5%', 'location_10%', 'location_15%',\n",
    " 'location_20%', 'location_25%', 'location_30%', 'location_35%',\n",
    " 'location_40%', 'location_45%', 'location_50%', 'location_55%',\n",
    " 'location_60%', 'location_65%', 'location_70%', 'location_75%',\n",
    " 'location_80%', 'location_85%', 'location_90%', 'location_95%',\n",
    " 'location_max', 'location_kirche',  'location_hotel', 'location_theater', 'location_cafe',\n",
    " 'location_stadthalle', 'location_buergerhaus', 'location_club', 'location_gaststaette',\n",
    " 'location_halle', 'location_festhalle', 'location_kurhaus', 'location_schloss',\n",
    " 'location_restaurant', 'location_kulturzentrum', 'location_festzelt', 'location_musikschule',\n",
    " 'location_mehrzweckhalle', 'location_pub', 'location_bar', 'location_gasthaus', 'location_turnhalle',\n",
    " 'location_kulturhaus', 'location_gymnasium', 'location_rathaus', 'location_gasthof',\n",
    " 'location_park', 'location_kabarett', 'location_schuetzenhalle', 'location_gemeindehalle',\n",
    " 'location_gemeindehaus']\n",
    "\n",
    "features['band'] = ['band_count', 'band_mean', 'band_std',\n",
    " 'band_min', 'band_5%', 'band_10%', 'band_15%', 'band_20%',\n",
    " 'band_25%', 'band_30%', 'band_35%', 'band_40%', 'band_45%',\n",
    " 'band_50%', 'band_55%', 'band_60%', 'band_65%', 'band_70%',\n",
    " 'band_75%', 'band_80%', 'band_85%', 'band_90%', 'band_95%',\n",
    " 'band_max','band_musikverein', 'band_band', 'band_mv', 'band_duo', 'band_trio', 'band_musikkapelle',\n",
    " 'band_chor', 'band_blaskapelle', 'band_orchester', 'band_stadtkapelle', 'band_gbr',\n",
    " 'band_jazz', 'band_kurorchester', 'band_amp', 'band_ensemble', 'band_blasorchester',\n",
    " 'band_partyband', 'band_friends', 'band_blues', 'band_original', 'band_live',\n",
    " 'band_swing', 'band_musikzug', 'band_solo', 'band_mgv', 'band_jugendkapelle',\n",
    " 'band_sound', 'band_harmonie', 'band_black', 'band_ev']\n",
    "\n",
    "features['promoter']=['promoter_count', 'promoter_mean', 'promoter_std',\n",
    " 'promoter_min', 'promoter_5%', 'promoter_10%', 'promoter_15%',\n",
    " 'promoter_20%', 'promoter_25%', 'promoter_30%', 'promoter_35%',\n",
    " 'promoter_40%', 'promoter_45%', 'promoter_50%', 'promoter_55%',\n",
    " 'promoter_60%', 'promoter_65%', 'promoter_70%', 'promoter_75%',\n",
    " 'promoter_80%', 'promoter_85%', 'promoter_90%', 'promoter_95%',\n",
    " 'promoter_max']\n",
    "    \n",
    "features['date']=['vg_datum_year', 'vg_datum_month', 'vg_datum_day_of_week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d98d93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T09:12:08.693245Z",
     "start_time": "2021-07-12T09:12:08.480112Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_features={}\n",
    "for feature_group_combination in feature_group_combinations:\n",
    "    model_name=\"_\".join(feature_group_combination)\n",
    "    model_features[model_name] = features['date']\n",
    "    for feature_group in feature_groups:\n",
    "        if feature_group in feature_group_combination:\n",
    "            model_features[model_name]=model_features[model_name]+features[feature_group]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b94dc4d",
   "metadata": {},
   "source": [
    "# Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808729e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T09:12:09.142820Z",
     "start_time": "2021-07-12T09:12:08.959996Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf1_models = {}\n",
    "clf2_models = {}\n",
    "reg2_models = {}\n",
    "reg3_models = {}\n",
    "reg2_logit_models = {}\n",
    "reg3_logit_models = {}\n",
    "\n",
    "for feature_group_combination in feature_group_combinations:\n",
    "\n",
    "    model_name=\"_\".join(feature_group_combination)\n",
    "    \n",
    "    clf1_models[model_name]=xgb.XGBClassifier(n_estimators=1100, max_depth=9, use_label_encoder=False, objective='binary:logistic',eval_metric = 'error')\n",
    "    clf1_models[model_name].set_params(tree_method = 'gpu_hist')\n",
    "    clf2_models[model_name]=xgb.XGBClassifier(n_estimators=1100, max_depth=9, use_label_encoder=False, objective='binary:logistic',eval_metric = 'error')\n",
    "    clf2_models[model_name].set_params(tree_method = 'gpu_hist')\n",
    "    reg2_models[model_name]=xgb.XGBRegressor(n_estimators=700, max_depth=7, min_child_weight=5, objective='reg:squarederror')\n",
    "    reg2_models[model_name].set_params(tree_method = 'gpu_hist')\n",
    "    reg2_logit_models[model_name]=xgb.XGBRegressor(n_estimators=700, max_depth=7, min_child_weight=5, objective='reg:squarederror')\n",
    "    reg2_logit_models[model_name].set_params(tree_method = 'gpu_hist')\n",
    "    reg3_models[model_name]=xgb.XGBRegressor(n_estimators=700, max_depth=7, min_child_weight=5, objective='reg:squarederror')\n",
    "    reg3_models[model_name].set_params(tree_method = 'gpu_hist')\n",
    "    reg3_logit_models[model_name]=xgb.XGBRegressor(n_estimators=700, max_depth=7, min_child_weight=5, objective='reg:squarederror')\n",
    "    reg3_logit_models[model_name].set_params(tree_method = 'gpu_hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042cc63e",
   "metadata": {},
   "source": [
    "# Prepare DataFrame for Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53989a28",
   "metadata": {},
   "source": [
    "We keep only features + 'amount' and 'amount_segment' columns.\n",
    "\n",
    "Encode segments for Classifier 1:\n",
    "* Variable y_clf1 \n",
    "* Positive class (seg 4+) y_clf1=1\n",
    "* Negative class (seg 2 or 3) y_clf1=0\n",
    "\n",
    "Encode segments for Classifier 2: \n",
    "* Variable y_clf2\n",
    "* Positive class (seg 3) y_clf2=1\n",
    "* Negative class (seg 2) y_clf2=0\n",
    "* Segments >3 are \"encoded\" as np.NaN. These NaN values will be dropped before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3c4cbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T09:17:31.680845Z",
     "start_time": "2021-07-12T09:17:31.477683Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[:,'y_clf1']=(df.amount_segment.astype(int) > 3).values.astype(int)\n",
    "df.loc[:,'y_clf2']=df.amount_segment.apply(lambda x: 1 if x==3 else (0 if x==2 else np.nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b185cf",
   "metadata": {},
   "source": [
    "# Train and Test Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316b12a6",
   "metadata": {},
   "source": [
    "In oder to have larger train and test sets, we use crossvalidation-like approach to verify the model performance.\n",
    "1. We split the dataset into 5 folds with StratifiedKFold. The criteria for split is to have the same proportion of data based on the segment in each fold.\n",
    "2. In each fold we train and test both classifiers independently from each other\n",
    "3. We save the predict_proba results from both classifiers\n",
    "4. Train and test iteration numbers are also saved (in case we want to evaluate the results based on the train/test iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af6d425",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T20:04:44.758263Z",
     "start_time": "2021-05-16T19:23:17.650986Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_model_eval = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "df_dict = {}\n",
    "\n",
    "for feature_group_combination in feature_group_combinations:\n",
    "    model_name=\"_\".join(feature_group_combination)\n",
    "    print('Crossvalidate models for {}'.format(model_name))\n",
    "    \n",
    "    df_dict[model_name]=df[model_features[model_name]+['amount','amount_segment','y_clf1','y_clf2']].dropna(subset=model_features[model_name]).copy()\n",
    "    \n",
    "    df_dict[model_name]['y_pred_proba_clf1']=np.NaN\n",
    "    df_dict[model_name]['y_pred_proba_clf2']=np.NaN\n",
    "    df_dict[model_name][\"y_pred_reg2\"]=np.NaN\n",
    "    df_dict[model_name][\"y_pred_reg2_logit\"]=np.NaN\n",
    "    df_dict[model_name][\"y_pred_reg3\"]=np.NaN\n",
    "    df_dict[model_name][\"y_pred_reg3_logit\"]=np.NaN\n",
    "\n",
    "    df_dict[model_name]['train_iter']=np.NaN\n",
    "    df_dict[model_name]['test_iter']=np.NaN\n",
    "\n",
    "    iter_nr=0\n",
    "\n",
    "    for train_idx, test_idx in tqdm(cv_model_eval.split(df_dict[model_name][model_features[model_name]], df_dict[model_name].amount_segment), total=cv_model_eval.n_splits):\n",
    "        print('Test iteration {}'.format(iter_nr))\n",
    "        \n",
    "        df_train=df_dict[model_name].iloc[train_idx]\n",
    "        df_test=df_dict[model_name].iloc[test_idx]\n",
    "\n",
    "        #Test dataset is the same for all models\n",
    "        X_test = df_test[model_features[model_name]]\n",
    "\n",
    "        X_train_clf1 = df_train[model_features[model_name]]\n",
    "        y_train_clf1 = df_train.y_clf1\n",
    "        y_test_clf1 = df_test.y_clf1\n",
    "\n",
    "        # For Classifier2 (clf2) we use data from segments 2 and 3 (so we drop records where y_clf2 is NaN)\n",
    "        df_train_clf2 = df_train.dropna(subset=['y_clf2'])\n",
    "\n",
    "        X_train_clf2 = df_train_clf2[model_features[model_name]]\n",
    "        y_train_clf2 = df_train_clf2.y_clf2\n",
    "\n",
    "\n",
    "        # For Regression seg2 use only Segment 2 and amount 0.00001>=amount>=49.9999 (because of logit)\n",
    "        df_train_reg2 = df_train[(df_train['amount_segment']==2) & \n",
    "                                 (df_train['amount']>=0.00001) & \n",
    "                                 (df_train['amount']<=49.9999)]\n",
    "\n",
    "        X_train_reg2 = df_train_reg2[model_features[model_name]]\n",
    "        y_train_reg2 = df_train_reg2.amount\n",
    "        y_train_reg2_logit = (y_train_reg2/50).apply(logit)\n",
    "\n",
    "        \n",
    "        df_test_reg2 = df_test[(df_test['amount_segment']==2) & \n",
    "                                 (df_test['amount']>=0.00001) & \n",
    "                                 (df_test['amount']<=49.9999)]\n",
    "\n",
    "        X_test_reg2 = df_test_reg2[model_features[model_name]]\n",
    "        y_test_reg2 = df_test_reg2.amount\n",
    "        y_test_reg2_logit = (y_test_reg2/50).apply(logit)\n",
    "\n",
    "\n",
    "        # For Regression seg3 use only Segment 3 and amount 50>amount>=99.9999 (because of logit)\n",
    "        df_train_reg3 = df_train[(df_train['amount_segment']==3) & \n",
    "                                 (df_train['amount']>50) & \n",
    "                                 (df_train['amount']<=99.9999)]\n",
    "\n",
    "        X_train_reg3 = df_train_reg3[model_features[model_name]]\n",
    "        y_train_reg3 = df_train_reg3.amount\n",
    "        y_train_reg3_logit = ((y_train_reg3-50)/50).apply(logit)\n",
    "\n",
    "        \n",
    "        df_test_reg3 = df_test[(df_test['amount_segment']==3) & \n",
    "                                 (df_test['amount']>50) & \n",
    "                                 (df_test['amount']<=99.9999)]\n",
    "\n",
    "        X_test_reg3 = df_test_reg3[model_features[model_name]]\n",
    "        y_test_reg3 = df_test_reg3.amount\n",
    "        y_test_reg3_logit = ((y_test_reg3-50)/50).apply(logit)\n",
    "\n",
    "\n",
    "\n",
    "        #Fit and test the models\n",
    "\n",
    "        #Classifier 1\n",
    "        clf1_models[model_name].fit(X_train_clf1, y_train_clf1)\n",
    "        y_pred_proba_clf1 = clf1_models[model_name].predict_proba(X_test)[:, 1]\n",
    "        print(\"CLF1 Train Score: {}\".format(clf1_models[model_name].score(X_train_clf1, y_train_clf1)))\n",
    "        print(\"CLF1 Test Score: {}\".format(clf1_models[model_name].score(X_test, y_test_clf1)))\n",
    "\n",
    "        #Classifier 2        \n",
    "        clf2_models[model_name].fit(X_train_clf2, y_train_clf2)\n",
    "        y_pred_proba_clf2 = clf2_models[model_name].predict_proba(X_test)[:, 1]\n",
    "        print(\"CLF2 Train Score: {}\".format(clf2_models[model_name].score(X_train_clf2, y_train_clf2)))\n",
    "        print(\"CLF2 Test Score: {}\".format(clf2_models[model_name].score(df_test.dropna(subset=['y_clf2'])[model_features[model_name]], \n",
    "                                                      df_test.dropna(subset=['y_clf2']).y_clf2)))\n",
    " \n",
    "\n",
    "        #Regression Segment 2\n",
    "        reg2_models[model_name].fit(X_train_reg2, y_train_reg2)\n",
    "        y_pred_reg2=reg2_models[model_name].predict(X_test)\n",
    "        print(\"REG2 Train Score: {}\".format(reg2_models[model_name].score(X_train_reg2, y_train_reg2)))\n",
    "        print(\"REG2 Test Score: {}\".format(reg2_models[model_name].score(X_test_reg2, y_test_reg2)))\n",
    "\n",
    "\n",
    "        #Regression Segment 2 with logit transformation\n",
    "        reg2_logit_models[model_name].fit(X_train_reg2, y_train_reg2_logit)\n",
    "        y_pred_reg2_logit = reg2_logit_models[model_name].predict(X_test)\n",
    "        \n",
    "        y_pred_reg2_logit_transf = pd.Series(y_pred_reg2_logit).apply(expit)*50\n",
    "        \n",
    "        print(\"REG2_Logit Train Score: {}\".format(reg2_logit_models[model_name].score(X_train_reg2, y_train_reg2_logit)))\n",
    "        print(\"REG2_Logit Test Score: {}\".format(reg2_logit_models[model_name].score(X_test_reg2, y_test_reg2_logit)))\n",
    "\n",
    "        #Regression Segment 3\n",
    "        reg3_models[model_name].fit(X_train_reg3, y_train_reg3)\n",
    "        y_pred_reg3=reg3_models[model_name].predict(X_test)\n",
    "        print(\"REG3 Train Score: {}\".format(reg3_models[model_name].score(X_train_reg3, y_train_reg3)))\n",
    "        print(\"REG3 Test Score: {}\".format(reg3_models[model_name].score(X_test_reg3, y_test_reg3)))\n",
    "\n",
    "        #Regression Segment 3 with logit transformation\n",
    "        reg3_logit_models[model_name].fit(X_train_reg3, y_train_reg3_logit)\n",
    "        y_pred_reg3_logit = reg3_logit_models[model_name].predict(X_test)\n",
    "        y_pred_reg3_logit_transf = pd.Series(y_pred_reg3_logit).apply(expit)*50+50\n",
    "        print(\"REG3_Logit Train Score: {}\".format(reg3_logit_models[model_name].score(X_train_reg3, y_train_reg3_logit)))\n",
    "        print(\"REG3_Logit Test Score: {}\".format(reg3_logit_models[model_name].score(X_test_reg3, y_test_reg3_logit)))\n",
    "\n",
    "        #Save the prediction results in separate columns\n",
    "        df_dict[model_name].iloc[test_idx,df_dict[model_name].columns.get_loc(\"y_pred_proba_clf1\")]=y_pred_proba_clf1\n",
    "        df_dict[model_name].iloc[test_idx,df_dict[model_name].columns.get_loc(\"y_pred_proba_clf2\")]=y_pred_proba_clf2\n",
    "        df_dict[model_name]['y_pred_proba_clf1']=df_dict[model_name]['y_pred_proba_clf1'].apply(lambda x: format(float(x),\".8f\")).astype(float)\n",
    "        df_dict[model_name]['y_pred_proba_clf2']=df_dict[model_name]['y_pred_proba_clf2'].apply(lambda x: format(float(x),\".8f\")).astype(float)\n",
    "        df_dict[model_name].iloc[test_idx,df_dict[model_name].columns.get_loc(\"y_pred_reg2\")]=y_pred_reg2\n",
    "        df_dict[model_name].iloc[test_idx,df_dict[model_name].columns.get_loc(\"y_pred_reg2_logit\")]=y_pred_reg2_logit_transf.values\n",
    "        df_dict[model_name].iloc[test_idx,df_dict[model_name].columns.get_loc(\"y_pred_reg3\")]=y_pred_reg3\n",
    "        df_dict[model_name].iloc[test_idx,df_dict[model_name].columns.get_loc(\"y_pred_reg3_logit\")]=y_pred_reg3_logit_transf.values\n",
    "\n",
    "        #Save train and test iteration number, in case we need it later \n",
    "        #(not sure if one record can be multiple times in train/test, anyway we save only the last iteration number for now...)\n",
    "        df_dict[model_name].iloc[train_idx,df_dict[model_name].columns.get_loc(\"train_iter\")]=iter_nr\n",
    "        df_dict[model_name].iloc[test_idx,df_dict[model_name].columns.get_loc(\"test_iter\")]=iter_nr\n",
    "\n",
    "        iter_nr=iter_nr+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f42263",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export dataframes with predictions for later\n",
    "for feature_group_combination in feature_group_combinations:\n",
    "    model_name=\"_\".join(feature_group_combination)\n",
    "    \n",
    "    df_dict[model_name].to_pickle('./predictions_df/export_predictions_'+model_name, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820d0ea6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T08:22:00.015958Z",
     "start_time": "2021-06-30T08:21:32.607253Z"
    }
   },
   "outputs": [],
   "source": [
    "df_dict = {}\n",
    "\n",
    "#Read dataframes with predictions\n",
    "for feature_group_combination in feature_group_combinations:\n",
    "    model_name=\"_\".join(feature_group_combination)\n",
    "    \n",
    "    df_dict[model_name]=pd.read_pickle('./predictions_df/export_predictions_'+model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393e2dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict['location']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eece5aa7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T09:21:39.848295Z",
     "start_time": "2021-05-04T09:21:39.138359Z"
    }
   },
   "source": [
    "# Threshold optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd659140",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T13:56:51.243413Z",
     "start_time": "2021-05-16T13:56:51.083283Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_treasholds = TimeSeriesSplit(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abef8fc",
   "metadata": {},
   "source": [
    "## Threshold optimization (clf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eff226b",
   "metadata": {},
   "source": [
    "We optimize only t_neg threshold since for clf1 only positive class is of importance. We want to prevent classification of segments 4+ as segments 2 or 3 since this will mean \"lost money\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449d4618",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:54:29.680580Z",
     "start_time": "2021-05-16T13:58:16.996932Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_for_clf1_list = [0.01, 0.02, 0.03, 0.04, 0.05, 0.07, 0.09]\n",
    "opt_t_neg_list_clf1 = {}\n",
    "classifier_thresholds_clf1 = {}\n",
    "\n",
    "df_dict={}\n",
    "\n",
    "for feature_group_combination in feature_group_combinations:\n",
    "    model_name=\"_\".join(feature_group_combination)\n",
    "    print('Optimize threshold for CLF1 for {}'.format(model_name))\n",
    "\n",
    "    opt_t_neg_list_clf1[model_name] = pd.DataFrame()\n",
    "    \n",
    "    df_dict[model_name]=df[model_features[model_name]+['amount','amount_segment','y_clf1','y_clf2']].dropna(subset=model_features[model_name]).copy()\n",
    "\n",
    "\n",
    "    #Use all features, encode segment 4+ as positive class, otherwise negative\n",
    "    X_train_clf1 = df_dict[model_name][model_features[model_name]].dropna()\n",
    "    y_train_clf1= df_dict[model_name].dropna(subset=model_features[model_name]).y_clf1\n",
    "\n",
    "    for i in tqdm(range(5)):\n",
    "        # we use all data for threshold optimization\n",
    "        for train_idx, test_idx in tqdm(cv_treasholds.split(X_train_clf1, y_train_clf1), total=cv_treasholds.n_splits) : \n",
    "\n",
    "            clf1_models[model_name].fit(X_train_clf1.iloc[train_idx], y_train_clf1.iloc[train_idx])\n",
    "\n",
    "            y_test_pred_proba_clf1 = clf1_models[model_name].predict_proba(X_train_clf1.iloc[test_idx])[:, 1]\n",
    "            y_test_clf1 = y_train_clf1.iloc[test_idx]\n",
    "\n",
    "            opt_t_neg_clf1 = 0\n",
    "            opt_fnr_clf1 = 0\n",
    "\n",
    "            for max_for_clf1 in max_for_clf1_list :\n",
    "                for t_neg_clf1 in np.linspace(0, 1, 1001):\n",
    "                    for_clf1 = (y_test_pred_proba_clf1[y_test_clf1 == 1] <= t_neg_clf1).sum() / (y_test_pred_proba_clf1 <= t_neg_clf1).sum()\n",
    "                    if for_clf1 > max_for_clf1:\n",
    "                        break\n",
    "                    opt_t_neg_clf1 = t_neg_clf1\n",
    "                    opt_for_clf1 = for_clf1\n",
    "\n",
    "                print(opt_t_neg_clf1, opt_for_clf1, max_for_clf1)\n",
    "                opt_t_neg_list_clf1[model_name] = opt_t_neg_list_clf1[model_name].append({\"max_for_clf1\": max_for_clf1, \"opt_t_neg_clf1\": opt_t_neg_clf1}, ignore_index=True)\n",
    "\n",
    "    for max_for_clf1 in max_for_clf1_list :\n",
    "        print('max_for_clf1 = {}'.format(max_for_clf1))\n",
    "        display(opt_t_neg_list_clf1[model_name][opt_t_neg_list_clf1[model_name].max_for_clf1==max_for_clf1].opt_t_neg_clf1.describe())\n",
    "\n",
    "    classifier_thresholds_clf1[model_name] = pd.DataFrame()\n",
    "    for max_for_clf1 in max_for_clf1_list :\n",
    "        #t_neg = min(opt_t_neg_list)\n",
    "        t_neg_clf1 = np.median(opt_t_neg_list_clf1[model_name][opt_t_neg_list_clf1[model_name].max_for_clf1==max_for_clf1].opt_t_neg_clf1)\n",
    "\n",
    "        classifier_thresholds_clf1[model_name] = classifier_thresholds_clf1[model_name].append({\"max_for_clf1\": max_for_clf1,\n",
    "            \"t_neg_clf1\": t_neg_clf1}, ignore_index=True)\n",
    "\n",
    "    print(\"Classfifier Thresholds for {}\".format(model_name))\n",
    "    display(classifier_thresholds_clf1[model_name])\n",
    "    \n",
    "    #Save Thresholds\n",
    "    classifier_thresholds_clf1[model_name].to_pickle(('./thresholds/export_thresholds_{}_clf1.pkl.bz2').format(model_name), protocol=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be66ccf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T15:07:23.264780Z",
     "start_time": "2021-05-16T15:06:31.309830Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_conf_matrix_clf1(t_neg, model_name):\n",
    "    df_dict[model_name]['y_pred_clf1']=df_dict[model_name]['y_pred_proba_clf1'].apply(lambda x: 1 if x >= t_neg else 0) \n",
    "    df_confusion_clf1 = pd.DataFrame(confusion_matrix(df_dict[model_name].y_clf1, df_dict[model_name].y_pred_clf1), index=['true neg (2,3)', 'true pos (4+)'], columns=['pred. neg (2,3)', 'pred. pos (4+)'])\n",
    "\n",
    "    return df_confusion_clf1\n",
    "\n",
    "\n",
    "for feature_group_combination in feature_group_combinations:\n",
    "    model_name=\"_\".join(feature_group_combination)\n",
    "    print('Plot thresholds for {}'.format(model_name))\n",
    "    \n",
    "    correct_positive=[]\n",
    "    correct_negative=[]\n",
    "    false_positive=[]\n",
    "    false_negative=[]\n",
    "\n",
    "    for v in np.linspace(0, 1, 100):\n",
    "        result = create_conf_matrix_clf1(v, model_name)\n",
    "\n",
    "        correct_positive.append(result.iloc[1,1]/df_dict[model_name].shape[0])\n",
    "        correct_negative.append(result.iloc[0,0]/df_dict[model_name].shape[0])\n",
    "        false_positive.append(result.iloc[0,1]/df_dict[model_name].shape[0])    \n",
    "        false_negative.append(result.iloc[1,0]/df_dict[model_name].shape[0])\n",
    "\n",
    "        #print('Threshold = {}'.format(v))\n",
    "        #print(result)\n",
    "\n",
    "    x=np.linspace(0, 1, 100)\n",
    "    plt.figure(figsize=(20,7))\n",
    "\n",
    "\n",
    "    plt.stackplot(x,correct_positive, correct_negative, false_positive, false_negative, \n",
    "                  labels=['Correct positive', \n",
    "                          'Correct negative',\n",
    "                          'False Positive (Extra work)', \n",
    "                          'False Negative (Lost money)'])\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b25d3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T20:02:41.429791Z",
     "start_time": "2021-05-05T20:02:41.270744Z"
    }
   },
   "source": [
    "## Threshold optimization (clf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0576f4cb",
   "metadata": {},
   "source": [
    "Both segment 2 (negative class) and segment 3 (positive class) are equally important --> we need both t_neg and t_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7b2f13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T15:43:58.837396Z",
     "start_time": "2021-05-16T15:09:45.918187Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define Thresholds (t_neg and t_pos) We want to minimize False Negative and False Positive\n",
    "\n",
    "max_for_clf2_list = [0.01, 0.02, 0.03, 0.04, 0.05, 0.07, 0.09]\n",
    "max_fdr_clf2_list = [0.01, 0.02, 0.03, 0.04, 0.05, 0.07, 0.09]\n",
    "\n",
    "opt_t_neg_list_clf2 = {}\n",
    "opt_t_pos_list_clf2 = {}\n",
    "\n",
    "classifier_thresholds_clf2={}\n",
    "\n",
    "for feature_group_combination in feature_group_combinations:\n",
    "    model_name=\"_\".join(feature_group_combination)\n",
    "    print('Optimize threshold for CLF2 for {}'.format(model_name))\n",
    "\n",
    "    opt_t_neg_list_clf2[model_name] = pd.DataFrame()\n",
    "    opt_t_pos_list_clf2[model_name] = pd.DataFrame()\n",
    "\n",
    "    #Use only data records from seg. 2 and 3. Use all features. \n",
    "    X_train_clf2 = df_dict[model_name][~df_dict[model_name].y_clf2.isnull()][model_features[model_name]].dropna()\n",
    "    #Encode segment 3 as 1 (positive class), seg. 2 as 0 (negative class)\n",
    "    y_train_clf2= df_dict[model_name][~df_dict[model_name].y_clf2.isnull()].dropna(subset=model_features[model_name]).y_clf2\n",
    "\n",
    "    for i in tqdm(range(5)):\n",
    "        for train_idx, test_idx in tqdm(cv_treasholds.split(X_train_clf2, y_train_clf2), total=cv_treasholds.n_splits) : \n",
    "            clf2_models[model_name].fit(X_train_clf2.iloc[train_idx], y_train_clf2.iloc[train_idx])\n",
    "\n",
    "            y_test_pred_proba_clf2 = clf2_models[model_name].predict_proba(X_train_clf2.iloc[test_idx])[:, 1]\n",
    "            y_test_clf2 = y_train_clf2.iloc[test_idx]\n",
    "\n",
    "            for max_for_clf2 in max_for_clf2_list :\n",
    "                opt_t_neg_clf2 = 0\n",
    "                opt_for_clf2 = 0\n",
    "                for t_neg_clf2 in np.linspace(0, 1, 1001):\n",
    "                    for_clf2 = (y_test_pred_proba_clf2[y_test_clf2 == 1] <= t_neg_clf2).sum() / (y_test_pred_proba_clf2 <= t_neg_clf2).sum()\n",
    "                    if for_clf2 > max_for_clf2:\n",
    "                        break\n",
    "                    opt_t_neg_clf2 = t_neg_clf2\n",
    "                    opt_for_clf2 = for_clf2\n",
    "\n",
    "                print(opt_t_neg_clf2, opt_for_clf2, max_for_clf2)\n",
    "                opt_t_neg_list_clf2[model_name] = opt_t_neg_list_clf2[model_name].append({\"max_for_clf2\": max_for_clf2, \"opt_t_neg_clf2\": opt_t_neg_clf2}, ignore_index=True)\n",
    "\n",
    "\n",
    "            for max_fdr_clf2 in max_fdr_clf2_list :\n",
    "                opt_t_pos_clf2 = 0\n",
    "                opt_fdr_clf2 = 0\n",
    "                for t_pos_clf2 in np.linspace(0, 1, 1001):\n",
    "                    fdr_clf2 = (y_test_pred_proba_clf2[y_test_clf2 == 0] >= t_pos_clf2).sum() / (y_test_pred_proba_clf2 >= t_pos_clf2).sum()\n",
    "                    opt_t_pos_clf2 = t_pos_clf2\n",
    "                    opt_fdr_clf2 = fdr_clf2\n",
    "                    if fdr_clf2 <= max_fdr_clf2:\n",
    "                        break\n",
    "                print(opt_t_pos_clf2, opt_fdr_clf2, max_fdr_clf2)\n",
    "                opt_t_pos_list_clf2[model_name] = opt_t_pos_list_clf2[model_name].append({\"max_fdr_clf2\": max_fdr_clf2, \"opt_t_pos_clf2\": opt_t_pos_clf2}, ignore_index=True)\n",
    "\n",
    "    for max_for_clf2 in max_for_clf2_list :\n",
    "        print('max_for_clf2 = {}'.format(max_for_clf2))\n",
    "        display(opt_t_neg_list_clf2[model_name][opt_t_neg_list_clf2[model_name].max_for_clf2==max_for_clf2].opt_t_neg_clf2.describe())\n",
    "\n",
    "    for max_fdr_clf2 in max_fdr_clf2_list :\n",
    "        print('max_fdr_clf2 = {}'.format(max_fdr_clf2))\n",
    "        display(opt_t_pos_list_clf2[model_name][opt_t_pos_list_clf2[model_name].max_fdr_clf2==max_fdr_clf2].opt_t_pos_clf2.describe())\n",
    "\n",
    "    classifier_thresholds_clf2[model_name] = pd.DataFrame()\n",
    "\n",
    "    for max_for_clf2 in max_for_clf2_list :\n",
    "        t_neg_clf2 = np.median(opt_t_neg_list_clf2[model_name][opt_t_neg_list_clf2[model_name].max_for_clf2==max_for_clf2].opt_t_neg_clf2)\n",
    "\n",
    "        classifier_thresholds_clf2[model_name] = classifier_thresholds_clf2[model_name].append({\"max_for_clf2\": max_for_clf2,\n",
    "            \"t_neg_clf2\": t_neg_clf2}, ignore_index=True)\n",
    "\n",
    "\n",
    "    for max_fdr_clf2 in max_fdr_clf2_list :\n",
    "        t_pos_clf2 = np.median(opt_t_pos_list_clf2[model_name][opt_t_pos_list_clf2[model_name].max_fdr_clf2==max_fdr_clf2].opt_t_pos_clf2)\n",
    "\n",
    "        classifier_thresholds_clf2[model_name] = classifier_thresholds_clf2[model_name].append({\"max_fdr_clf2\": max_fdr_clf2,\n",
    "            \"t_pos_clf2\": t_pos_clf2}, ignore_index=True)\n",
    "\n",
    "    display(classifier_thresholds_clf2[model_name])\n",
    "    \n",
    "    #Save Thresholds\n",
    "    classifier_thresholds_clf2[model_name].to_pickle(('./thresholds/export_thresholds_{}_clf2.pkl.bz2').format(model_name), protocol=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8430e7",
   "metadata": {},
   "source": [
    "# Load Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0efb1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T08:22:00.294834Z",
     "start_time": "2021-06-30T08:22:00.015958Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier_thresholds_clf1 = {}\n",
    "classifier_thresholds_clf2 = {}\n",
    "\n",
    "#Read dataframes with predictions\n",
    "for feature_group_combination in feature_group_combinations:\n",
    "    model_name=\"_\".join(feature_group_combination)\n",
    "    \n",
    "    classifier_thresholds_clf1[model_name]=pd.read_pickle(('./thresholds/export_thresholds_{}_clf1.pkl.bz2').format(model_name))\n",
    "    classifier_thresholds_clf2[model_name]=pd.read_pickle(('./thresholds/export_thresholds_{}_clf2.pkl.bz2').format(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2c6a4e",
   "metadata": {},
   "source": [
    "# Create combined confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68b8fc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T09:16:46.762323Z",
     "start_time": "2021-06-30T08:56:27.378412Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_for_clf1 = 0.03\n",
    "max_for_clf2 = 0.05 #False omission rate\n",
    "max_fdr_clf2 = 0.05 #False discovery rate\n",
    "\n",
    "for feature_group_combination in feature_group_combinations:\n",
    "    model_name=\"_\".join(feature_group_combination)\n",
    "    print('Create combined confusion matrix for {}'.format(model_name))\n",
    "\n",
    "    t_neg_clf1=0.32#classifier_thresholds_clf1[model_name][classifier_thresholds_clf1[model_name].max_for_clf1==max_for_clf1].t_neg_clf1.values[0]\n",
    "\n",
    "    t_neg_clf2=0.467#classifier_thresholds_clf2[model_name][classifier_thresholds_clf2[model_name].max_for_clf2==max_for_clf2].t_neg_clf2.values[0]\n",
    "    t_pos_clf2=0.843#classifier_thresholds_clf2[model_name][classifier_thresholds_clf2[model_name].max_fdr_clf2==max_fdr_clf2].t_pos_clf2.values[0]\n",
    "\n",
    "    df_dict[model_name]=df_dict[model_name][df_dict[model_name].test_iter.isnull()!=True]\n",
    "    df_dict[model_name]['y']=df_dict[model_name].amount_segment.apply(lambda x: '2' if x==2 else ('3' if x==3 else '4+'))\n",
    "\n",
    "    df_dict[model_name]['y_pred_clf1']=df_dict[model_name]['y_pred_proba_clf1'].apply(lambda x: '2 or 3' if x <= t_neg_clf1 else '4+')\n",
    "    df_dict[model_name]['y_pred_clf2']=df_dict[model_name]['y_pred_proba_clf2'].apply(lambda x: '2' if x<=t_neg_clf2 else \n",
    "                                                    ('2?' if x<=0.5 else \n",
    "                                                     ('3?' if x<=t_pos_clf2 else '3')))\n",
    "\n",
    "    df_dict[model_name]['y_pred']=df_dict[model_name].apply(lambda x: x['y_pred_clf1'] if x['y_pred_clf1']=='4+' else x['y_pred_clf2'], axis=1)\n",
    "    \n",
    "    df_confusion_comb = pd.DataFrame(confusion_matrix(df_dict[model_name].y, df_dict[model_name].y_pred, \n",
    "                                                  labels=['2','2?','3?','3','4+']), \n",
    "                                 index=['true 2', 'true 2?', 'true 3?', 'true 3', 'true 4+'], \n",
    "                                 columns=['pred 2', 'pred 2?', 'pred 3?', 'pred 3', 'pred 4+'])\n",
    "    print('Abs. Numbers')\n",
    "    display(df_confusion_comb.drop(index=['true 2?', 'true 3?']))\n",
    "    \n",
    "    df_confusion_comb = pd.DataFrame(confusion_matrix(df_dict[model_name].y, df_dict[model_name].y_pred, \n",
    "                                                  labels=['2','2?','3?','3','4+'], normalize=\"true\"), \n",
    "                                index=['true 2', 'true 2?', 'true 3?', 'true 3', 'true 4+'], \n",
    "                                 columns=['pred 2', 'pred 2?', 'pred 3?', 'pred 3', 'pred 4+'])\n",
    "    print('Rel. to true count per class in %')\n",
    "    display((df_confusion_comb.drop(index=['true 2?', 'true 3?']) * 100).round(1))\n",
    "    \n",
    "    print('Classification Report')\n",
    "    print(classification_report(df_dict[model_name].y, df_dict[model_name].y_pred, labels=['2','2?','3?','3','4+'],zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad015256",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T08:42:05.811732Z",
     "start_time": "2021-06-30T08:30:01.433336Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for feature_group_combination in feature_group_combinations:\n",
    "    model_name=\"_\".join(feature_group_combination)\n",
    "    print('Make prediction for {}'.format(model_name))\n",
    "    \n",
    "    df_dict[model_name]['predicted_segment'] = df_dict[model_name].y_pred.map({\n",
    "    '2': '2',\n",
    "    '2?': '2',\n",
    "    '3': '3',\n",
    "    '3?': '3',\n",
    "    '4+': '4+'\n",
    "    })\n",
    "    \n",
    "    df_dict[model_name]['segment']=df_dict[model_name].amount_segment.map({2:'2', 3: '3', \n",
    "                                                                 4: '4+',5: '4+',6: '4+',7: '4+',8: '4+',\n",
    "                                                                9: '4+', 10: '4+',11: '4+',12: '4+'})\n",
    "    \n",
    "    cm = pd.DataFrame(confusion_matrix(df_dict[model_name].segment, df_dict[model_name].predicted_segment, labels=['2', '3', '4+']),\n",
    "                  index=['true 2', 'true 3', 'true 4+'],\n",
    "                  columns=['pred 2', 'pred 3', 'pred 4+'])\n",
    "\n",
    "    display(cm)\n",
    "    \n",
    "    cm_norm = pd.DataFrame(confusion_matrix(df_dict[model_name].segment, df_dict[model_name].predicted_segment, labels=['2', '3', '4+'], normalize='true'),\n",
    "                  index=['true 2', 'true 3', 'true 4+'],\n",
    "                  columns=['pred 2', 'pred 3', 'pred 4+'])\n",
    "\n",
    "    display(cm_norm)\n",
    "    \n",
    "    print(classification_report(df_dict[model_name].segment, df_dict[model_name].predicted_segment, target_names=['2', '3', '4']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6c85e6",
   "metadata": {},
   "source": [
    "# Get scores for regressions (with correct class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a88e1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_group_combination in feature_group_combinations:\n",
    "    model_name=\"_\".join(feature_group_combination)\n",
    "    print(df_dict[model_name].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eba96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict['location'][(df_dict['location'].amount_segment==2)&(df_dict['location'].test_iter>0)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7172938",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T18:55:33.931981Z",
     "start_time": "2021-05-16T18:55:24.703275Z"
    }
   },
   "outputs": [],
   "source": [
    "for feature_group_combination in feature_group_combinations:\n",
    "    model_name=\"_\".join(feature_group_combination)\n",
    "    print('Regression score for {}'.format(model_name))\n",
    "    \n",
    "    df_reg2 = df_dict[model_name][df_dict[model_name].amount_segment==2].copy()\n",
    "    df_reg2['discr'] = df_reg2.amount-df_reg2.y_pred_reg2\n",
    "    df_reg2['discr_abs'] = abs(df_reg2.amount-df_reg2.y_pred_reg2)\n",
    "    df_reg2['discr_logit'] = df_reg2.amount-df_reg2.y_pred_reg2_logit\n",
    "    df_reg2['discr_logit_abs'] = abs(df_reg2.amount-df_reg2.y_pred_reg2_logit)\n",
    "\n",
    "    df_reg3 = df_dict[model_name][df_dict[model_name].amount_segment==3].copy()\n",
    "    df_reg3['discr'] = df_reg3.amount-df_reg3.y_pred_reg3\n",
    "    df_reg3['discr_abs'] = abs(df_reg3.amount-df_reg3.y_pred_reg3)\n",
    "    df_reg3['discr_logit'] = df_reg3.amount-df_reg3.y_pred_reg3_logit\n",
    "    df_reg3['discr_logit_abs'] = abs(df_reg3.amount-df_reg3.y_pred_reg3_logit)\n",
    "\n",
    "    scores = pd.DataFrame(columns=('min', 'mean', 'max'), \n",
    "                          index=('reg2','reg2_abs', 'reg2_logit', 'reg2_logit_abs', \n",
    "                                 'reg3','reg3_abs', 'reg3_logit', 'reg3_logit_abs'))\n",
    "    scores.loc['reg2']=[np.min(df_reg2.discr), np.mean(df_reg2.discr), np.max(df_reg2.discr)]\n",
    "    scores.loc['reg2_abs']=[np.min(df_reg2.discr_abs), np.mean(df_reg2.discr_abs), np.max(df_reg2.discr_abs)]\n",
    "    scores.loc['reg2_logit']=[np.min(df_reg2.discr_logit), np.mean(df_reg2.discr_logit), np.max(df_reg2.discr_logit)]\n",
    "    scores.loc['reg2_logit_abs']=[np.min(df_reg2.discr_logit_abs), np.mean(df_reg2.discr_logit_abs), np.max(df_reg2.discr_logit_abs)]\n",
    "    scores.loc['reg3']=[np.min(df_reg3.discr), np.mean(df_reg3.discr), np.max(df_reg3.discr)]\n",
    "    scores.loc['reg3_abs']=[np.min(df_reg3.discr_abs), np.mean(df_reg3.discr_abs), np.max(df_reg3.discr_abs)]\n",
    "    scores.loc['reg3_logit']=[np.min(df_reg3.discr_logit), np.mean(df_reg3.discr_logit), np.max(df_reg3.discr_logit)]\n",
    "    scores.loc['reg3_logit_abs']=[np.min(df_reg3.discr_logit_abs), np.mean(df_reg3.discr_logit_abs), np.max(df_reg3.discr_logit_abs)]\n",
    "\n",
    "    display(scores)\n",
    "    \n",
    "    plt.hist(df_reg2.discr, bins=100)\n",
    "    plt.title('Regression 2 Discrepancy')\n",
    "    plt.show()\n",
    "    plt.hist(df_reg2.discr_logit, bins=100)\n",
    "    plt.title('Regression 2 Logit Discrepancy')\n",
    "    plt.show()\n",
    "\n",
    "    plt.hist(df_reg3.discr, bins=100)\n",
    "    plt.title('Regression 3 Discrepancy')\n",
    "    plt.show()\n",
    "    plt.hist(df_reg3.discr_logit, bins=100)\n",
    "    plt.title('Regression 3 Logit Discrepancy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2c97a9",
   "metadata": {},
   "source": [
    "# Regression Segment 2 and 3 (Review combined result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f387c10",
   "metadata": {},
   "source": [
    "## Calculate discrepancy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97597faf",
   "metadata": {},
   "source": [
    "Keep Regression results only for correct classes (reg2 for '2' and '2?', reg3 for '3' and '3?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc14bc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T19:06:11.433248Z",
     "start_time": "2021-05-16T19:05:32.434168Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for feature_group_combination in feature_group_combinations:\n",
    "    model_name=\"_\".join(feature_group_combination)\n",
    "    print('Combined regression score for {}'.format(model_name))\n",
    "    \n",
    "    df_dict[model_name].loc[df_dict[model_name]['y_pred'].isin(['3','3?','4+']),['y_pred_reg2','y_pred_reg2_logit']]=np.nan\n",
    "    df_dict[model_name].loc[df_dict[model_name]['y_pred'].isin(['2','2?','4+']),['y_pred_reg3','y_pred_reg3_logit']]=np.nan\n",
    "\n",
    "    df_dict[model_name]['discr_reg2']=df_dict[model_name].amount-df_dict[model_name].y_pred_reg2\n",
    "    df_dict[model_name]['discr_reg2_abs']=abs(df_dict[model_name].amount-df_dict[model_name].y_pred_reg2)\n",
    "    df_dict[model_name]['discr_reg2_logit']=df_dict[model_name].amount-df_dict[model_name].y_pred_reg2_logit\n",
    "    df_dict[model_name]['discr_reg2_logit_abs']=abs(df_dict[model_name].amount-df_dict[model_name].y_pred_reg2_logit)\n",
    "    df_dict[model_name]['discr_reg3']=df_dict[model_name].amount-df_dict[model_name].y_pred_reg3\n",
    "    df_dict[model_name]['discr_reg3_abs']=abs(df_dict[model_name].amount-df_dict[model_name].y_pred_reg3)\n",
    "    df_dict[model_name]['discr_reg3_logit']=df_dict[model_name].amount-df_dict[model_name].y_pred_reg3_logit\n",
    "    df_dict[model_name]['discr_reg3_logit_abs']=abs(df_dict[model_name].amount-df_dict[model_name].y_pred_reg3_logit)\n",
    "    \n",
    "    print(\"Only for classes 2 and 3\")\n",
    "\n",
    "    scores = pd.DataFrame(columns=('min', 'mean', 'max'), \n",
    "                          index=('reg2','reg2_abs', 'reg2_logit', 'reg2_logit_abs', \n",
    "                                 'reg3','reg3_abs', 'reg3_logit', 'reg3_logit_abs'))\n",
    "    \n",
    "    \n",
    "    scores.loc['reg2']=[np.min(df_dict[model_name][df_dict[model_name].y_pred.isin(['2'])].discr_reg2), \n",
    "                        np.mean(df_dict[model_name][df_dict[model_name].y_pred.isin(['2'])].discr_reg2), \n",
    "                        np.max(df_dict[model_name][df_dict[model_name].y_pred.isin(['2'])].discr_reg2)]\n",
    "    scores.loc['reg2_abs']=[np.min(df_dict[model_name][df_dict[model_name].y_pred.isin(['2'])].discr_reg2_abs), \n",
    "                            np.mean(df_dict[model_name][df_dict[model_name].y_pred.isin(['2'])].discr_reg2_abs), \n",
    "                            np.max(df_dict[model_name][df_dict[model_name].y_pred.isin(['2'])].discr_reg2_abs)]\n",
    "    scores.loc['reg2_logit']=[np.min(df_dict[model_name][df_dict[model_name].y_pred.isin(['2'])].discr_reg2_logit), \n",
    "                              np.mean(df_dict[model_name][df_dict[model_name].y_pred.isin(['2'])].discr_reg2_logit),\n",
    "                              np.max(df_dict[model_name][df_dict[model_name].y_pred.isin(['2'])].discr_reg2_logit)]\n",
    "    scores.loc['reg2_logit_abs']=[np.min(df_dict[model_name][df_dict[model_name].y_pred.isin(['2'])].discr_reg2_logit_abs), \n",
    "                                  np.mean(df_dict[model_name][df_dict[model_name].y_pred.isin(['2'])].discr_reg2_logit_abs), \n",
    "                                  np.max(df_dict[model_name][df_dict[model_name].y_pred.isin(['2'])].discr_reg2_logit_abs)]\n",
    "    scores.loc['reg3']=[np.min(df_dict[model_name][df_dict[model_name].y_pred.isin(['3'])].discr_reg3), \n",
    "                        np.mean(df_dict[model_name][df_dict[model_name].y_pred.isin(['3'])].discr_reg3), \n",
    "                        np.max(df_dict[model_name][df_dict[model_name].y_pred.isin(['3'])].discr_reg3)]\n",
    "    scores.loc['reg3_abs']=[np.min(df_dict[model_name][df_dict[model_name].y_pred.isin(['3'])].discr_reg3_abs), \n",
    "                            np.mean(df_dict[model_name][df_dict[model_name].y_pred.isin(['3'])].discr_reg3_abs), \n",
    "                            np.max(df_dict[model_name][df_dict[model_name].y_pred.isin(['3'])].discr_reg3_abs)]\n",
    "    scores.loc['reg3_logit']=[np.min(df_dict[model_name][df_dict[model_name].y_pred.isin(['3'])].discr_reg3_logit), \n",
    "                              np.mean(df_dict[model_name][df_dict[model_name].y_pred.isin(['3'])].discr_reg3_logit),\n",
    "                              np.max(df_dict[model_name][df_dict[model_name].y_pred.isin(['3'])].discr_reg3_logit)]\n",
    "    scores.loc['reg3_logit_abs']=[np.min(df_dict[model_name][df_dict[model_name].y_pred.isin(['3'])].discr_reg3_logit_abs), \n",
    "                                  np.mean(df_dict[model_name][df_dict[model_name].y_pred.isin(['3'])].discr_reg3_logit_abs), \n",
    "                                  np.max(df_dict[model_name][df_dict[model_name].y_pred.isin(['3'])].discr_reg3_logit_abs)]\n",
    "    \n",
    "    display(scores)\n",
    "\n",
    "    \n",
    "    print(\"For classes 2, 2?,3? and 3\")\n",
    "\n",
    "    scores = pd.DataFrame(columns=('min', 'mean', 'max'), \n",
    "                          index=('reg2','reg2_abs', 'reg2_logit', 'reg2_logit_abs', \n",
    "                                 'reg3','reg3_abs', 'reg3_logit', 'reg3_logit_abs'))\n",
    "    scores.loc['reg2']=[np.min(df_dict[model_name][df_dict[model_name].y_pred.isin(['2','2?'])].discr_reg2), \n",
    "                        np.mean(df_dict[model_name][df_dict[model_name].y_pred.isin(['2','2?'])].discr_reg2), \n",
    "                        np.max(df_dict[model_name][df_dict[model_name].y_pred.isin(['2','2?'])].discr_reg2)]\n",
    "    scores.loc['reg2_abs']=[np.min(df_dict[model_name][df_dict[model_name].y_pred.isin(['2','2?'])].discr_reg2_abs), \n",
    "                            np.mean(df_dict[model_name][df_dict[model_name].y_pred.isin(['2','2?'])].discr_reg2_abs), \n",
    "                            np.max(df_dict[model_name][df_dict[model_name].y_pred.isin(['2','2?'])].discr_reg2_abs)]\n",
    "    scores.loc['reg2_logit']=[np.min(df_dict[model_name][df_dict[model_name].y_pred.isin(['2','2?'])].discr_reg2_logit), \n",
    "                              np.mean(df_dict[model_name][df_dict[model_name].y_pred.isin(['2','2?'])].discr_reg2_logit),\n",
    "                              np.max(df_dict[model_name][df_dict[model_name].y_pred.isin(['2','2?'])].discr_reg2_logit)]\n",
    "    scores.loc['reg2_logit_abs']=[np.min(df_dict[model_name][df_dict[model_name].y_pred.isin(['2','2?'])].discr_reg2_logit_abs), \n",
    "                                  np.mean(df_dict[model_name][df_dict[model_name].y_pred.isin(['2','2?'])].discr_reg2_logit_abs), \n",
    "                                  np.max(df_dict[model_name][df_dict[model_name].y_pred.isin(['2','2?'])].discr_reg2_logit_abs)]\n",
    "    scores.loc['reg3']=[np.min(df_dict[model_name][df_dict[model_name].y_pred.isin(['3','3?'])].discr_reg3), \n",
    "                        np.mean(df_dict[model_name][df_dict[model_name].y_pred.isin(['3','3?'])].discr_reg3), \n",
    "                        np.max(df_dict[model_name][df_dict[model_name].y_pred.isin(['3','3?'])].discr_reg3)]\n",
    "    scores.loc['reg3_abs']=[np.min(df_dict[model_name][df_dict[model_name].y_pred.isin(['3','3?'])].discr_reg3_abs), \n",
    "                            np.mean(df_dict[model_name][df_dict[model_name].y_pred.isin(['3','3?'])].discr_reg3_abs), \n",
    "                            np.max(df_dict[model_name][df_dict[model_name].y_pred.isin(['3','3?'])].discr_reg3_abs)]\n",
    "    scores.loc['reg3_logit']=[np.min(df_dict[model_name][df_dict[model_name].y_pred.isin(['3','3?'])].discr_reg3_logit), \n",
    "                              np.mean(df_dict[model_name][df_dict[model_name].y_pred.isin(['3','3?'])].discr_reg3_logit),\n",
    "                              np.max(df_dict[model_name][df_dict[model_name].y_pred.isin(['3','3?'])].discr_reg3_logit)]\n",
    "    scores.loc['reg3_logit_abs']=[np.min(df_dict[model_name][df_dict[model_name].y_pred.isin(['3','3?'])].discr_reg3_logit_abs), \n",
    "                                  np.mean(df_dict[model_name][df_dict[model_name].y_pred.isin(['3','3?'])].discr_reg3_logit_abs), \n",
    "                                  np.max(df_dict[model_name][df_dict[model_name].y_pred.isin(['3','3?'])].discr_reg3_logit_abs)]\n",
    "    \n",
    "    display(scores)\n",
    "\n",
    "    plt.hist(df_dict[model_name][(df_dict[model_name].y_pred.isin(['2']))].discr_reg2, bins=100)\n",
    "    plt.title('Discrepancy for predicted seg. \"2\"' )\n",
    "    plt.show()\n",
    "    plt.hist(df_dict[model_name][(df_dict[model_name].y_pred.isin(['2','2?']))].discr_reg2, bins=100)\n",
    "    plt.title('Discrepancy for predicted seg. \"2\" and \"2?\"' )\n",
    "    plt.show()\n",
    "\n",
    "    plt.hist(df_dict[model_name][(df_dict[model_name].y_pred.isin(['2']))&(df_dict[model_name].discr_reg2_abs<=100)].discr_reg2, bins=100)\n",
    "    plt.title('Discrepancy for predicted seg. \"2\" where absolute discr. <= 100' )\n",
    "    plt.show()\n",
    "    plt.hist(df_dict[model_name][(df_dict[model_name].y_pred.isin(['2','2?']))&(df_dict[model_name].discr_reg2_abs<=100)].discr_reg2, bins=100)\n",
    "    plt.title('Discrepancy for predicted seg. \"2\" and \"2?\" where absolute discr. <= 100' )\n",
    "    plt.show()\n",
    "    \n",
    "    plt.hist(df_dict[model_name][(df_dict[model_name].y_pred.isin(['2']))].discr_reg2_logit, bins=100)\n",
    "    plt.title('(Logit) Discrepancy for predicted seg. \"2\" ' )\n",
    "    plt.show()\n",
    "    plt.hist(df_dict[model_name][(df_dict[model_name].y_pred.isin(['2','2?']))].discr_reg2_logit, bins=100)\n",
    "    plt.title('(Logit) Discrepancy for predicted seg. \"2\" and \"2?\" ' )\n",
    "    plt.show()\n",
    "\n",
    "    plt.hist(df_dict[model_name][(df_dict[model_name].y_pred.isin(['2']))&(df_dict[model_name].discr_reg2_logit_abs<=100)].discr_reg2_logit, bins=100)\n",
    "    plt.title('(Logit) Discrepancy for predicted seg. \"2\" where absolute discr. <= 100' )\n",
    "    plt.show()\n",
    "    plt.hist(df_dict[model_name][(df_dict[model_name].y_pred.isin(['2','2?']))&(df_dict[model_name].discr_reg2_logit_abs<=100)].discr_reg2_logit, bins=100)\n",
    "    plt.title('(Logit) Discrepancy for predicted seg. \"2\" and \"2?\" where absolute discr. <= 100' )\n",
    "    plt.show()\n",
    "    \n",
    "    plt.hist(df_dict[model_name][(df_dict[model_name].y_pred.isin(['3']))].discr_reg3, bins=100)\n",
    "    plt.title('Discrepancy for predicted seg. \"3\"' )\n",
    "    plt.show()\n",
    "    plt.hist(df_dict[model_name][(df_dict[model_name].y_pred.isin(['3','3?']))].discr_reg3, bins=100)\n",
    "    plt.title('Discrepancy for predicted seg. \"3\" and \"3?\"' )\n",
    "    plt.show()\n",
    "\n",
    "    plt.hist(df_dict[model_name][(df_dict[model_name].y_pred.isin(['3']))&(df_dict[model_name].discr_reg3_abs<=100)].discr_reg3, bins=100)\n",
    "    plt.title('Discrepancy for predicted seg. \"3\" where absolute discr. <= 100' )\n",
    "    plt.show()\n",
    "    plt.hist(df_dict[model_name][(df_dict[model_name].y_pred.isin(['3','3?']))&(df_dict[model_name].discr_reg3_abs<=100)].discr_reg3, bins=100)\n",
    "    plt.title('Discrepancy for predicted seg. \"3\" and \"3?\" where absolute discr. <= 100' )\n",
    "    plt.show()\n",
    "    \n",
    "    plt.hist(df_dict[model_name][(df_dict[model_name].y_pred.isin(['3']))].discr_reg3_logit, bins=100)\n",
    "    plt.title('(Logit) Discrepancy for predicted seg. \"3\" ' )\n",
    "    plt.show()\n",
    "    plt.hist(df_dict[model_name][(df_dict[model_name].y_pred.isin(['3','3?']))].discr_reg3_logit, bins=100)\n",
    "    plt.title('(Logit) Discrepancy for predicted seg. \"3\" and \"3?\" ' )\n",
    "    plt.show()\n",
    "\n",
    "    plt.hist(df_dict[model_name][(df_dict[model_name].y_pred.isin(['3']))&(df_dict[model_name].discr_reg3_logit_abs<=100)].discr_reg3_logit, bins=100)\n",
    "    plt.title('(Logit) Discrepancy for predicted seg. \"3\" where absolute discr. <= 100' )\n",
    "    plt.show()\n",
    "    plt.hist(df_dict[model_name][(df_dict[model_name].y_pred.isin(['3','3?']))&(df_dict[model_name].discr_reg3_logit_abs<=100)].discr_reg3_logit, bins=100)\n",
    "    plt.title('(Logit) Discrepancy for predicted seg. \"3\" and \"3?\" where absolute discr. <= 100' )\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21129049",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5f2448",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2790e920",
   "metadata": {},
   "source": [
    "### CLF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a90ddfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T19:12:04.715380Z",
     "start_time": "2021-05-16T19:08:30.666907Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for feature_group_combination in feature_group_combinations:\n",
    "    model_name=\"_\".join(feature_group_combination)\n",
    "    print('Plot feature importance for CLF1 for {}'.format(model_name))\n",
    "    \n",
    "    explainer_clf1 = shap.TreeExplainer(clf1_models[model_name])\n",
    "    shap_values_clf1 = explainer_clf1.shap_values(df_dict[model_name][model_features[model_name]])\n",
    "    shap.summary_plot(shap_values_clf1, features=df_dict[model_name][model_features[model_name]], \n",
    "                      feature_names=df_dict[model_name][model_features[model_name]].columns)\n",
    "    \n",
    "    shap.summary_plot(shap_values_clf1, features=df_dict[model_name][model_features[model_name]], \n",
    "                      feature_names=df_dict[model_name][model_features[model_name]].columns, plot_type='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee9bbec",
   "metadata": {},
   "source": [
    "### Feature importance CLF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090a2ad2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T19:14:01.034560Z",
     "start_time": "2021-05-16T19:12:04.715380Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for feature_group_combination in feature_group_combinations:\n",
    "    model_name=\"_\".join(feature_group_combination)\n",
    "    print('Plot feature importance for CLF2 for {}'.format(model_name))\n",
    "    \n",
    "    explainer_clf2 = shap.TreeExplainer(clf2_models[model_name])\n",
    "    shap_values_clf2 = explainer_clf2.shap_values(df_dict[model_name][~df_dict[model_name].y_clf2.isnull()][model_features[model_name]])\n",
    "    shap.summary_plot(shap_values_clf2, \n",
    "                      features=df_dict[model_name][~df_dict[model_name].y_clf2.isnull()][model_features[model_name]], \n",
    "                      feature_names=df_dict[model_name][model_features[model_name]].columns)\n",
    "    \n",
    "    shap.summary_plot(shap_values_clf2, features=df_dict[model_name][~df_dict[model_name].y_clf2.isnull()][model_features[model_name]], \n",
    "                      feature_names=df_dict[model_name][model_features[model_name]].columns, plot_type='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f75e1c",
   "metadata": {},
   "source": [
    "### Feature Importance Reg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d880db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T19:16:40.786567Z",
     "start_time": "2021-05-16T19:14:01.036580Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for feature_group_combination in feature_group_combinations:\n",
    "    model_name=\"_\".join(feature_group_combination)\n",
    "    print('Plot feature importance for REG2 for {}'.format(model_name))\n",
    "    \n",
    "    explainer_reg2 = shap.TreeExplainer(reg2_models[model_name])\n",
    "    shap_values_reg2 = explainer_reg2.shap_values(df_dict[model_name][df_dict[model_name].amount_segment==2][model_features[model_name]])\n",
    "    shap.summary_plot(shap_values_reg2, features=df_dict[model_name][df_dict[model_name].amount_segment==2][model_features[model_name]], feature_names=df_dict[model_name][model_features[model_name]].columns)\n",
    "    shap.summary_plot(shap_values_reg2, features=df_dict[model_name][df_dict[model_name].amount_segment==2][model_features[model_name]], feature_names=df_dict[model_name][model_features[model_name]].columns, plot_type='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3449672",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T19:19:00.209740Z",
     "start_time": "2021-05-16T19:16:40.786567Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for feature_group_combination in feature_group_combinations:\n",
    "    model_name=\"_\".join(feature_group_combination)\n",
    "    print('Plot feature importance for REG2 LOGIT for {}'.format(model_name))\n",
    "    \n",
    "    explainer_reg2_logit = shap.TreeExplainer(reg2_logit_models[model_name])\n",
    "    shap_values_reg2_logit = explainer_reg2_logit.shap_values(df_dict[model_name][df_dict[model_name].amount_segment==2][model_features[model_name]])\n",
    "    shap.summary_plot(shap_values_reg2_logit, features=df_dict[model_name][df_dict[model_name].amount_segment==2][model_features[model_name]], feature_names=df_dict[model_name][model_features[model_name]].columns)\n",
    "    \n",
    "    shap.summary_plot(shap_values_reg2_logit, features=df_dict[model_name][df_dict[model_name].amount_segment==2][model_features[model_name]], feature_names=df_dict[model_name][model_features[model_name]].columns, plot_type='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0004a743",
   "metadata": {},
   "source": [
    "### Feature Importance reg3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eada452e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T19:20:09.851854Z",
     "start_time": "2021-05-16T19:19:00.209740Z"
    }
   },
   "outputs": [],
   "source": [
    "for feature_group_combination in feature_group_combinations:\n",
    "    model_name=\"_\".join(feature_group_combination)\n",
    "    print('Plot feature importance for REG3 for {}'.format(model_name))\n",
    "    \n",
    "    explainer_reg3 = shap.TreeExplainer(reg3_models[model_name])\n",
    "    shap_values_reg3 = explainer_reg3.shap_values(df_dict[model_name][df_dict[model_name].amount_segment==3][model_features[model_name]])\n",
    "    shap.summary_plot(shap_values_reg3, features=df_dict[model_name][df_dict[model_name].amount_segment==3][model_features[model_name]], feature_names=df_dict[model_name][model_features[model_name]].columns)\n",
    "    \n",
    "    shap.summary_plot(shap_values_reg3, features=df_dict[model_name][df_dict[model_name].amount_segment==3][model_features[model_name]], feature_names=df_dict[model_name][model_features[model_name]].columns, plot_type='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444cd6ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T20:31:14.343405Z",
     "start_time": "2021-05-16T20:30:04.235686Z"
    }
   },
   "outputs": [],
   "source": [
    "for feature_group_combination in feature_group_combinations:\n",
    "    model_name=\"_\".join(feature_group_combination)\n",
    "    print('Plot feature importance for REG3 Logit for {}'.format(model_name))\n",
    "  \n",
    "    explainer_reg3_logit = shap.TreeExplainer(reg3_logit_models[model_name])\n",
    "    shap_values_reg3_logit = explainer_reg3_logit.shap_values(df_dict[model_name][df_dict[model_name].amount_segment==3][model_features[model_name]])\n",
    "    shap.summary_plot(shap_values_reg3_logit, features=df_dict[model_name][df_dict[model_name].amount_segment==3][model_features[model_name]], feature_names=df_dict[model_name][model_features[model_name]].columns)\n",
    "    \n",
    "    shap.summary_plot(shap_values_reg3_logit, features=df_dict[model_name][df_dict[model_name].amount_segment==3][model_features[model_name]], feature_names=df_dict[model_name][model_features[model_name]].columns, plot_type='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f314503",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dba00eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T08:08:16.433796Z",
     "start_time": "2021-06-30T08:07:19.747234Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_model_eval = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "df_dict = {}\n",
    "cv_results_clf1 = {}\n",
    "cv_results_clf2 = {}\n",
    "cv_results_reg2 = {}\n",
    "cv_results_reg2_logit = {}\n",
    "cv_results_reg3 = {}\n",
    "cv_results_reg3_logit = {}\n",
    "\n",
    "for feature_group_combination in tqdm(feature_group_combinations):\n",
    "    model_name=\"_\".join(feature_group_combination)\n",
    "    print('Crossvalidate models for {}'.format(model_name))\n",
    "    \n",
    "    df_dict[model_name]=df[model_features[model_name]+['amount','amount_segment','y_clf1','y_clf2']].dropna(subset=model_features[model_name]).copy()\n",
    "\n",
    "    #Test dataset is the same for all models\n",
    "    X_test = df_dict[model_name][model_features[model_name]]\n",
    "\n",
    "    X_train_clf1 = df_dict[model_name][model_features[model_name]]\n",
    "    y_train_clf1 = df_dict[model_name].y_clf1\n",
    "\n",
    "    # For Classifier2 (clf2) we use data from segments 2 and 3 (so we drop records where y_clf2 is NaN)\n",
    "    df_train_clf2 = df_dict[model_name].dropna(subset=['y_clf2'])\n",
    "\n",
    "    X_train_clf2 = df_train_clf2[model_features[model_name]]\n",
    "    y_train_clf2 = df_train_clf2.y_clf2.astype(int)\n",
    "\n",
    "\n",
    "    # For Regression seg2 use only Segment 2 and amount 0.00001>=amount>=49.9999 (because of logit)\n",
    "    df_train_reg2 = df_dict[model_name][(df_dict[model_name]['amount_segment']==2) & \n",
    "                             (df_dict[model_name]['amount']>=0.00001) & \n",
    "                             (df_dict[model_name]['amount']<=49.9999)]\n",
    "\n",
    "    X_train_reg2 = df_train_reg2[model_features[model_name]]\n",
    "    y_train_reg2 = df_train_reg2.amount\n",
    "    y_train_reg2_logit = (y_train_reg2/50).apply(logit)\n",
    "\n",
    "\n",
    "    # For Regression seg3 use only Segment 3 and amount 50>amount>=99.9999 (because of logit)\n",
    "    df_train_reg3 = df_dict[model_name][(df_dict[model_name]['amount_segment']==3) & \n",
    "                             (df_dict[model_name]['amount']>50) & \n",
    "                             (df_dict[model_name]['amount']<=99.9999)]\n",
    "\n",
    "    X_train_reg3 = df_train_reg3[model_features[model_name]]\n",
    "    y_train_reg3 = df_train_reg3.amount\n",
    "    y_train_reg3_logit = ((y_train_reg3-50)/50).apply(logit)\n",
    "\n",
    "\n",
    "    cv_results_clf1[model_name] = cross_validate(clf1_models[model_name], X_train_clf1, y_train_clf1, scoring=['accuracy', 'roc_auc'], cv=cv_model_eval, return_train_score=True)   \n",
    "    cv_results_clf2[model_name] = cross_validate(clf2_models[model_name], X_train_clf2, y_train_clf2, scoring=['accuracy', 'roc_auc'], cv=cv_model_eval, return_train_score=True)\n",
    "    cv_results_reg2[model_name] = cross_validate(reg2_models[model_name], X_train_reg2, y_train_reg2, scoring=['neg_mean_squared_error', 'r2'], cv=cv_model_eval, return_train_score=True)\n",
    "    cv_results_reg3[model_name] = cross_validate(reg3_models[model_name], X_train_reg3, y_train_reg3, scoring=['neg_mean_squared_error', 'r2'], cv=cv_model_eval, return_train_score=True)\n",
    "    cv_results_reg2_logit[model_name] = cross_validate(reg2_logit_models[model_name], X_train_reg2, y_train_reg2_logit, scoring=['neg_mean_squared_error', 'r2'], cv=cv_model_eval, return_train_score=True)\n",
    "    cv_results_reg3_logit[model_name] = cross_validate(reg3_logit_models[model_name], X_train_reg3, y_train_reg3_logit, scoring=['neg_mean_squared_error', 'r2'], cv=cv_model_eval, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dc8770",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for feature_group_combination in tqdm(feature_group_combinations):\n",
    "    model_name=\"_\".join(feature_group_combination)\n",
    "    print('Cross-validation results for {}'.format(model_name))\n",
    "    print(cv_results_clf1[model_name])\n",
    "    print(cv_results_clf2[model_name])\n",
    "    print(cv_results_reg2[model_name])\n",
    "    print(cv_results_reg2_logit[model_name])\n",
    "    print(cv_results_reg3[model_name])\n",
    "    print(cv_results_reg3_logit[model_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70be2eaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T07:54:45.300481Z",
     "start_time": "2021-06-30T07:54:44.890614Z"
    }
   },
   "outputs": [],
   "source": [
    "columns_list_clf=['Model', 'Feature_Combination', \n",
    "              'Mean_Train_Accuracy', 'Std_Train_Accuracy',\n",
    "              'Mean_Test_Accuracy', 'Std_Test_Accuracy', \n",
    "              'Mean_Train_AUC', 'Std_Train_AUC', \n",
    "              'Mean_Test_AUC', 'Std_Test_AUC']\n",
    "cv_results_clf = pd.DataFrame(columns=columns_list_clf)\n",
    "\n",
    "columns_list_reg=['Model', 'Feature_Combination', \n",
    "              'Mean_Train_NMSE', 'Std_Train_NMSE',\n",
    "              'Mean_Test_NMSE', 'Std_Test_NMSE', \n",
    "              'Mean_Train_R2', 'Std_Train_R2', \n",
    "              'Mean_Test_R2', 'Std_Test_R2']\n",
    "cv_results_reg = pd.DataFrame(columns=columns_list_reg)\n",
    "\n",
    "\n",
    "for feature_group_combination in tqdm(feature_group_combinations):\n",
    "    model_name=\"_\".join(feature_group_combination)\n",
    "    cv_results_clf=cv_results_clf.append(pd.DataFrame([['clf1', model_name,\n",
    "                                 np.mean(cv_results_clf1[model_name]['train_accuracy']),\n",
    "                                 np.std(cv_results_clf1[model_name]['train_accuracy']),\n",
    "                                 np.mean(cv_results_clf1[model_name]['test_accuracy']),\n",
    "                                 np.std(cv_results_clf1[model_name]['test_accuracy']),\n",
    "                                 np.mean(cv_results_clf1[model_name]['train_roc_auc']),\n",
    "                                 np.std(cv_results_clf1[model_name]['train_roc_auc']),\n",
    "                                 np.mean(cv_results_clf1[model_name]['test_roc_auc']),\n",
    "                                 np.std(cv_results_clf1[model_name]['test_roc_auc'])]], columns=columns_list_clf),\n",
    "                                ignore_index=True)\n",
    "\n",
    "    cv_results_clf=cv_results_clf.append(pd.DataFrame([['clf2', model_name,\n",
    "                                 np.mean(cv_results_clf2[model_name]['train_accuracy']),\n",
    "                                 np.std(cv_results_clf2[model_name]['train_accuracy']),\n",
    "                                 np.mean(cv_results_clf2[model_name]['test_accuracy']),\n",
    "                                 np.std(cv_results_clf2[model_name]['test_accuracy']),\n",
    "                                 np.mean(cv_results_clf2[model_name]['train_roc_auc']),\n",
    "                                 np.std(cv_results_clf2[model_name]['train_roc_auc']),\n",
    "                                 np.mean(cv_results_clf2[model_name]['test_roc_auc']),\n",
    "                                 np.std(cv_results_clf2[model_name]['test_roc_auc'])]], columns=columns_list_clf),\n",
    "                                ignore_index=True)\n",
    "\n",
    "    cv_results_reg=cv_results_reg.append(pd.DataFrame([['reg2', model_name,\n",
    "                                 np.mean(cv_results_reg2[model_name]['train_neg_mean_squared_error']),\n",
    "                                 np.std(cv_results_reg2[model_name]['train_neg_mean_squared_error']),\n",
    "                                 np.mean(cv_results_reg2[model_name]['test_neg_mean_squared_error']),\n",
    "                                 np.std(cv_results_reg2[model_name]['test_neg_mean_squared_error']),\n",
    "                                 np.mean(cv_results_reg2[model_name]['train_r2']),\n",
    "                                 np.std(cv_results_reg2[model_name]['train_r2']),\n",
    "                                 np.mean(cv_results_reg2[model_name]['test_r2']),\n",
    "                                 np.std(cv_results_reg2[model_name]['test_r2'])]], columns=columns_list_reg),\n",
    "                                ignore_index=True)\n",
    "    \n",
    "    cv_results_reg=cv_results_reg.append(pd.DataFrame([['reg2_logit', model_name,\n",
    "                                 np.mean(cv_results_reg2_logit[model_name]['train_neg_mean_squared_error']),\n",
    "                                 np.std(cv_results_reg2_logit[model_name]['train_neg_mean_squared_error']),\n",
    "                                 np.mean(cv_results_reg2_logit[model_name]['test_neg_mean_squared_error']),\n",
    "                                 np.std(cv_results_reg2_logit[model_name]['test_neg_mean_squared_error']),\n",
    "                                 np.mean(cv_results_reg2_logit[model_name]['train_r2']),\n",
    "                                 np.std(cv_results_reg2_logit[model_name]['train_r2']),\n",
    "                                 np.mean(cv_results_reg2_logit[model_name]['test_r2']),\n",
    "                                 np.std(cv_results_reg2_logit[model_name]['test_r2'])]], columns=columns_list_reg),\n",
    "                                ignore_index=True)\n",
    "    cv_results_reg=cv_results_reg.append(pd.DataFrame([['reg3', model_name,\n",
    "                                 np.mean(cv_results_reg3[model_name]['train_neg_mean_squared_error']),\n",
    "                                 np.std(cv_results_reg3[model_name]['train_neg_mean_squared_error']),\n",
    "                                 np.mean(cv_results_reg3[model_name]['test_neg_mean_squared_error']),\n",
    "                                 np.std(cv_results_reg3[model_name]['test_neg_mean_squared_error']),\n",
    "                                 np.mean(cv_results_reg3[model_name]['train_r2']),\n",
    "                                 np.std(cv_results_reg3[model_name]['train_r2']),\n",
    "                                 np.mean(cv_results_reg3[model_name]['test_r2']),\n",
    "                                 np.std(cv_results_reg3[model_name]['test_r2'])]], columns=columns_list_reg),\n",
    "                                ignore_index=True)\n",
    "    cv_results_reg=cv_results_reg.append(pd.DataFrame([['reg3_logit', model_name,\n",
    "                                 np.mean(cv_results_reg3_logit[model_name]['train_neg_mean_squared_error']),\n",
    "                                 np.std(cv_results_reg3_logit[model_name]['train_neg_mean_squared_error']),\n",
    "                                 np.mean(cv_results_reg3_logit[model_name]['test_neg_mean_squared_error']),\n",
    "                                 np.std(cv_results_reg3_logit[model_name]['test_neg_mean_squared_error']),\n",
    "                                 np.mean(cv_results_reg3_logit[model_name]['train_r2']),\n",
    "                                 np.std(cv_results_reg3_logit[model_name]['train_r2']),\n",
    "                                 np.mean(cv_results_reg3_logit[model_name]['test_r2']),\n",
    "                                 np.std(cv_results_reg3_logit[model_name]['test_r2'])]], columns=columns_list_reg),\n",
    "                                ignore_index=True)\n",
    "\n",
    "\n",
    "display(cv_results_clf)\n",
    "display(cv_results_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65af0758",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a663a7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T09:20:44.689336Z",
     "start_time": "2021-07-12T09:20:25.111216Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "df_dict={}\n",
    "\n",
    "def grid_search_model_clf1(feature_group_combination):\n",
    "    print('Grid Search model for: ', \" and \".join(feature_group_combination))\n",
    "        \n",
    "    df_dict[model_name]=df[model_features[model_name]+['amount','amount_segment','y_clf1','y_clf2']].dropna(subset=model_features[model_name]).copy()\n",
    "\n",
    "    #Test dataset is the same for all models\n",
    "    X = df_dict[model_name][model_features[model_name]]\n",
    "    y = df_dict[model_name].y_clf1\n",
    "                \n",
    "    scoring = {'AUC': 'roc_auc', 'Accuracy': 'accuracy'}\n",
    "\n",
    "    parameters = {\n",
    "        'max_depth': [6,7,8,9], # [default=6]\n",
    "        'n_estimators': [500,700,900,1100], # [default=100]\n",
    "        'tree_method': ['gpu_hist'],\n",
    "        'use_label_encoder': [False], \n",
    "        'objective': ['binary:logistic'],\n",
    "        'eval_metric': ['error']\n",
    "    }\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier()\n",
    "    cv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    grid_search = GridSearchCV(xgb_model, parameters, n_jobs=4, cv=cv, scoring=scoring, refit=False, verbose=2, return_train_score=True) \n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    print(grid_search.cv_results_)\n",
    "    \n",
    "    return grid_search\n",
    "\n",
    "\n",
    "gs_results={}\n",
    "for feature_group_combination in feature_group_combinations :\n",
    "    pkl_filename = Path('./grid_search/'+'_'.join(feature_group_combination)+\"_grid_search_clf1.pkl\")\n",
    "    \n",
    "    if pkl_filename.exists() :\n",
    "        print('Load Grid Search results for clf1 model '+'_'.join(feature_group_combination))\n",
    "        \n",
    "        with open(pkl_filename, 'rb') as file:\n",
    "            gs_result= pickle.load(file)\n",
    "    else :\n",
    "        print('Grid Search for clf1 model '+'_'.join(feature_group_combination))\n",
    "        gs_result = grid_search_model_clf1(feature_group_combination)\n",
    "        \n",
    "        with open(pkl_filename, 'wb') as file:\n",
    "            pickle.dump(gs_result, file)\n",
    "    \n",
    "    df_result = pd.DataFrame(gs_result.cv_results_['params'])\n",
    "    df_result['mean_test_AUC'] = gs_result.cv_results_['mean_test_AUC']\n",
    "    df_result['std_test_AUC']=gs_result.cv_results_['std_test_AUC']\n",
    "    df_result['mean_train_AUC'] = gs_result.cv_results_['mean_train_AUC']\n",
    "    df_result['std_train_AUC']=gs_result.cv_results_['std_train_AUC']\n",
    "    df_result['rank_test_AUC']=gs_result.cv_results_['rank_test_AUC']\n",
    "    df_result['mean_test_Accuracy']=gs_result.cv_results_['mean_test_Accuracy']\n",
    "    df_result['std_test_Accuracy']=gs_result.cv_results_['std_test_Accuracy']\n",
    "    df_result['rank_test_Accuracy']=gs_result.cv_results_['rank_test_Accuracy']\n",
    "    df_result['mean_train_Accuracy']=gs_result.cv_results_['mean_train_Accuracy']\n",
    "    df_result['std_train_Accuracy']=gs_result.cv_results_['std_train_Accuracy']\n",
    "    df_result['mean_time']=gs_result.cv_results_['mean_fit_time']+gs_result.cv_results_['mean_score_time']\n",
    "\n",
    "\n",
    "    gs_results['_'.join(feature_group_combination)]=pd.DataFrame(df_result)\n",
    "\n",
    "\n",
    "print(gs_results['location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ccaf30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T09:21:37.179705Z",
     "start_time": "2021-07-12T09:21:36.989079Z"
    }
   },
   "outputs": [],
   "source": [
    "best_param = pd.DataFrame()\n",
    "\n",
    "for feature_group_combination in feature_group_combinations:\n",
    "    result = gs_results['_'.join(feature_group_combination)]\n",
    "    best_param.loc[\n",
    "        '_'.join(feature_group_combination),\n",
    "        ['max_depth_Acc', 'n_estimators_Acc', 'Acc_score', 'max_depth_AUC', 'n_estimators_AUC', 'AUC_score'\n",
    "         ]] = result.loc[result.rank_test_Accuracy == 1,\n",
    "                         ['max_depth', 'n_estimators', 'mean_test_Accuracy']].values[0].tolist() + result.loc[\n",
    "                             result.rank_test_Accuracy == 1,\n",
    "                             ['max_depth', 'n_estimators', 'mean_test_AUC']].values[0].tolist()\n",
    "\n",
    "print(best_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83926914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_model_clf2(feature_group_combination):\n",
    "    print('Grid Search model for: ', \" and \".join(feature_group_combination))\n",
    "        \n",
    "    df_dict[model_name]=df[model_features[model_name]+['amount','amount_segment','y_clf1','y_clf2']].dropna(subset=model_features[model_name]).copy()\n",
    "    df_dict[model_name] = df_dict[model_name].dropna(subset=['y_clf2'])\n",
    "\n",
    "    #Test dataset is the same for all models\n",
    "    X = df_dict[model_name][model_features[model_name]]\n",
    "    y = df_dict[model_name].y_clf2\n",
    "                \n",
    "    scoring = {'AUC': 'roc_auc', 'Accuracy': 'accuracy'}\n",
    "\n",
    "    parameters = {\n",
    "        'max_depth': [6,7,8,9], # [default=6]\n",
    "        'n_estimators': [500,700,900,1100], # [default=100]\n",
    "        'tree_method': ['gpu_hist'],\n",
    "        'use_label_encoder': [False], \n",
    "        'objective': ['binary:logistic'],\n",
    "        'eval_metric': ['error']\n",
    "    }\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier()\n",
    "    cv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    grid_search = GridSearchCV(xgb_model, parameters, n_jobs=4, cv=cv, scoring=scoring, refit=False, verbose=2, return_train_score=True) \n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    print(grid_search.cv_results_)\n",
    "    \n",
    "    return grid_search\n",
    "\n",
    "\n",
    "gs_results={}\n",
    "for feature_group_combination in feature_group_combinations :\n",
    "    pkl_filename = Path('./grid_search/'+'_'.join(feature_group_combination)+\"_grid_search_clf2.pkl\")\n",
    "    \n",
    "    if pkl_filename.exists() :\n",
    "        print('Load Grid Search results for clf2 model '+'_'.join(feature_group_combination))\n",
    "        \n",
    "        with open(pkl_filename, 'rb') as file:\n",
    "            gs_result= pickle.load(file)\n",
    "    else :\n",
    "        print('Grid Search for clf2 model '+'_'.join(feature_group_combination))\n",
    "        gs_result = grid_search_model_clf2(feature_group_combination)\n",
    "        \n",
    "        with open(pkl_filename, 'wb') as file:\n",
    "            pickle.dump(gs_result, file)\n",
    "    \n",
    "    df_result = pd.DataFrame(gs_result.cv_results_['params'])\n",
    "    df_result['mean_test_AUC'] = gs_result.cv_results_['mean_test_AUC']\n",
    "    df_result['std_test_AUC']=gs_result.cv_results_['std_test_AUC']\n",
    "    df_result['mean_train_AUC'] = gs_result.cv_results_['mean_train_AUC']\n",
    "    df_result['std_train_AUC']=gs_result.cv_results_['std_train_AUC']\n",
    "    df_result['rank_test_AUC']=gs_result.cv_results_['rank_test_AUC']\n",
    "    df_result['mean_test_Accuracy']=gs_result.cv_results_['mean_test_Accuracy']\n",
    "    df_result['std_test_Accuracy']=gs_result.cv_results_['std_test_Accuracy']\n",
    "    df_result['rank_test_Accuracy']=gs_result.cv_results_['rank_test_Accuracy']\n",
    "    df_result['mean_train_Accuracy']=gs_result.cv_results_['mean_train_Accuracy']\n",
    "    df_result['std_train_Accuracy']=gs_result.cv_results_['std_train_Accuracy']\n",
    "    df_result['mean_time']=gs_result.cv_results_['mean_fit_time']+gs_result.cv_results_['mean_score_time']\n",
    "\n",
    "\n",
    "    gs_results['_'.join(feature_group_combination)]=pd.DataFrame(df_result)\n",
    "\n",
    "\n",
    "print(gs_results['location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2f05db",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param = pd.DataFrame()\n",
    "\n",
    "for feature_group_combination in feature_group_combinations:\n",
    "    result = gs_results['_'.join(feature_group_combination)]\n",
    "    best_param.loc[\n",
    "        '_'.join(feature_group_combination),\n",
    "        ['max_depth_Acc', 'n_estimators_Acc', 'Acc_score', 'max_depth_AUC', 'n_estimators_AUC', 'AUC_score'\n",
    "         ]] = result.loc[result.rank_test_Accuracy == 1,\n",
    "                         ['max_depth', 'n_estimators', 'mean_test_Accuracy']].values[0].tolist() + result.loc[\n",
    "                             result.rank_test_Accuracy == 1,\n",
    "                             ['max_depth', 'n_estimators', 'mean_test_AUC']].values[0].tolist()\n",
    "\n",
    "print(best_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dbab97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m75"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "125624270022b3522b0d9fce357b71cad5d467865128e7f38888e7b3f5116099"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
