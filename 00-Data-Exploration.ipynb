{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import spacy\n",
    "from string import punctuation\n",
    "from faker import Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('C:/Saravana/Data/Raw/export-cleansed-4851f054c66579780503d70880731802.pkl.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.VERANST_SEGMENT.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all the event segments > 3 to a event segment - 4\n",
    "df.loc[df['VERANST_SEGMENT'] > 3, 'VERANST_SEGMENT'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter segment 2(0-50 euros) & segment 3(50-100 euros) & segment 4(>100 euros)\n",
    "allclaims_df = df.query('VERANST_SEGMENT <= 4')\n",
    "len(allclaims_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unique bands: ',len(df.BAND.unique()))\n",
    "print('Unique locations: ',len(df.VG_ORT.unique()))\n",
    "print('Unique venues: ',len(df.VG_RAUM.unique()))\n",
    "print('Unique Promoters: ',len(df.PROMOTER.unique()))\n",
    "print('Unique tariffs: ',len(df.TARIF_BEZ.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data_in_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Attributes\": [\"imp_id\", \"gj\", \"import\", \"mufo_referenz_n\", \"barcode_nr\", \"veranst_segment\", \"rekla_jn\", \"vg_datum_von\", \"vg_ort\",\n",
    "                      \"vg_raum\", \"nutzfall\", \"nutzfall_raum\", \"musikleiter_name\", \"kapelle_name\", \"tarif_nr\", \"tarif_bez\", \n",
    "                      \"nutzfall_nr\", \"vg_inkasso\", \"inkasso_netto\", \"inkasso_brutto\", \"veranst_geschaeftszeichen\", \"veranst_name\",\n",
    "                      \"veranst_strasse\", \"veranst_plz\", \"veranst_ort\", \"nutzliznehm_geschaeftszeichen\", \"nutzliznehm_name\", \"nutzliznehm_vorname\",\n",
    "                      \"nutzliznehm_strasse\", \"nutzliznehm_plz\", \"nutzliznehm_ort\", \"location\", \"band\", \"promoter\"], \n",
    "        \"missing (in million(s))\": [0, 0, 0, 3205313, 0, 0, 0, 0, 222, 155, 0, 151, 2104917, 1477728, 0, 0, 0, 0, 2933130, 2933130, 0, 121, 20393, \n",
    "                    1225, 697, 0, 1038426, 2941456, 1052479, 1038887, 1038426, 151, 1453088, 121],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "missing_data_in_df.plot.bar(x=\"Attributes\", ax= ax)\n",
    "ax.legend([\"Empty values(in million(s))\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class distribution plot  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# fads = allclaims_df.copy()\n",
    "# fads.rename(columns = {'test':'TEST'}, inplace = True)\n",
    "\n",
    "allclaims_df.VERANST_SEGMENT.value_counts().plot(ax=ax, kind='bar', xlabel='Event Segments', ylabel='Frequency')\n",
    "# ax.legend([\"2.0 - Class 0(0-50€)\", \"3.0 - Class 1(50€-100€)\", \"4.0 - Class 2(>100€)\"])\n",
    "# ax.legend([\"3.0 - Class 1(50€-100€)\"])\n",
    "# ax.legend([\"4.0 - Class 2(>100€)\"])\n",
    "ax.legend(['Frequency of classes'])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove missing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove missing values from VG_ORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allclaims_df = allclaims_df[allclaims_df['VG_ORT'].isnull()==False]\n",
    "len(allclaims_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove missing values from BAND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allclaims_df = allclaims_df[allclaims_df['BAND'].isnull()==False]\n",
    "len(allclaims_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove missing values from PROMOTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allclaims_df = allclaims_df[allclaims_df['PROMOTER'].isnull()==False]\n",
    "len(allclaims_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unique locations: ',len(allclaims_df.VG_ORT.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMP_ID                                 \n",
    "# GJ                               GeschäftsJahr      \n",
    "# IMPORT                           Distribution where the data came from      \n",
    "# MUFO_REFERENZ_N                  \n",
    "# BARCODE_NR                             \n",
    "# VERANST_SEGMENT                  Event segment\n",
    "# REKLA_JN                         Reclamation \n",
    "\n",
    "# VG_DATUM_VON                     Event Date\n",
    "# VG_ORT                           Event place\n",
    "# VG_RAUM                          Event room\n",
    "\n",
    "# NUTZFALL                         Usage of event  \n",
    "# NUTZFALL_RAUM                    Usage Room or Music hall where the music is used\n",
    "\n",
    "# MUSIKLEITER_NAME                 Music Leader name\n",
    "# KAPELLE_NAME                     Chapel name\n",
    "\n",
    "# TARIF_NR                         Tariff Number\n",
    "# TARIF_BEZ                        Tariff Bez\n",
    "# NUTZFALL_NR                      Usecase Number\n",
    "\n",
    "# VG_INKASSO                       Event collection\n",
    "# INKASSO_NETTO                    Net-Collection\n",
    "# INKASSO_BRUTTO                   Gross-Collection\n",
    "# VERANST_GESCHAEFTSZEICHEN        Event business sign or mark \n",
    "# VERANST_NAME                     Event name\n",
    "# VERANST_STRASSE                  Event street\n",
    "# VERANST_PLZ                      Event post code\n",
    "# VERANST_ORT                      Event place\n",
    "\n",
    "# # Nutzungs Lizenznehmer - someone who has got the license(Usage licence) for the music works\n",
    "# NUTZLIZNEHM_GESCHAEFTSZEICHEN    \n",
    "# NUTZLIZNEHM_NAME                 name of person who aquired usage license\n",
    "# NUTZLIZNEHM_VORNAME              surname of person who aquired usage license\n",
    "# NUTZLIZNEHM_STRASSE              street of person who aquired usage license\n",
    "# NUTZLIZNEHM_PLZ                  postcode of person who aquired usage license\n",
    "# NUTZLIZNEHM_ORT                  place of person who aquired usage license\n",
    "\n",
    "# LOCATION                         location - VG_ORT + VG_RAUM\n",
    "# BAND                             band - KAPELLE_NAME and empty rows of kapelle name is filled with MUSIKLEITER_NAME\n",
    "# PROMOTER                         promoter - VERANST_NAME + VERANST_PLZ\n",
    "\n",
    "# VG_RAUM = 'IM FREIEN'(In Outside) or Name of the City\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping German Cities and States from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the webpage\n",
    "r = requests.get(\"https://en.wikipedia.org/wiki/List_of_cities_and_towns_in_Germany\")\n",
    "\n",
    "# Convert the webpage content to soup object\n",
    "webpage = bs(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove special characters and convert to uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_char_convert_to_uppercase(text):\n",
    "    each = text.replace('ß','SS')\n",
    "    each = each.upper()\n",
    "    each = each.replace('Ä', 'AE')\n",
    "    each = each.replace('Ö', 'OE')\n",
    "    each = each.replace('Ü', 'UE')\n",
    "    return each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_names = [\"City\", \"State\"]\n",
    "l = []\n",
    "for i in range(0,25):\n",
    "    table = webpage.select(\"table\")[i]\n",
    "    list = table.select(\"li\")\n",
    "    for c in list:\n",
    "        each = remove_special_char_convert_to_uppercase(c.get_text(','))\n",
    "        texts = each.split(',')\n",
    "        city = texts[0]\n",
    "        if city == 'MUNICH': city = 'MUENCHEN'\n",
    "        if city == 'COLOGNE': city = 'KOELN' \n",
    "        if city == 'NUREMBERG': city = 'NUERNBERG'\n",
    "        if city == 'HANOVER': city = 'HANNOVER'\n",
    "         \n",
    "        state = texts[1].replace('(','').replace(')','')\n",
    "        l.append([city , state])\n",
    "print(l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_states_de_df = pd.DataFrame(l, columns=c_names)\n",
    "cities_states_de_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_states_de_df[cities_states_de_df['City']=='KOELN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VG_ORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some VG_ORT values contains of format-1:<cityname, specific region name>\n",
    "# Replace all the places containing of format-1 with cityname\n",
    "allPlaces=allclaims_df['VG_ORT'].tolist()\n",
    "place_indicies_with_comma = [i for i in range(len(allPlaces)) if ',' in allPlaces[i]]\n",
    "\n",
    "iter_index = 0\n",
    "for each_index in allclaims_df.index[allclaims_df['VG_ORT'].str.contains(',') == True].tolist():\n",
    "    cityname = allclaims_df.at[each_index, 'VG_ORT'].split(r\",\")[0]\n",
    "    allPlaces[place_indicies_with_comma[iter_index]] = cityname\n",
    "    iter_index += 1\n",
    "\n",
    "allclaims_df['VG_ORT'] = pd.Categorical(allPlaces, ordered = False)\n",
    "\n",
    "len(allclaims_df[allclaims_df['VG_ORT'].str.contains(',') == True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching the state for each city or town (location) in VG_ORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allclaims_df['vg_state'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in cities_states_de_df.itertuples() :\n",
    "    city = each.City\n",
    "    state = each.State\n",
    "    allclaims_df.loc[allclaims_df['VG_ORT'] == city, 'vg_state'] = state.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Municipality and states in Germany that are scraped from WikiData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_municipality_df = pd.read_csv('C:/Saravana/Projects/Intellizenz/intellizenz-model-training/data/submunicipality_municipality_district_state_germany_v3.csv')\n",
    "wiki_municipality_df[['stateLabel','municipalityLabel','submunicipalityLabel']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in wiki_municipality_df.itertuples() :\n",
    "    municipality = remove_special_char_convert_to_uppercase(each.municipalityLabel)\n",
    "    state = remove_special_char_convert_to_uppercase(each.stateLabel)\n",
    "    submunicipality = remove_special_char_convert_to_uppercase(str(each.submunicipalityLabel))\n",
    "    allclaims_df.loc[allclaims_df['VG_ORT'] == municipality, 'vg_state'] = state.strip()\n",
    "    allclaims_df.loc[allclaims_df['VG_ORT'] == submunicipality, 'vg_state'] = state.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the empty values in vg_state column\n",
    "empty_vg_state = allclaims_df[allclaims_df['vg_state'] == '']\n",
    "print(len(empty_vg_state))\n",
    "print('Unique : {}'.format(len(empty_vg_state.VG_ORT.unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove empty values from vg_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allclaims_df = allclaims_df[allclaims_df['vg_state'] != '']\n",
    "len(allclaims_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list_vg_state = [item for item in allclaims_df['vg_state']]\n",
    "fdist_vg_state = FreqDist(flat_list_vg_state)\n",
    "fdist_vg_state.plot(cumulative=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the percentage of events in different states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_categories_df = allclaims_df['vg_state'].value_counts(normalize=True)\n",
    "state_categories_df = state_categories_df.mul(100).rename('Percent').reset_index()\n",
    "state_categories_df.rename(columns = {'index':'State'}, inplace = True)\n",
    "\n",
    "g = sns.catplot(x='State', y='Percent', kind='bar', data=state_categories_df)\n",
    "g.ax.tick_params(axis='x', rotation=90)\n",
    "\n",
    "for p in g.ax.patches:\n",
    "    txt = str(p.get_height().round(1)) + '%'\n",
    "    txt_x = p.get_x()\n",
    "    txt_y = p.get_height()\n",
    "    g.ax.text(txt_x,txt_y,txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retreive the states, where event takes place - Featurize vg_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states = [i[0] for i in fdist_vg_state.items()] \n",
    "df_featurize_vg_state = pd.DataFrame(allclaims_df['vg_state'])\n",
    "\n",
    "for term in all_states :\n",
    "    df_featurize_vg_state['state_'+term.lower()]=df_featurize_vg_state['vg_state'].apply(lambda x: 1 if term in x else 0)\n",
    "\n",
    "display(df_featurize_vg_state.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurize TARIF_BEZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unique Tarif: ', len(allclaims_df['TARIF_BEZ'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list_tarif_desc = [item for item in allclaims_df['TARIF_BEZ']]\n",
    "fdist_tarif_desc = FreqDist(flat_list_tarif_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_terms_tarif = [i[0] for i in fdist_tarif_desc.most_common(30)] \n",
    "df_featurize_tarif = pd.DataFrame(allclaims_df['TARIF_BEZ'])\n",
    "\n",
    "for term in most_common_terms_tarif :\n",
    "    df_featurize_tarif['tarif_'+term.lower()]=df_featurize_tarif['TARIF_BEZ'].apply(lambda x: 1 if term in x else 0)\n",
    "\n",
    "display(df_featurize_tarif.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_tarif_desc.plot(30,cumulative=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group the frequent tariffs and rest of the tarifs to 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_featurize_tarif['grouped_tarif']=df_featurize_tarif['TARIF_BEZ'].apply(lambda x: 'Selected Tariffs' if x in most_common_terms_tarif else 'Other')\n",
    "\n",
    "tarif_categories_df = df_featurize_tarif['grouped_tarif'].value_counts(normalize=True)\n",
    "tarif_categories_df = tarif_categories_df.mul(100).rename('Percent').reset_index()\n",
    "tarif_categories_df.rename(columns = {'index':'Tarif'}, inplace = True)\n",
    "\n",
    "g = sns.catplot(x='Tarif', y='Percent', kind='bar', data=tarif_categories_df)\n",
    "\n",
    "for p in g.ax.patches:\n",
    "    txt = str(p.get_height().round(1)) + '%'\n",
    "    txt_x = p.get_x()\n",
    "    txt_y = p.get_height()\n",
    "    g.ax.text(txt_x,txt_y,txt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anonymization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faker = Faker(['de_DE'])\n",
    "Faker.seed(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize anonymized Band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list_fake_bands = [item for item in allclaims_df['anonymized_band']]\n",
    "fdist_fake_band = FreqDist(flat_list_fake_bands)\n",
    "fdist_fake_band.plot(30,cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurize Promoter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Promoter values; promoter_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_featurize_promoter = pd.DataFrame(allclaims_df['PROMOTER'])\n",
    "df_featurize_promoter[\"promoter_clean\"] = allclaims_df['VERANST_NAME']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove repeating company types from 'Promoter'. \n",
    "#### Get the frequencies of unique promoter entries until total of 1742197 counts(len of dataframe) is reached. Set remaining promoter entries to 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeating_org_names(column,threshold=0.75,substrings=[],return_categories_list=True):\n",
    "  #Find the threshold value using the percentage and number of instances in the column\n",
    "  threshold_value=int(threshold*len(column))\n",
    "  #Initialise an empty list for our new minimised categories\n",
    "  categories_list=[]\n",
    "  #Initialise a variable to calculate the sum of frequencies\n",
    "  s=0\n",
    "  #Create a counter dictionary of the form unique_value: frequency\n",
    "  counts=Counter(column)\n",
    "\n",
    "  # Loop through the category name and its corresponding frequency after sorting the categories by descending order of frequency\n",
    "  for i,j in counts.most_common():\n",
    "    #Add the frequency to the global sum\n",
    "    s+=dict(counts)[i]\n",
    "    category_name = i\n",
    "\n",
    "    for substr in substrings:\n",
    "      category_name = category_name.replace('K. D. OE. R','K.D.OE.R')\n",
    "      if category_name.count(substr) == 2:\n",
    "          category_name = category_name.replace(substr,'X',1) # replace 1st occurance of the string with X\n",
    "          category_name = category_name.replace(substr,'').strip() # replace 2st occurance of the string with empty\n",
    "          category_name = category_name.replace('X', substr) # replace X with substring value\n",
    "\n",
    "    #Append the category name to the list\n",
    "    categories_list.append(category_name)\n",
    "    #Check if the global sum has reached the threshold value, if so break the loop\n",
    "    if s>=threshold_value:\n",
    "      break\n",
    "\n",
    "  # Append the category Other to the list\n",
    "  categories_list.append('Other')\n",
    "\n",
    "  #Replace all instances not in our new categories by Other  \n",
    "  new_column=column.apply(lambda x: x if x in categories_list else 'Other')\n",
    "\n",
    "  \n",
    "  #Return transformed column and unique values if return_categories=True\n",
    "  if(return_categories_list):\n",
    "    return new_column,categories_list\n",
    "  #Return only the transformed column if return_categories=False\n",
    "  else:\n",
    "    return new_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_strings = ['GMBH & CO. KG', 'E.V', 'GMBH', 'GBR', 'K.D.OE.R', 'OHG']\n",
    "tran_new_column,new_cat_list=remove_repeating_org_names(df_featurize_promoter['promoter_clean'],threshold=1.00,substrings=org_strings,return_categories_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_new_column.describe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the Promoter column to remove repetitive company name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the repeating organisation(company types) substring from Organizer/Promoter\n",
    "def transform_promoter(x, substrings):\n",
    "    str_value = x\n",
    "    str_value = str_value.replace('K. D. OE. R','K.D.OE.R')\n",
    "\n",
    "    # y = lambda substrings, str_value: (subs if(str_value.count(subs)==2) else ''  for subs in substrings)\n",
    "    # result_sub_str = y(substrings, str_value)\n",
    "\n",
    "    for subs in substrings:\n",
    "        if str_value.count(subs) == 2:\n",
    "            str_value = str_value.replace(subs,'X',1) # replace 1st occurance of the string with X\n",
    "            str_value = str_value.replace(subs,'').strip() # replace 2st occurance of the string with empty\n",
    "            str_value = str_value.replace('X', subs) # replace X with substring value\n",
    "            return str_value\n",
    "        else:\n",
    "            return str_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orgs = ['GMBH & CO. KG', 'E.V', 'GMBH', 'GBR', 'K.D.OE.R', 'OHG']\n",
    "\n",
    "df_featurize_promoter['promoter_transform'] = df_featurize_promoter.apply(lambda x: transform_promoter(x['promoter_clean'], substrings=[orgs[0]]), axis=1)\n",
    "df_featurize_promoter['promoter_transform'] = df_featurize_promoter.apply(lambda x: transform_promoter(x['promoter_transform'], substrings=[orgs[1]]), axis=1)\n",
    "df_featurize_promoter['promoter_transform'] = df_featurize_promoter.apply(lambda x: transform_promoter(x['promoter_transform'], substrings=[orgs[2]]), axis=1)\n",
    "df_featurize_promoter['promoter_transform'] = df_featurize_promoter.apply(lambda x: transform_promoter(x['promoter_transform'], substrings=[orgs[3]]), axis=1)\n",
    "df_featurize_promoter['promoter_transform'] = df_featurize_promoter.apply(lambda x: transform_promoter(x['promoter_transform'], substrings=[orgs[4]]), axis=1)\n",
    "df_featurize_promoter['promoter_transform'] = df_featurize_promoter.apply(lambda x: transform_promoter(x['promoter_transform'], substrings=[orgs[5]]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allclaims_df['promoter_transform'] = df_featurize_promoter['promoter_transform']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anonymize Promoter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_promoters = {promoter: faker.unique.company() for promoter in allclaims_df['promoter_transform'].unique()}\n",
    "allclaims_df['anonymized_promoter'] = allclaims_df['promoter_transform'].map(dict_promoters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the distribution of promoter and Anonymized promoter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Actual unique promoters: ', len(allclaims_df['promoter_transform'].unique()))\n",
    "print('Anonymized unique promoters: ', len(allclaims_df['anonymized_promoter'].unique()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Anonymized Promoter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list_fake_promoters = [item for item in allclaims_df['anonymized_promoter']]\n",
    "fdist_fake_promoter = FreqDist(flat_list_fake_promoters)\n",
    "fdist_fake_promoter.plot(30,cumulative=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Actual Promoter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flat_list_actual_promoters = [item for item in df_featurize_promoter['promoter_transform']]\n",
    "# fdist_actual_promoter = FreqDist(flat_list_actual_promoters)\n",
    "# fdist_actual_promoter.plot(30,cumulative=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group the frequent promoters and rest of the promoters to 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_common_terms_promoter = [i[0] for i in fdist_actual_promoter.most_common(30)]\n",
    "most_common_terms_promoter = [i[0] for i in fdist_fake_promoter.most_common(30)]\n",
    "\n",
    "\n",
    "df_featurize_promoter['grouped_promoter']=df_featurize_promoter['promoter_transform'].apply(lambda x: 'Selected Promoters' if x in most_common_terms_promoter else 'Other')\n",
    "\n",
    "promoter_categories_df = df_featurize_promoter['grouped_promoter'].value_counts(normalize=True)\n",
    "promoter_categories_df = promoter_categories_df.mul(100).rename('Percent').reset_index()\n",
    "promoter_categories_df.rename(columns = {'index':'Promoter'}, inplace = True)\n",
    "\n",
    "g = sns.catplot(x='Promoter', y='Percent', kind='bar', data=promoter_categories_df)\n",
    "\n",
    "for p in g.ax.patches:\n",
    "    txt = str(p.get_height().round(1)) + '%'\n",
    "    txt_x = p.get_x()\n",
    "    txt_y = p.get_height()\n",
    "    g.ax.text(txt_x,txt_y,txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition = df_featurize_promoter['promoter_clean']!='Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_featurize_promoter['anonymized_promoter'] = allclaims_df['anonymized_promoter'] \n",
    "df_featurize_promoter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flat_list_clean_promoter = [item for item in df_featurize_promoter[condition]['promoter_clean']]\n",
    "flat_list_clean_promoter = [item for item in df_featurize_promoter['anonymized_promoter']]\n",
    "fdist_clean_promoter = FreqDist(flat_list_clean_promoter)\n",
    "most_common_terms_clean_promoter = [i[0] for i in fdist_clean_promoter.most_common(30)] \n",
    "\n",
    "\n",
    "for term in most_common_terms_clean_promoter :\n",
    "    # df_featurize_promoter['promoter_'+term.lower()]=df_featurize_promoter['promoter_clean'].apply(lambda x: 1 if term in str(x) else 0)\n",
    "    df_featurize_promoter['promoter_'+term.lower()]=df_featurize_promoter['anonymized_promoter'].apply(lambda x: 1 if term in str(x) else 0)\n",
    "\n",
    "display(df_featurize_promoter.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_clean_promoter.plot(30,cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google maps API\n",
    "# how big the city is?\n",
    "# how close the location to the city centre?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('C:/Users/sgopalakrish/Miniconda3/Lib/site-packages/de_core_news_sm/de_core_news_sm-3.4.0/')\n",
    "nlp_en = spacy.load('C:/Users/sgopalakrish/Miniconda3/Lib/site-packages/en_core_web_sm/en_core_web_sm-3.4.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_stop_words = nlp.Defaults.stop_words\n",
    "english_stop_words = nlp_en.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_chars_from(stopwords):\n",
    "    cleaned_stop_words = []\n",
    "    for each in stopwords:\n",
    "        each = each.replace('ß','SS')\n",
    "        each = each.upper()\n",
    "        each = each.replace('Ä', 'AE')\n",
    "        each = each.replace('Ö', 'OE')\n",
    "        each = each.replace('Ü', 'UE')\n",
    "        \n",
    "        cleaned_stop_words.append(each)\n",
    "    return cleaned_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_de_stopwords = remove_special_chars_from(german_stop_words)\n",
    "\n",
    "additional_stopwords = [\n",
    "    'ST', 'FREIEN', 'BAD', 'HAUS', 'EV', 'BERLIN', 'KATH', 'S', 'HOF', 'ALTE', 'MITTE', 'LUTH', 'MUENCHEN',\n",
    "    'IRISH', 'MUSIK', 'KULTUR', 'FUER', 'EVANG', 'MARITIM', 'KOELN', 'U', 'TURN', 'E', 'STUTTGART', 'ALTES',\n",
    "    'A', 'GASTES', 'THE', 'EUROPA', 'HANNOVER', 'STADT', 'BADEN', 'NUERNBERG', 'HAMBURG', 'NEUE',\n",
    "    'EVANGELISCHE', 'LEIPZIG', 'B', 'DRESDEN', 'BREMEN', 'PETER', '1','ALTER', 'AM', 'DIE', 'DER',\n",
    "    'DAS', 'DES', 'DEN', 'DEM', 'EIN', 'EINER', 'EINEM', 'EINES', 'EINE',\n",
    "    'MEIN', 'MEINER', 'MEINES', 'MEINEM', 'MEINE', 'UND'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stop_words = []\n",
    "for each in english_stop_words:\n",
    "    en_stop_words.append(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords = cleaned_de_stopwords + additional_stopwords + en_stop_words\n",
    "all_stopwords = [x.lower() for x in all_stopwords]\n",
    "# all_stopwords = list(map(lambda x: x.lower(), all_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the categories of VG_RAUM\n",
    "allclaims_df['VG_RAUM_clean']=allclaims_df['VG_RAUM'].astype(str).fillna('').map(lambda x: re.sub(r'\\W+', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "room_list_wo_stopwords = []\n",
    "for each_room in allclaims_df['VG_RAUM_clean'].tolist():\n",
    "    for each_stopword in all_stopwords:\n",
    "        # Remove stopword from each row in VG_RAUM_clean \n",
    "        each_room.replace(each_stopword,'')\n",
    "        \n",
    "    if 'KIRCHE' in each_room:\n",
    "        room_list_wo_stopwords.append('KIRCHE')\n",
    "    else:\n",
    "        room_list_wo_stopwords.append(each_room)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "room_list_wo_stopwords = [x.lower() for x in room_list_wo_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allclaims_df['VG_RAUM_WO_STOPWORDS'] = room_list_wo_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming - removes suffixes and prefixes from word roots, \n",
    "# Lemmatization - maps the remaining root forms (which may not always be proper words) back to an actual word that occurs in natural language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get keywords using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(text):\n",
    "    result = []\n",
    "    pos_tag = ['PROPN', 'NOUN', 'VERB']\n",
    "    for token in text:\n",
    "        if(token.text in all_stopwords or token.text in punctuation):\n",
    "            continue\n",
    "        if(token.pos_ in pos_tag):\n",
    "            result.append(token.text) \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raum_cleaned_df = (row.VG_RAUM_WO_STOPWORDS for row in allclaims_df.itertuples())\n",
    "\n",
    "vg_raum_keywords = []\n",
    "for each_object in nlp.pipe(raum_cleaned_df):\n",
    "  vg_raum_keywords.append(get_keywords(each_object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allclaims_df['VG_RAUM_KEYWORDS'] = vg_raum_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurize VG_RAUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list_vg_raum_keywords = [item for sublist in allclaims_df['VG_RAUM_KEYWORDS'] for item in sublist]\n",
    "fdist_vg_raum_keywords = FreqDist(flat_list_vg_raum_keywords)\n",
    "most_common_terms_vg_raum = [i[0] for i in fdist_vg_raum_keywords.most_common(30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_featurize_vg_raum_keywords = pd.DataFrame(allclaims_df['VG_RAUM_KEYWORDS'])\n",
    "\n",
    "for term in most_common_terms_vg_raum :\n",
    "    df_featurize_vg_raum_keywords['place_'+term]=df_featurize_vg_raum_keywords['VG_RAUM_KEYWORDS'].apply(lambda x: 1 if term in x else 0)\n",
    "\n",
    "display(df_featurize_vg_raum_keywords.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_vg_raum_keywords.plot(30,cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(allclaims_df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group the frequent venues and rest of the venues to 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_common_terms_vg_raum = [i[0] for i in fdist_vg_raum_keywords.most_common(200)]\n",
    "\n",
    "list_venue_wo_stopwords = [item for item in allclaims_df['VG_RAUM_WO_STOPWORDS']]\n",
    "fdist_venue = FreqDist(list_venue_wo_stopwords)\n",
    "most_common_terms_vg_raum = [i[0] for i in fdist_venue.most_common(2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for term in most_common_terms_vg_raum :\n",
    "    # df_featurize_vg_raum_keywords['grouped_venue']=df_featurize_vg_raum_keywords['VG_RAUM_KEYWORDS'].apply(lambda x: 'Selected Venues' if term in x else 'Other')\n",
    "\n",
    "df_featurize_vg_raum_keywords['grouped_venue']=allclaims_df['VG_RAUM_WO_STOPWORDS'].apply(lambda x: 'Selected Venues' if x in most_common_terms_vg_raum else 'Other') \n",
    "\n",
    "\n",
    "venue_categories_df = df_featurize_vg_raum_keywords['grouped_venue'].value_counts(normalize=True)\n",
    "venue_categories_df = venue_categories_df.mul(100).rename('Percent').reset_index()\n",
    "venue_categories_df.rename(columns = {'index':'Venue'}, inplace = True)\n",
    "\n",
    "g = sns.catplot(x='Venue', y='Percent', kind='bar', data=venue_categories_df)\n",
    "\n",
    "for p in g.ax.patches:\n",
    "    txt = str(p.get_height().round(1)) + '%'\n",
    "    txt_x = p.get_x()\n",
    "    txt_y = p.get_height()\n",
    "    g.ax.text(txt_x,txt_y,txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurize BAND"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anonymized BAND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_bands = {band: faker.unique.company() for band in allclaims_df['BAND'].unique()}\n",
    "allclaims_df['anonymized_band'] = allclaims_df['BAND'].map(dict_bands)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the distribution of actual band & anonymized band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Actual unique bands: ', len(allclaims_df['BAND'].unique()))\n",
    "print('Anonymized unique bands: ', len(allclaims_df['anonymized_band'].unique()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize anonymized Band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list_fake_bands = [item for item in allclaims_df['anonymized_band']]\n",
    "fdist_fake_band = FreqDist(flat_list_fake_bands)\n",
    "fdist_fake_band.plot(30,cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flat_list_band = [item for item in allclaims_df['BAND']]\n",
    "flat_list_band = [item for item in allclaims_df['anonymized_band']]\n",
    "fdist_band_desc = FreqDist(flat_list_band)\n",
    "most_common_terms_band = [i[0] for i in fdist_band_desc.most_common(30)] \n",
    "# df_featurize_band = pd.DataFrame(allclaims_df['BAND'])\n",
    "df_featurize_band = pd.DataFrame(allclaims_df['anonymized_band'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term in most_common_terms_band :\n",
    "    # df_featurize_band['band_'+term.lower()]=df_featurize_band['BAND'].apply(lambda x: 1 if term in x else 0)\n",
    "    df_featurize_band['band_'+term.lower()]=df_featurize_band['anonymized_band'].apply(lambda x: 1 if term in x else 0)\n",
    "\n",
    "display(df_featurize_band.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_band_desc.plot(30,cumulative=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group the frequent bands and rest of the bands to 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_terms_band = [i[0] for i in fdist_band_desc.most_common(50)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_featurize_band['grouped_band']=allclaims_df['BAND'].apply(lambda x: 'Selected Bands' if x in most_common_terms_band else 'Other')\n",
    "\n",
    "band_categories_df = df_featurize_band['grouped_band'].value_counts(normalize=True)\n",
    "band_categories_df = band_categories_df.mul(100).rename('Percent').reset_index()\n",
    "band_categories_df.rename(columns = {'index':'Band'}, inplace = True)\n",
    "\n",
    "g = sns.catplot(x='Band', y='Percent', kind='bar', data=band_categories_df)\n",
    "\n",
    "for p in g.ax.patches:\n",
    "    txt = str(p.get_height().round(1)) + '%'\n",
    "    txt_x = p.get_x()\n",
    "    txt_y = p.get_height()\n",
    "    g.ax.text(txt_x,txt_y,txt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VERANST_SEGMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the VERANST_SEGMENT values from float to Int\n",
    "allclaims_df.VERANST_SEGMENT = allclaims_df['VERANST_SEGMENT'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "allclaims_df['VERANST_SEGMENT'] = le.fit_transform(allclaims_df['VERANST_SEGMENT'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VG_DATUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allclaims_df['VG_DATUM_YEAR'] = le.fit_transform(allclaims_df['VG_DATUM_VON'].dt.year)\n",
    "allclaims_df['VG_DATUM_MONTH'] = le.fit_transform(allclaims_df['VG_DATUM_VON'].dt.month)\n",
    "allclaims_df['VG_DATUM_DAY_OF_WEEK']= le.fit_transform(allclaims_df['VG_DATUM_VON'].dt.dayofweek)\n",
    "\n",
    "seasons = [0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 0]\n",
    "claim_season_list = [seasons[item] for item in allclaims_df['VG_DATUM_MONTH']]\n",
    "\n",
    "allclaims_df['VG_DATUM_SEASON'] = claim_season_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Heatmap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a new dataframe corr_df1, that contains only venue details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the list of keywords with most frequest keyword\n",
    "def transform_venue(x, venue):\n",
    "    venue_keywords = x\n",
    "\n",
    "    if venue in venue_keywords:\n",
    "        return venue\n",
    "    else:\n",
    "        return venue_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df1 = allclaims_df[['VG_RAUM_KEYWORDS']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venue_strs = []\n",
    "for keyword_list in corr_df1['VG_RAUM_KEYWORDS'].values:\n",
    "    if len(keyword_list) == 0:\n",
    "        venue_strs.append('')\n",
    "    else:\n",
    "        ad = ' '.join(e for e in keyword_list)\n",
    "        venue_strs.append(ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df1['venue'] = venue_strs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['kirche', 'hotel', 'cafe', 'theater', 'club', 'halle', 'gaststaette', 'festhalle', 'kulturzentrum', 'festzelt', \n",
    "# 'schloss', 'pub', 'stadthalle', 'park', 'gasthof', 'kabarett', 'arena', 'schlachthof', 'wandelhalle', 'turnhalle', \n",
    "# 'buergerhaus', 'museum', 'rathaus', 'staatsbad', 'zelt', 'jazz', 'forum', 'gymnasium', 'schule', 'sporthalle']\n",
    "\n",
    "corr_df1['venue_clean'] = corr_df1.apply(lambda x: transform_venue(x['venue'], 'kirche'), axis=1)\n",
    "corr_df1['venue_clean'] = corr_df1.apply(lambda x: transform_venue(x['venue_clean'], 'hotel'), axis=1)\n",
    "corr_df1['venue_clean'] = corr_df1.apply(lambda x: transform_venue(x['venue_clean'], 'cafe'), axis=1)\n",
    "corr_df1['venue_clean'] = corr_df1.apply(lambda x: transform_venue(x['venue_clean'], 'theater'), axis=1)\n",
    "corr_df1['venue_clean'] = corr_df1.apply(lambda x: transform_venue(x['venue_clean'], 'club'), axis=1)\n",
    "corr_df1['venue_clean'] = corr_df1.apply(lambda x: transform_venue(x['venue_clean'], 'halle'), axis=1)\n",
    "corr_df1['venue_clean'] = corr_df1.apply(lambda x: transform_venue(x['venue_clean'], 'gaststaette'), axis=1)\n",
    "corr_df1['venue_clean'] = corr_df1.apply(lambda x: transform_venue(x['venue_clean'], 'festhalle'), axis=1)\n",
    "corr_df1['venue_clean'] = corr_df1.apply(lambda x: transform_venue(x['venue_clean'], 'kulturzentrum'), axis=1)\n",
    "corr_df1['venue_clean'] = corr_df1.apply(lambda x: transform_venue(x['venue_clean'], 'festzelt'), axis=1)\n",
    "corr_df1['venue_clean'] = corr_df1.apply(lambda x: transform_venue(x['venue_clean'], 'schloss'), axis=1)\n",
    "corr_df1['venue_clean'] = corr_df1.apply(lambda x: transform_venue(x['venue_clean'], 'pub'), axis=1)\n",
    "corr_df1['venue_clean'] = corr_df1.apply(lambda x: transform_venue(x['venue_clean'], 'stadthalle'), axis=1)\n",
    "corr_df1['venue_clean'] = corr_df1.apply(lambda x: transform_venue(x['venue_clean'], 'park'), axis=1)\n",
    "corr_df1['venue_clean'] = corr_df1.apply(lambda x: transform_venue(x['venue_clean'], 'gasthof'), axis=1)\n",
    "corr_df1['venue_clean'] = corr_df1.apply(lambda x: transform_venue(x['venue_clean'], 'kabarett'), axis=1)\n",
    "corr_df1['venue_clean'] = corr_df1.apply(lambda x: transform_venue(x['venue_clean'], 'arena'), axis=1)\n",
    "corr_df1['venue_clean'] = corr_df1.apply(lambda x: transform_venue(x['venue_clean'], 'schlachthof'), axis=1)\n",
    "corr_df1['venue_clean'] = corr_df1.apply(lambda x: transform_venue(x['venue_clean'], 'wandelhalle'), axis=1)\n",
    "corr_df1['venue_clean'] = corr_df1.apply(lambda x: transform_venue(x['venue_clean'], 'turnhalle'), axis=1)\n",
    "corr_df1['venue_clean'] = corr_df1.apply(lambda x: transform_venue(x['venue_clean'], 'buergerhaus'), axis=1)\n",
    "corr_df1['venue_clean'] = corr_df1.apply(lambda x: transform_venue(x['venue_clean'], 'museum'), axis=1)\n",
    "corr_df1['venue_clean'] = corr_df1.apply(lambda x: transform_venue(x['venue_clean'], 'rathaus'), axis=1)\n",
    "corr_df1['venue_clean'] = corr_df1.apply(lambda x: transform_venue(x['venue_clean'], 'staatsbad'), axis=1)\n",
    "corr_df1['venue_clean'] = corr_df1.apply(lambda x: transform_venue(x['venue_clean'], 'zelt'), axis=1)\n",
    "corr_df1['venue_clean'] = corr_df1.apply(lambda x: transform_venue(x['venue_clean'], 'jazz'), axis=1)\n",
    "corr_df1['venue_clean'] = corr_df1.apply(lambda x: transform_venue(x['venue_clean'], 'forum'), axis=1)\n",
    "corr_df1['venue_clean'] = corr_df1.apply(lambda x: transform_venue(x['venue_clean'], 'gymnasium'), axis=1)\n",
    "corr_df1['venue_clean'] = corr_df1.apply(lambda x: transform_venue(x['venue_clean'], 'schule'), axis=1)\n",
    "corr_df1['venue_clean'] = corr_df1.apply(lambda x: transform_venue(x['venue_clean'], 'sporthalle'), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the selected features into a new dataframe corr_df, to visualize the correlation heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_columns = ['VG_DATUM_YEAR','VG_DATUM_MONTH', 'VG_DATUM_DAY_OF_WEEK', 'VG_DATUM_SEASON', 'VERANST_SEGMENT', \n",
    "            'BAND', 'TARIF_BEZ', 'VG_RAUM_KEYWORDS', 'promoter_transform', 'vg_state']\n",
    "\n",
    "\n",
    "corr_df = allclaims_df[feat_columns].copy()\n",
    "corr_df['BAND'] = le.fit_transform(corr_df['BAND'])\n",
    "corr_df['TARIF_BEZ'] = le.fit_transform(corr_df['TARIF_BEZ'])\n",
    "corr_df['promoter_transform'] = le.fit_transform(corr_df['promoter_transform'])\n",
    "corr_df['vg_state'] = le.fit_transform(corr_df['vg_state'])\n",
    "\n",
    "corr_df['venue'] = corr_df1['venue_clean'].values \n",
    "corr_df['venue'] = le.fit_transform(corr_df['venue'])\n",
    "\n",
    "feat_columns.append('venue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df=corr_df.rename(str.lower, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between features\n",
    "lower_feat_columns = [each.lower() for each in feat_columns]\n",
    "\n",
    "corr = corr_df[lower_feat_columns].corr()\n",
    "f, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "# Configure a custom diverging colormap\n",
    "# cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "sns.heatmap(corr, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allclaims_feature_df = allclaims_df.merge(df_featurize_vg_raum_keywords, how='left', on='ID')\n",
    "allclaims_feature_df = allclaims_feature_df.merge(df_featurize_tarif, how='left', on='ID')\n",
    "allclaims_feature_df = allclaims_feature_df.merge(df_featurize_vg_state, how='left', on='ID')\n",
    "allclaims_feature_df = allclaims_feature_df.merge(df_featurize_band, how='left', on='ID')\n",
    "allclaims_feature_df = allclaims_feature_df.merge(df_featurize_promoter, how='left', on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allclaims_feature_df=allclaims_feature_df.rename(str.lower, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(allclaims_feature_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allclaims_feature_df.to_pickle('./data/export_features_2016_2020_v1.pkl.bz2', protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ArrowTypeError: (\"Expected bytes, got a 'float' object\", 'Conversion failed for column nutzliznehm_plz with type object')\n",
    "allclaims_feature_df = allclaims_feature_df.drop('nutzliznehm_plz', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allclaims_feature_df.to_parquet('./data/export_features_2016_2020_v2.parquet.gzip',compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allclaims_feature_df.to_parquet('./data/export_features_2016_2020_v3.parquet.gzip',compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Essential anonymized columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anony_columns = ['VG_RAUM_KEYWORDS', 'VG_RAUM_WO_STOPWORDS', 'VG_RAUM', 'anonymized_band', 'anonymized_promoter',\n",
    "'TARIF_BEZ', 'vg_state', 'VG_ORT',\n",
    "'VG_DATUM_SEASON', 'VG_DATUM_MONTH', 'VG_DATUM_DAY_OF_WEEK', 'VG_DATUM_YEAR',\n",
    "'VG_DATUM_VON', 'VERANST_SEGMENT', 'VG_INKASSO']\n",
    "anonymized_essential_df =  allclaims_df[anony_columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anonymized_essential_df=anonymized_essential_df.rename(str.lower, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anonymized_essential_df.to_parquet('./data/export_anonymized_features_2016_2020.parquet.gzip',compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anony_df = pd.read_parquet('C:/Users/sgopalakrish/Downloads/intellizenz-model-training/data/export_anonymized_features_2016_2020.parquet.gzip')\n",
    "display(anony_df.head())\n",
    "print(anony_df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract venue keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raum_keywords(text):\n",
    "    result = []\n",
    "    pos_tag = ['PROPN', 'ADV', 'NOUN', 'VERB', 'ORG', 'PER']\n",
    "    for token in text:\n",
    "        print(token.pos_)\n",
    "        if(token.text in all_stopwords or token.text in punctuation):\n",
    "            continue\n",
    "        if(token.pos_ in pos_tag):\n",
    "            result.append(token.text) \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raum_clean = (row.vg_raum_wo_stopwords for row in anony_df.itertuples())\n",
    "\n",
    "raum_keywords = []\n",
    "for each_object in nlp.pipe(raum_clean):\n",
    "  raum_keywords.append(get_raum_keywords(each_object))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['kirche', 'hotel', 'cafe', 'theater', 'club', 'halle', 'gaststaette', 'festhalle', 'kulturzentrum', 'festzelt', \n",
    "# 'schloss', 'pub', 'stadthalle', 'park', 'gasthof', 'kabarett', 'arena', 'schlachthof', 'wandelhalle', 'turnhalle', \n",
    "# 'buergerhaus', 'museum', 'rathaus', 'staatsbad', 'zelt', 'jazz', 'forum', 'gymnasium', 'schule', 'sporthalle']\n",
    "anony_df['vg_raum_new_keywords'] = raum_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list_raum_keywords = [item for sublist in anony_df['vg_raum_new_keywords'] for item in sublist]\n",
    "fdist_raum_keywordss = FreqDist(flat_list_raum_keywords)\n",
    "most_common_terms_raum = [i[0] for i in fdist_raum_keywordss.most_common(100)]\n",
    "print(most_common_terms_raum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the list of keywords with most frequest keyword\n",
    "def transform_venue_with_count(x, venue):\n",
    "    venue_keywords = x\n",
    "    venue_keywords_list = venue_keywords.split(' ')\n",
    "    \n",
    "    if venue_keywords_list.count(venue) > 0:\n",
    "        return venue\n",
    "    # if venue in venue_keywords:\n",
    "    #         return venue\n",
    "    else:\n",
    "        return venue_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new 'venue'\n",
    "venue_keyword_strs = []\n",
    "for keyword_list in anony_df['vg_raum_new_keywords'].values:\n",
    "    if len(keyword_list) == 0:\n",
    "        venue_keyword_strs.append('')\n",
    "    else:\n",
    "        ad = ' '.join(e for e in keyword_list)\n",
    "        venue_keyword_strs.append(ad)\n",
    "\n",
    "anony_df['venue'] = venue_keyword_strs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '' empty venues with vg_raum_wo_stopwords values\n",
    "empty_venue_condition = anony_df['venue'] == ''\n",
    "anony_df.loc[empty_venue_condition, 'venue'] = anony_df[empty_venue_condition]['vg_raum_wo_stopwords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hundred_common_venues = most_common_terms_raum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common venues without specific location\n",
    "eighty_seven_common_venues = ['kirche', 'hotel', 'cafe', 'theater', 'club', 'halle', \n",
    "'kulturzentrum', 'gaststaette', 'buergerhaus', 'festhalle', 'stadthalle', 'festzelt', \n",
    "'schloss', 'pub', 'restaurant', 'gasthaus', 'bar', 'kurhaus', \n",
    "'kulturhaus', 'kabarett', 'rathaus', 'arena', 'gasthof', 'park', \n",
    "'wandelhalle', 'schlachthof', 'turnhalle', 'staatsbad', 'zelt', 'mehrzweckhalle', \n",
    "'museum', 'zentrum', 'forum', 'gymnasium', 'gemeindehalle', \n",
    "'saal', 'grundschule', 'sporthalle', 'musikschule', 'schule', 'gemeindehaus', \n",
    "'circus', 'jugendzentrum', 'haus des gastes', 'dorfgemeinschaftshaus', 'fabrik', 'landgasthof', \n",
    "'live', 'gop', 'messe', 'hofbraeuhaus', 'schuetzenhaus', 'bereich', 'jazzclub', \n",
    "'jazz', 'buergerzentrum', 'burg', 'center', 'sachs', 'galerie', \n",
    "'kurpark', 'weingut', 'wirtshaus', 'werk', 'brauhaus', 'freizeitzentrum', \n",
    "'bistro', 'feierwerk', 'backstage', 'ms', 'stadttheater', 'kulturcafe', 'buergersaal', 'sport', 'villa', \n",
    "'bahnhof', 'sportheim', 'brauerei', 'kulturfabrik', 'jugend', \n",
    "'kantine', 'music', 'parkhotel', 'scheune', 'woerishofen', 'markthalle', 'knust']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anony_df['venue_clean'] = anony_df.apply(lambda x: transform_venue_with_count(x['venue'], 'kirche'), axis=1)\n",
    "\n",
    "for i in range(1,len(hundred_common_venues)):\n",
    "    venue = hundred_common_venues[i]\n",
    "    anony_df['venue_clean'] = anony_df.apply(lambda x: transform_venue_with_count(x['venue_clean'], venue), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anonymize Venue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_venues = {venue: faker.unique.street_name() if venue not in eighty_seven_common_venues else venue for venue in anony_df['venue_clean'].unique()}\n",
    "anony_df['anonymized_venue'] = anony_df['venue_clean'].map(dict_venues)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the distribution of actual venue & anonymized venue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Actual unique venue: ', len(anony_df['venue_clean'].unique()))\n",
    "print('Anonymized unique venue: ', len(anony_df['anonymized_venue'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(anony_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anony_essential_columns = ['anonymized_band', 'anonymized_promoter', 'anonymized_venue',\n",
    "# 'tarif_bez', 'vg_state', 'vg_ort',\n",
    "# 'vg_datum_season', 'vg_datum_month', 'vg_datum_day_of_week', 'vg_datum_year',\n",
    "# 'vg_datum_von', 'veranst_segment', 'vg_inkasso']\n",
    "anony_essential_columns = ['anonymized_band', 'anonymized_promoter', 'anonymized_venue',\n",
    "'tarif_bez', 'vg_state',\n",
    "'vg_datum_season', 'vg_datum_month', 'vg_datum_day_of_week', 'vg_datum_year',\n",
    "'vg_datum_von', 'veranst_segment', 'vg_inkasso']\n",
    "anony_essential_df =  anony_df[anony_essential_columns].copy()\n",
    "display(anony_essential_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anony_essential_df.to_parquet('./data/export_anonymized_features_2016_2020.parquet.gzip',compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encoded_df = pd.read_parquet('C:/Users/sgopalakrish/Downloads/intellizenz-model-training/data/export_features_2016_2020_v5.parquet.gzip')\n",
    "display(target_encoded_df.head())\n",
    "print(target_encoded_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline SVC Classifier models to classify event segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = allclaims_feature_df[features]\n",
    "y = allclaims_feature_df['veranst_segment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(allclaims_df['VG_RAUM_KEYWORDS'].dtypes)\n",
    "print(allclaims_df['VG_DATUM_VON'].dtypes)\n",
    "print(allclaims_df['VG_ORT'].dtypes)\n",
    "print(allclaims_df['TARIF_BEZ'].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "xgboost_classifier = xgboost.XGBClassifier()\n",
    "xgboost_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "xbg_filename = 'xgb_classifier_model.sav'\n",
    "pickle.dump(xgboost_classifier, open(xbg_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgboost_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "xgboost_accuracy = accuracy_score(y_test, y_pred, normalize=False)\n",
    "print(xgboost_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'svc_classifier_model.sav'\n",
    "pickle.dump(svclassifier, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svclassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "svc_accuracy = accuracy_score(y_test, y_pred, normalize=False)\n",
    "print(svc_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intellizenz-model-training-zEgBkwlM-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a4b28c995b80fd745e361acd60072fe9d8810b0685f9e510d186f16e2eae93e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
