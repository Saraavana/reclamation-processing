{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-bargain",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T20:28:04.563300Z",
     "start_time": "2021-05-19T20:28:04.478677Z"
    }
   },
   "outputs": [],
   "source": [
    "# Enable autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30b8590",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T20:28:18.228912Z",
     "start_time": "2021-05-19T20:28:04.563300Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from scipy.special import logit, expit\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "pd.set_option('max_columns',None)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-girlfriend",
   "metadata": {},
   "source": [
    "# Import pickle file with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-confidentiality",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T20:29:51.647471Z",
     "start_time": "2021-05-19T20:28:18.367036Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./data/export_features_2016_2018.pkl.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcd0bbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T20:30:12.072960Z",
     "start_time": "2021-05-19T20:29:51.663047Z"
    }
   },
   "outputs": [],
   "source": [
    "df_orig = df.copy() # save all data for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809abb24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T20:30:15.724207Z",
     "start_time": "2021-05-19T20:30:12.104195Z"
    }
   },
   "outputs": [],
   "source": [
    "df=df_orig # get all data back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfb22c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T20:30:15.902657Z",
     "start_time": "2021-05-19T20:30:15.724207Z"
    }
   },
   "outputs": [],
   "source": [
    "all_features = ['location_count', 'location_mean', 'location_std',\n",
    " 'location_min', 'location_5%', 'location_10%', 'location_15%',\n",
    " 'location_20%', 'location_25%', 'location_30%', 'location_35%',\n",
    " 'location_40%', 'location_45%', 'location_50%', 'location_55%',\n",
    " 'location_60%', 'location_65%', 'location_70%', 'location_75%',\n",
    " 'location_80%', 'location_85%', 'location_90%', 'location_95%',\n",
    " 'location_max', \n",
    "                \n",
    " 'band_count', 'band_mean', 'band_std',\n",
    " 'band_min', 'band_5%', 'band_10%', 'band_15%', 'band_20%',\n",
    " 'band_25%', 'band_30%', 'band_35%', 'band_40%', 'band_45%',\n",
    " 'band_50%', 'band_55%', 'band_60%', 'band_65%', 'band_70%',\n",
    " 'band_75%', 'band_80%', 'band_85%', 'band_90%', 'band_95%',\n",
    " 'band_max', \n",
    "                \n",
    " 'promoter_count', 'promoter_mean', 'promoter_std',\n",
    " 'promoter_min', 'promoter_5%', 'promoter_10%', 'promoter_15%',\n",
    " 'promoter_20%', 'promoter_25%', 'promoter_30%', 'promoter_35%',\n",
    " 'promoter_40%', 'promoter_45%', 'promoter_50%', 'promoter_55%',\n",
    " 'promoter_60%', 'promoter_65%', 'promoter_70%', 'promoter_75%',\n",
    " 'promoter_80%', 'promoter_85%', 'promoter_90%', 'promoter_95%',\n",
    " 'promoter_max', \n",
    " \n",
    " 'vg_datum_year', 'vg_datum_month', 'vg_datum_day_of_week',\n",
    "                \n",
    " 'location_kirche', 'location_theater', 'location_hotel', 'location_cafe',\n",
    " 'location_stadthalle', 'location_buergerhaus', 'location_club', 'location_gaststaette',\n",
    " 'location_halle', 'location_schloss', 'location_festhalle', 'location_musikschule',\n",
    " 'location_restaurant', 'location_kulturzentrum', 'location_kurhaus',\n",
    " 'location_festzelt', 'location_mehrzweckhalle', 'location_pub',\n",
    " 'location_gasthaus', 'location_bar', 'location_turnhalle', 'location_klinik',\n",
    " 'location_gymnasium', 'location_kulturhaus', 'location_rathaus', 'location_gasthof',\n",
    " 'location_park', 'location_schuetzenhalle', 'location_hochschule', 'location_gemeindehalle',\n",
    "                \n",
    " 'band_musikverein', 'band_band', 'band_mv', 'band_duo', 'band_trio', 'band_musikkapelle',\n",
    " 'band_chor', 'band_blaskapelle', 'band_stadtkapelle', 'band_gbr', 'band_orchester',\n",
    " 'band_jazz', 'band_blasorchester', 'band_original', 'band_partyband', 'band_kurorchester',\n",
    " 'band_friends', 'band_ensemble', 'band_blues', 'band_ev', 'band_swing', 'band_live',\n",
    " 'band_musikzug', 'band_solo', 'band_sound', 'band_jugendkapelle', 'band_alleinunterhalter',\n",
    " 'band_musikanten', 'band_harmonie', 'band_spielmannszug']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d3ba39",
   "metadata": {},
   "source": [
    "Drop segments 1 and 13, since data in those segments is not relevant for us\n",
    "\n",
    "Drop data before 2014, since data before 2014 is noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df1d6f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T20:30:58.777635Z",
     "start_time": "2021-05-19T20:30:15.902657Z"
    }
   },
   "outputs": [],
   "source": [
    "df=df.loc[~df.amount_segment.isin([1,13]),['amount', 'amount_segment']+all_features]\n",
    "df = df[df['vg_datum_year'] >= 2014]\n",
    "df=df.dropna(subset=['amount_segment'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20bd83b",
   "metadata": {},
   "source": [
    "# Get sample of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e68a6de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T19:59:02.495238Z",
     "start_time": "2021-05-14T19:58:58.081690Z"
    }
   },
   "outputs": [],
   "source": [
    "#Get a sample of the DataFrame \n",
    "splitSample = StratifiedShuffleSplit(n_splits=1, test_size=0.01, random_state=42)\n",
    "\n",
    "for train_idx, test_idx in splitSample.split(df[all_features], df.amount_segment):\n",
    "    df_sample=df.iloc[test_idx]\n",
    "    \n",
    "    plt.figure()\n",
    "    df.amount_segment.astype(int).value_counts().sort_index().plot.bar(color='r')\n",
    "    df_sample.amount_segment.astype(int).value_counts().sort_index().plot.bar(color='g')\n",
    "\n",
    "    plt.title('Inkasso-Segment')\n",
    "    plt.legend(['Full DF', 'Sample DF'])\n",
    "    plt.show()\n",
    "\n",
    "df=df_sample.copy()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58865c6b",
   "metadata": {},
   "source": [
    "# Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7411f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T20:30:59.310083Z",
     "start_time": "2021-05-19T20:30:58.777635Z"
    }
   },
   "outputs": [],
   "source": [
    "#Classification\n",
    "clf1 = xgb.XGBClassifier(n_estimators=500, max_depth=7, use_label_encoder=False, objective='binary:logistic',eval_metric = 'error')\n",
    "clf2 = xgb.XGBClassifier(n_estimators=500, max_depth=7, use_label_encoder=False, objective='binary:logistic',eval_metric = 'error')\n",
    "\n",
    "#Regression\n",
    "reg2 = xgb.XGBRegressor(n_estimators=700, max_depth=7, min_child_weight=5, objective='reg:squarederror')\n",
    "reg2_logit = xgb.XGBRegressor(n_estimators=700, max_depth=7, min_child_weight=5, objective='reg:squarederror')\n",
    "\n",
    "reg3 = xgb.XGBRegressor(n_estimators=700, max_depth=7, min_child_weight=5, objective='reg:squarederror')\n",
    "reg3_logit = xgb.XGBRegressor(n_estimators=700, max_depth=7, min_child_weight=5, objective='reg:squarederror')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2874563",
   "metadata": {},
   "source": [
    "# Prepare DataFrame for Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f119200b",
   "metadata": {},
   "source": [
    "We keep only features + 'amount' and 'amount_segment' columns.\n",
    "\n",
    "Encode segments for Classifier 1:\n",
    "* Variable y_clf1 \n",
    "* Positive class (seg 4+) y_clf1=1\n",
    "* Negative class (seg 2 or 3) y_clf1=0\n",
    "\n",
    "Encode segments for Classifier 2: \n",
    "* Variable y_clf2\n",
    "* Positive class (seg 3) y_clf2=1\n",
    "* Negative class (seg 2) y_clf2=0\n",
    "* Segments >3 are \"encoded\" as np.NaN. These NaN values will be dropped before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06a280b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T20:31:00.629010Z",
     "start_time": "2021-05-19T20:30:59.310083Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[:,'y_clf1']=(df.amount_segment.astype(int) > 3).values.astype(int)\n",
    "df.loc[:,'y_clf2']=df.amount_segment.apply(lambda x: 1 if x==3 else (0 if x==2 else np.nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94889d0",
   "metadata": {},
   "source": [
    "# Train and Test Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f862071",
   "metadata": {},
   "source": [
    "In oder to have larger train and test sets, we use crossvalidation-like approach to verify the model performance.\n",
    "1. We split the dataset into 5 folds with StratifiedKFold. The criteria for split is to have the same proportion of data based on the segment in each fold.\n",
    "2. In each fold we train and test both classifiers independently from each other\n",
    "3. We save the predict_proba results from both classifiers\n",
    "4. Train and test iteration numbers are also saved (in case we want to evaluate the results based on the train/test iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d25b35c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T21:10:49.193415Z",
     "start_time": "2021-05-19T20:31:00.629010Z"
    }
   },
   "outputs": [],
   "source": [
    "#skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_model_eval = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "df['y_pred_proba_clf1']=np.NaN\n",
    "df['y_pred_proba_clf2']=np.NaN\n",
    "df[\"y_pred_reg2\"]=np.NaN\n",
    "df[\"y_pred_reg2_logit\"]=np.NaN\n",
    "df[\"y_pred_reg3\"]=np.NaN\n",
    "df[\"y_pred_reg3_logit\"]=np.NaN\n",
    "\n",
    "df['train_iter']=np.NaN\n",
    "df['test_iter']=np.NaN\n",
    "\n",
    "iter_nr=0\n",
    "\n",
    "for train_idx, test_idx in tqdm(cv_model_eval.split(df[all_features], df.amount_segment), total=cv_model_eval.n_splits):\n",
    "    \n",
    "    df_train=df.iloc[train_idx]\n",
    "    df_test=df.iloc[test_idx]\n",
    "    \n",
    "    #Test dataset is the same for all models\n",
    "    X_test = df_test[all_features]\n",
    "    y_test_clf1=df_test.y_clf1\n",
    "    \n",
    "    X_train_clf1 = df_train[all_features]\n",
    "    y_train_clf1 = df_train.y_clf1\n",
    "\n",
    "    # For Classifier2 (clf2) we use data from segments 2 and 3 (so we drop records where y_clf2 is NaN)\n",
    "    df_train_clf2 = df_train.dropna(subset=['y_clf2'])\n",
    "\n",
    "    X_train_clf2 = df_train_clf2[all_features]\n",
    "    y_train_clf2 = df_train_clf2.y_clf2\n",
    "    \n",
    "    y_test_clf2=df_test.y_clf2\n",
    "    \n",
    "    # For Regression seg2 use only Segment 2 and amount 0.00001>=amount>=49.9999 (because of logit)\n",
    "    df_train_reg2 = df_train[(df_train['amount_segment']==2) & \n",
    "                             (df_train['amount']>=0.00001) & \n",
    "                             (df_train['amount']<=49.9999)]#.dropna().drop_duplicates()\n",
    "\n",
    "    X_train_reg2 = df_train_reg2[all_features]\n",
    "    y_train_reg2 = df_train_reg2.amount\n",
    "    y_train_reg2_logit = (y_train_reg2/50).apply(logit)\n",
    "    \n",
    "    \n",
    "    # For Regression seg3 use only Segment 3 and amount 50>amount>=99.9999 (because of logit)\n",
    "    df_train_reg3 = df_train[(df_train['amount_segment']==3) & \n",
    "                             (df_train['amount']>50) & \n",
    "                             (df_train['amount']<=99.9999)]#.dropna().drop_duplicates()\n",
    "\n",
    "    X_train_reg3 = df_train_reg3[all_features]\n",
    "    y_train_reg3 = df_train_reg3.amount\n",
    "    y_train_reg3_logit = ((y_train_reg3-50)/50).apply(logit)\n",
    "    \n",
    "        \n",
    "  \n",
    "    #Fit and test the models\n",
    "    \n",
    "    #Classifier 1\n",
    "    clf1.fit(X_train_clf1, y_train_clf1)\n",
    "    y_pred_proba_clf1 = clf1.predict_proba(X_test)[:, 1]\n",
    "    print(\"CLF1 Train Error: {}\".format(clf1.score(X_train_clf1, y_train_clf1)))\n",
    "    print(\"CLF1 Test Error: {}\".format(clf1.score(X_test, y_test_clf1)))\n",
    "    \n",
    "    #Classifier 2        \n",
    "    clf2.fit(X_train_clf2, y_train_clf2)\n",
    "    y_pred_proba_clf2 = clf2.predict_proba(X_test)[:, 1]\n",
    "    print(\"CLF2 Train Error: {}\".format(clf2.score(X_train_clf2, y_train_clf2)))\n",
    "    print(\"CLF2 Test Error: {}\".format(clf2.score(df_test.dropna(subset=['y_clf2'])[all_features], df_test.dropna(subset=['y_clf2']).y_clf2)))\n",
    "    \n",
    "    \n",
    "    #Regression Segment 2\n",
    "    reg2.fit(X_train_reg2, y_train_reg2)\n",
    "    y_pred_reg2=reg2.predict(X_test)\n",
    "    print(\"REG2 Train Error: {}\".format(reg2.score(X_train_reg2, y_train_reg2)))\n",
    "    print(\"REG2 Test Error: {}\".format(reg2.score(df_test[df_test.amount_segment==2][all_features], df_test[df_test.amount_segment==2].amount)))\n",
    "\n",
    "    #Regression Segment 2 with logit transformation\n",
    "    reg2_logit.fit(X_train_reg2, y_train_reg2_logit)\n",
    "    y_pred_reg2_logit = reg2_logit.predict(X_test)\n",
    "    y_pred_reg2_logit_transf = pd.Series(y_pred_reg2_logit).apply(expit)*50\n",
    "    print(\"REG2_Logit Train Error: {}\".format(reg2_logit.score(X_train_reg2, y_train_reg2_logit)))\n",
    "    print(\"REG2_Logit Test Error: {}\".format(reg2_logit.score(df_test[(df_test.amount_segment==2)& \n",
    "                                                             (df_test.amount>=0.00001) & \n",
    "                                                             (df_test.amount<=49.9999)][all_features], \n",
    "                                                             (df_test[(df_test.amount_segment==2)& \n",
    "                                                             (df_test.amount>=0.00001) & \n",
    "                                                             (df_test.amount<=49.9999)\n",
    "                                                                     ].amount/50).apply(logit))))\n",
    "\n",
    "    \n",
    "    #Regression Segment 3\n",
    "    reg3.fit(X_train_reg3, y_train_reg3)\n",
    "    y_pred_reg3=reg3.predict(X_test)\n",
    "    print(\"REG3 Train Error: {}\".format(reg3.score(X_train_reg3, y_train_reg3)))\n",
    "    print(\"REG3 Test Error: {}\".format(reg3.score(df_test[df_test.amount_segment==3][all_features], df_test[df_test.amount_segment==3].amount)))\n",
    "\n",
    "\n",
    "    #Regression Segment 3 with logit transformation\n",
    "    reg3_logit.fit(X_train_reg3, y_train_reg3_logit)\n",
    "    y_pred_reg3_logit = reg3_logit.predict(X_test)\n",
    "    y_pred_reg3_logit_transf = pd.Series(y_pred_reg3_logit).apply(expit)*50+50\n",
    "    print(\"REG3_Logit Train Error: {}\".format(reg3_logit.score(X_train_reg3, y_train_reg3_logit)))\n",
    "    print(\"REG3_Logit Test Error: {}\".format(reg3_logit.score(df_test[(df_test['amount_segment']==3) & \n",
    "                                                                      (df_test['amount']>50) & \n",
    "                                                                      (df_test['amount']<=99.9999)][all_features], \n",
    "                                                        ((df_test[(df_test['amount_segment']==3) & \n",
    "                                                                  (df_test['amount']>50) & \n",
    "                                                                  (df_test['amount']<=99.9999)].amount-50)/50).apply(logit))))\n",
    "    \n",
    "    \n",
    "    #Save the prediction results in separate columns\n",
    "    df.iloc[test_idx,df.columns.get_loc(\"y_pred_proba_clf1\")]=y_pred_proba_clf1\n",
    "    df.iloc[test_idx,df.columns.get_loc(\"y_pred_proba_clf2\")]=y_pred_proba_clf2\n",
    "    df.iloc[test_idx,df.columns.get_loc(\"y_pred_reg2\")]=y_pred_reg2\n",
    "    df.iloc[test_idx,df.columns.get_loc(\"y_pred_reg2_logit\")]=y_pred_reg2_logit_transf\n",
    "    df.iloc[test_idx,df.columns.get_loc(\"y_pred_reg3\")]=y_pred_reg3\n",
    "    df.iloc[test_idx,df.columns.get_loc(\"y_pred_reg3_logit\")]=y_pred_reg3_logit_transf\n",
    "    \n",
    "    #Save train and test iteration number, in case we need it later \n",
    "    #(not sure if one record can be multiple times in train/test, anyway we save only the last iteration number for now...)\n",
    "    df.iloc[train_idx,df.columns.get_loc(\"train_iter\")]=iter_nr\n",
    "    df.iloc[test_idx,df.columns.get_loc(\"test_iter\")]=iter_nr\n",
    "    \n",
    "    iter_nr=iter_nr+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59179a9",
   "metadata": {},
   "source": [
    "Check if all records were used in train/test (was not the case with StratifiedShuffleSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2fcd71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:10:24.689640Z",
     "start_time": "2021-05-14T20:10:24.473428Z"
    }
   },
   "outputs": [],
   "source": [
    "df[['train_iter', 'test_iter']].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc8c055",
   "metadata": {},
   "source": [
    "Visualisation of train/test split for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9427131a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:10:27.805638Z",
     "start_time": "2021-05-14T20:10:26.564311Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_cv_indices(cv, X, y, ax, n_splits, lw=10):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
    "                   c=indices, marker='_', lw=lw, cmap=cmap_cv,\n",
    "                   vmin=-.2, vmax=1.2)\n",
    "\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits))\n",
    "    ax.set(yticks=np.arange(n_splits) + .5, yticklabels=yticklabels,\n",
    "           xlabel='Sample index', ylabel=\"CV iteration\",\n",
    "           ylim=[n_splits, -.2])\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "    return ax\n",
    "\n",
    "cmap_data = plt.cm.Paired\n",
    "cmap_cv = plt.cm.coolwarm\n",
    "fig, ax = plt.subplots()\n",
    "plot_cv_indices(cv_model_eval, df[all_features], df.amount_segment, ax, cv_model_eval.n_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615741c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T09:21:39.848295Z",
     "start_time": "2021-05-04T09:21:39.138359Z"
    }
   },
   "source": [
    "# Threshold optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f11968",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:10:31.868707Z",
     "start_time": "2021-05-14T20:10:31.695838Z"
    }
   },
   "outputs": [],
   "source": [
    "df['y_pred_proba_clf1']=df['y_pred_proba_clf1'].apply(lambda x: format(float(x),\".8f\")).astype(float)\n",
    "df['y_pred_proba_clf2']=df['y_pred_proba_clf2'].apply(lambda x: format(float(x),\".8f\")).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a485f02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:10:34.367767Z",
     "start_time": "2021-05-14T20:10:34.258238Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_treasholds = TimeSeriesSplit(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1f8f3d",
   "metadata": {},
   "source": [
    "## Threshold optimization (clf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47801cda",
   "metadata": {},
   "source": [
    "We optimize only t_neg threshold since for clf1 only positive class is of importance. We want to prevent classification of segments 4+ as segments 2 or 3 since this will mean \"lost money\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11d1424",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:25:29.882055Z",
     "start_time": "2021-05-14T20:10:39.089595Z"
    }
   },
   "outputs": [],
   "source": [
    "#Define Thresholds (t_neg and t_pos) We want to minimize False Negative and False Positive\n",
    "#max_for_clf1 = 0.03 # False Omission Rate\n",
    "\n",
    "#opt_t_neg_list_clf1 = []\n",
    "\n",
    "max_for_clf1_list = [0.01, 0.03, 0.05, 0.07, 0.09]\n",
    "opt_t_neg_list_clf1 = pd.DataFrame()\n",
    "\n",
    "#Use all features, encode segment 4+ as positive class, otherwise negative\n",
    "X_train_clf1 = df[all_features]\n",
    "y_train_clf1= df.y_clf1\n",
    "\n",
    "for i in tqdm(range(5)):\n",
    "    # we use all data for threshold optimization\n",
    "    for train_idx, test_idx in tqdm(cv_treasholds.split(X_train_clf1, y_train_clf1), total=cv_treasholds.n_splits) : \n",
    "        \n",
    "        clf1.fit(X_train_clf1.iloc[train_idx], y_train_clf1.iloc[train_idx])\n",
    "        \n",
    "        y_test_pred_proba_clf1 = clf1.predict_proba(X_train_clf1.iloc[test_idx])[:, 1]\n",
    "        y_test_clf1 = y_train_clf1.iloc[test_idx]\n",
    "        \n",
    "        opt_t_neg_clf1 = 0\n",
    "        opt_fnr_clf1 = 0\n",
    "        \n",
    "        for max_for_clf1 in max_for_clf1_list :\n",
    "            for t_neg_clf1 in np.linspace(0, 1, 1001):\n",
    "                for_clf1 = (y_test_pred_proba_clf1[y_test_clf1 == 1] <= t_neg_clf1).sum() / (y_test_pred_proba_clf1 <= t_neg_clf1).sum()\n",
    "                if for_clf1 > max_for_clf1:\n",
    "                    break\n",
    "                opt_t_neg_clf1 = t_neg_clf1\n",
    "                opt_for_clf1 = for_clf1\n",
    "\n",
    "            print(opt_t_neg_clf1, opt_for_clf1, max_for_clf1)\n",
    "            opt_t_neg_list_clf1 = opt_t_neg_list_clf1.append({\"max_for_clf1\": max_for_clf1, \"opt_t_neg_clf1\": opt_t_neg_clf1}, ignore_index=True)\n",
    "\n",
    "for max_for_clf1 in max_for_clf1_list :\n",
    "    print('max_for_clf1 = {}'.format(max_for_clf1))\n",
    "    display(opt_t_neg_list_clf1[opt_t_neg_list_clf1.max_for_clf1==max_for_clf1].opt_t_neg_clf1.describe())\n",
    "\n",
    "classifier_thresholds_clf1 = pd.DataFrame()\n",
    "for max_for_clf1 in max_for_clf1_list :\n",
    "    #t_neg = min(opt_t_neg_list)\n",
    "    t_neg_clf1 = np.median(opt_t_neg_list_clf1[opt_t_neg_list_clf1.max_for_clf1==max_for_clf1].opt_t_neg_clf1)\n",
    "\n",
    "    classifier_thresholds_clf1 = classifier_thresholds_clf1.append({\"max_for_clf1\": max_for_clf1,\n",
    "        \"t_neg_clf1\": t_neg_clf1}, ignore_index=True)\n",
    "\n",
    "display(classifier_thresholds_clf1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75e7170",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:29:12.116879Z",
     "start_time": "2021-05-14T20:29:04.310725Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_conf_matrix_clf1(t_neg):\n",
    "    df['y_pred_clf1']=df['y_pred_proba_clf1'].apply(lambda x: 1 if x >= t_neg else 0) \n",
    "    df_confusion_clf1 = pd.DataFrame(confusion_matrix(df.y_clf1, df.y_pred_clf1), index=['true neg (2,3)', 'true pos (4+)'], columns=['pred. neg (2,3)', 'pred. pos (4+)'])\n",
    "\n",
    "    return df_confusion_clf1\n",
    "\n",
    "correct_positive=[]\n",
    "correct_negative=[]\n",
    "false_positive=[]\n",
    "false_negative=[]\n",
    "\n",
    "\n",
    "for v in np.linspace(0, 1, 100):\n",
    "    result = create_conf_matrix_clf1(v)\n",
    "    \n",
    "    correct_positive.append(result.iloc[1,1]/df.shape[0])\n",
    "    correct_negative.append(result.iloc[0,0]/df.shape[0])\n",
    "    false_positive.append(result.iloc[0,1]/df.shape[0])    \n",
    "    false_negative.append(result.iloc[1,0]/df.shape[0])\n",
    "    \n",
    "    #print('Threshold = {}'.format(v))\n",
    "    #print(result)\n",
    "    \n",
    "x=np.linspace(0, 1, 100)\n",
    "plt.figure(figsize=(20,7))\n",
    "\n",
    "\n",
    "plt.stackplot(x,correct_positive, correct_negative, false_positive, false_negative, \n",
    "              labels=['Correct positive', \n",
    "                      'Correct negative',\n",
    "                      'False Positive (Extra work)', \n",
    "                      'False Negative (Lost money)'])\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4710b087",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T20:02:41.429791Z",
     "start_time": "2021-05-05T20:02:41.270744Z"
    }
   },
   "source": [
    "## Threshold optimization (clf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d340edc2",
   "metadata": {},
   "source": [
    "Both segment 2 (negative class) and segment 3 (positive class) are equally important --> we need both t_neg and t_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b64272",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:32:03.270479Z",
     "start_time": "2021-05-14T20:30:53.315247Z"
    }
   },
   "outputs": [],
   "source": [
    "#Define Thresholds (t_neg and t_pos) We want to minimize False Negative and False Positive\n",
    "#max_for_clf2 = 0.05 #False omission rate\n",
    "#max_fdr_clf2 = 0.05 #False discovery rate\n",
    "max_for_clf2_list = [0.01, 0.03, 0.05, 0.07, 0.09]\n",
    "max_fdr_clf2_list = [0.01, 0.03, 0.05, 0.07, 0.09]\n",
    "\n",
    "opt_t_neg_list_clf2 = pd.DataFrame()\n",
    "opt_t_pos_list_clf2 = pd.DataFrame()\n",
    "\n",
    "#Use only data records from seg. 2 and 3. Use all features. \n",
    "X_train_clf2 = df[~df.y_clf2.isnull()][all_features]\n",
    "#Encode segment 3 as 1 (positive class), seg. 2 as 0 (negative class)\n",
    "y_train_clf2= df[~df.y_clf2.isnull()].y_clf2\n",
    "\n",
    "for i in tqdm(range(5)):\n",
    "    for train_idx, test_idx in tqdm(cv_treasholds.split(X_train_clf2, y_train_clf2), total=cv_treasholds.n_splits) : \n",
    "        clf2.fit(X_train_clf2.iloc[train_idx], y_train_clf2.iloc[train_idx])\n",
    "        \n",
    "        y_test_pred_proba_clf2 = clf2.predict_proba(X_train_clf2.iloc[test_idx])[:, 1]\n",
    "        y_test_clf2 = y_train_clf2.iloc[test_idx]\n",
    "        \n",
    "        for max_for_clf2 in max_for_clf2_list :\n",
    "            opt_t_neg_clf2 = 0\n",
    "            opt_for_clf2 = 0\n",
    "            for t_neg_clf2 in np.linspace(0, 1, 1001):\n",
    "                for_clf2 = (y_test_pred_proba_clf2[y_test_clf2 == 1] <= t_neg_clf2).sum() / (y_test_pred_proba_clf2 <= t_neg_clf2).sum()\n",
    "                if for_clf2 > max_for_clf2:\n",
    "                    break\n",
    "                opt_t_neg_clf2 = t_neg_clf2\n",
    "                opt_for_clf2 = for_clf2\n",
    "\n",
    "            print(opt_t_neg_clf2, opt_for_clf2, max_for_clf2)\n",
    "            opt_t_neg_list_clf2 = opt_t_neg_list_clf2.append({\"max_for_clf2\": max_for_clf2, \"opt_t_neg_clf2\": opt_t_neg_clf2}, ignore_index=True)\n",
    "\n",
    "\n",
    "        for max_fdr_clf2 in max_fdr_clf2_list :\n",
    "            opt_t_pos_clf2 = 0\n",
    "            opt_fdr_clf2 = 0\n",
    "            for t_pos_clf2 in np.linspace(0, 1, 1001):\n",
    "                fdr_clf2 = (y_test_pred_proba_clf2[y_test_clf2 == 0] >= t_pos_clf2).sum() / (y_test_pred_proba_clf2 >= t_pos_clf2).sum()\n",
    "                opt_t_pos_clf2 = t_pos_clf2\n",
    "                opt_fdr_clf2 = fdr_clf2\n",
    "                if fdr_clf2 <= max_fdr_clf2:\n",
    "                    break\n",
    "            print(opt_t_pos_clf2, opt_fdr_clf2, max_fdr_clf2)\n",
    "            opt_t_pos_list_clf2 = opt_t_pos_list_clf2.append({\"max_fdr_clf2\": max_fdr_clf2, \"opt_t_pos_clf2\": opt_t_pos_clf2}, ignore_index=True)\n",
    "\n",
    "for max_for_clf2 in max_for_clf2_list :\n",
    "    print('max_for_clf2 = {}'.format(max_for_clf2))\n",
    "    display(opt_t_neg_list_clf2[opt_t_neg_list_clf2.max_for_clf2==max_for_clf2].opt_t_neg_clf2.describe())\n",
    "\n",
    "for max_fdr_clf2 in max_fdr_clf2_list :\n",
    "    print('max_fdr_clf2 = {}'.format(max_fdr_clf2))\n",
    "    display(opt_t_pos_list_clf2[opt_t_pos_list_clf2.max_fdr_clf2==max_fdr_clf2].opt_t_pos_clf2.describe())\n",
    "\n",
    "classifier_thresholds_clf2 = pd.DataFrame()\n",
    "\n",
    "for max_for_clf2 in max_for_clf2_list :\n",
    "    t_neg_clf2 = np.median(opt_t_neg_list_clf2[opt_t_neg_list_clf2.max_for_clf2==max_for_clf2].opt_t_neg_clf2)\n",
    "\n",
    "    classifier_thresholds_clf2 = classifier_thresholds_clf2.append({\"max_for_clf2\": max_for_clf2,\n",
    "        \"t_neg_clf2\": t_neg_clf2}, ignore_index=True)\n",
    "\n",
    "    \n",
    "for max_fdr_clf2 in max_fdr_clf2_list :\n",
    "    t_pos_clf2 = np.median(opt_t_pos_list_clf2)\n",
    "    t_pos_clf2 = np.median(opt_t_pos_list_clf2[opt_t_pos_list_clf2.max_fdr_clf2==max_fdr_clf2].opt_t_pos_clf2)\n",
    "\n",
    "    classifier_thresholds_clf2 = classifier_thresholds_clf2.append({\"max_fdr_clf2\": max_fdr_clf2,\n",
    "        \"t_pos_clf2\": t_pos_clf2}, ignore_index=True)\n",
    "    \n",
    "display(classifier_thresholds_clf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826070e6",
   "metadata": {},
   "source": [
    "# Create combined confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673c7d16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:36:56.159994Z",
     "start_time": "2021-05-14T20:36:55.601599Z"
    }
   },
   "outputs": [],
   "source": [
    "max_for_clf1 = 0.03\n",
    "max_for_clf2 = 0.05 #False omission rate\n",
    "max_fdr_clf2 = 0.05 #False discovery rate\n",
    "\n",
    "t_neg_clf1=classifier_thresholds_clf1[classifier_thresholds_clf1.max_for_clf1==max_for_clf1].t_neg_clf1.values[0]\n",
    "\n",
    "t_neg_clf2=classifier_thresholds_clf2[classifier_thresholds_clf2.max_for_clf2==max_for_clf2].t_neg_clf2.values[0]\n",
    "t_pos_clf2=classifier_thresholds_clf2[classifier_thresholds_clf2.max_fdr_clf2==max_fdr_clf2].t_pos_clf2.values[0]\n",
    "\n",
    "#t_neg_clf2=classifier_thresholds_clf2['t_neg_clf2']\n",
    "#t_pos_clf2=classifier_thresholds_clf2['t_pos_clf2']\n",
    "\n",
    "#remove entries with no predictions\n",
    "df=df.dropna(subset=['test_iter'])\n",
    "\n",
    "\n",
    "df['y']=df.amount_segment.apply(lambda x: '2' if x==2 else ('3' if x==3 else '4+'))\n",
    "\n",
    "df['y_pred_clf1']=df['y_pred_proba_clf1'].apply(lambda x: '2 or 3' if x <= t_neg_clf1 else '4+')\n",
    "df['y_pred_clf2']=df['y_pred_proba_clf2'].apply(lambda x: '2' if x<=t_neg_clf2 else \n",
    "                                                ('2?' if x<=0.5 else \n",
    "                                                 ('3?' if x<=t_pos_clf2 else '3')))\n",
    "\n",
    "df['y_pred']=df.apply(lambda x: x['y_pred_clf1'] if x['y_pred_clf1']=='4+' else x['y_pred_clf2'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cfec60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:37:08.517408Z",
     "start_time": "2021-05-14T20:37:08.301303Z"
    }
   },
   "outputs": [],
   "source": [
    "df[['amount_segment', 'y','y_pred_proba_clf1','y_pred_clf1', 'y_pred_proba_clf2', 'y_pred_clf2', 'y_pred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32700201",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:38:30.837824Z",
     "start_time": "2021-05-14T20:38:30.650438Z"
    }
   },
   "outputs": [],
   "source": [
    "df_confusion_comb = pd.DataFrame(confusion_matrix(df.y, df.y_pred, \n",
    "                                                  labels=['2','2?','3?','3','4+']), \n",
    "                                 index=['true 2', 'true 2?', 'true 3?', 'true 3', 'true 4+'], \n",
    "                                 columns=['pred 2', 'pred 2?', 'pred 3?', 'pred 3', 'pred 4+'])\n",
    "print('Abs. Numbers')\n",
    "df_confusion_comb.drop(index=['true 2?', 'true 3?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6585ff7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:38:34.200527Z",
     "start_time": "2021-05-14T20:38:34.020840Z"
    }
   },
   "outputs": [],
   "source": [
    "df_confusion_comb = pd.DataFrame(confusion_matrix(df.y, df.y_pred, \n",
    "                                                  labels=['2','2?','3?','3','4+'], normalize=\"true\"), \n",
    "                                 index=['true 2', 'true 2?', 'true 3?', 'true 3', 'true 4+'], \n",
    "                                 columns=['pred 2', 'pred 2?', 'pred 3?', 'pred 3', 'pred 4+'])\n",
    "print('Rel. to true count per class in %')\n",
    "(df_confusion_comb.drop(index=['true 2?', 'true 3?']) * 100).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552203d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:38:37.531146Z",
     "start_time": "2021-05-14T20:38:36.715799Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Classification Report')\n",
    "print(classification_report(df.y, df.y_pred, labels=['2','2?','3?','3','4+'],zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282a3d55",
   "metadata": {},
   "source": [
    "# Get scores for regressions (with correct class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d49723",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:38:39.695795Z",
     "start_time": "2021-05-14T20:38:39.513848Z"
    }
   },
   "outputs": [],
   "source": [
    "df_reg2 = df[df.amount_segment==2].copy()\n",
    "df_reg2['discr'] = df.amount-df.y_pred_reg2\n",
    "df_reg2['discr_abs'] = abs(df.amount-df.y_pred_reg2)\n",
    "df_reg2['discr_logit'] = df.amount-df.y_pred_reg2_logit\n",
    "df_reg2['discr_logit_abs'] = abs(df.amount-df.y_pred_reg2_logit)\n",
    "\n",
    "df_reg3 = df[df.amount_segment==3].copy()\n",
    "df_reg3['discr'] = df.amount-df.y_pred_reg3\n",
    "df_reg3['discr_abs'] = abs(df.amount-df.y_pred_reg3)\n",
    "df_reg3['discr_logit'] = df.amount-df.y_pred_reg3_logit\n",
    "df_reg3['discr_logit_abs'] = abs(df.amount-df.y_pred_reg3_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1354c0f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:39:24.527902Z",
     "start_time": "2021-05-14T20:39:24.372967Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Discrepancy reg2')\n",
    "print(df_reg2.discr.describe())\n",
    "print('')\n",
    "print('Absolute discrepancy reg2')\n",
    "print(df_reg2.discr_abs.describe())\n",
    "print('')\n",
    "print('Discrepancy reg2_logit')\n",
    "print(df_reg2.discr_logit.describe())\n",
    "print('')\n",
    "print('Absolute discrepancy reg2_logit')\n",
    "print(df_reg2.discr_logit_abs.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59fd5a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:39:38.582474Z",
     "start_time": "2021-05-14T20:39:38.440454Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Discrepancy reg3')\n",
    "print(df_reg3.discr.describe())\n",
    "print('')\n",
    "print('Absolute discrepancy reg3')\n",
    "print(df_reg3.discr_abs.describe())\n",
    "print('')\n",
    "print('Discrepancy reg3_logit')\n",
    "print(df_reg3.discr_logit.describe())\n",
    "print('')\n",
    "print('Absolute discrepancy reg3_logit')\n",
    "print(df_reg3.discr_logit_abs.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74586f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:39:46.753955Z",
     "start_time": "2021-05-14T20:39:45.708861Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(df_reg2.discr, bins=100)\n",
    "plt.show()\n",
    "plt.hist(df_reg2.discr_logit, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b2b443",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:39:54.787466Z",
     "start_time": "2021-05-14T20:39:54.036201Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(df_reg3.discr, bins=100)\n",
    "plt.show()\n",
    "plt.hist(df_reg3.discr_logit, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550b373a",
   "metadata": {},
   "source": [
    "# Regression Segment 2 and 3 (Review combined result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0ce03f",
   "metadata": {},
   "source": [
    "## Calculate discrepancy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f0bfb3",
   "metadata": {},
   "source": [
    "Keep Regression results only for correct classes (reg2 for '2' and '2?', reg3 for '3' and '3?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7322823",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:40:02.107543Z",
     "start_time": "2021-05-14T20:40:01.988023Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[df['y_pred'].isin(['3','3?','4+']),['y_pred_reg2','y_pred_reg2_logit']]=np.nan\n",
    "df.loc[df['y_pred'].isin(['2','2?','4+']),['y_pred_reg3','y_pred_reg3_logit']]=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41339ad8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:40:03.853534Z",
     "start_time": "2021-05-14T20:40:03.723787Z"
    }
   },
   "outputs": [],
   "source": [
    "df['discr_reg2']=df.amount-df.y_pred_reg2\n",
    "df['discr_reg2_abs']=abs(df.amount-df.y_pred_reg2)\n",
    "df['discr_reg2_logit']=df.amount-df.y_pred_reg2_logit\n",
    "df['discr_reg2_logit_abs']=abs(df.amount-df.y_pred_reg2_logit)\n",
    "df['discr_reg3']=df.amount-df.y_pred_reg3\n",
    "df['discr_reg3_abs']=abs(df.amount-df.y_pred_reg3)\n",
    "df['discr_reg3_logit']=df.amount-df.y_pred_reg3_logit\n",
    "df['discr_reg3_logit_abs']=abs(df.amount-df.y_pred_reg3_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21608483",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:40:09.219180Z",
     "start_time": "2021-05-14T20:40:09.079154Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[df.y_pred!='4+',['amount', 'amount_segment', \n",
    "    'y', 'y_pred', \n",
    "    'y_pred_reg2', 'discr_reg2', 'discr_reg2_abs', \n",
    "    'y_pred_reg2_logit', 'discr_reg2_logit',   'discr_reg2_logit_abs',\n",
    "    'y_pred_reg3', 'discr_reg3', 'discr_reg3_abs',\n",
    "    'y_pred_reg3_logit', 'discr_reg3_logit','discr_reg3_logit_abs'\n",
    "   ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c13e40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:40:15.915203Z",
     "start_time": "2021-05-14T20:40:15.742564Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Discrepancy reg2, for predicted 2 and 2?')\n",
    "print(df[df.y_pred.isin(['2','2?'])].discr_reg2.describe())\n",
    "print('')\n",
    "print('Discrepancy reg2, for predicted 2')\n",
    "print(df[df.y_pred.isin(['2'])].discr_reg2.describe())\n",
    "print('')\n",
    "print('Absolute discrepancy reg2, for predicted 2 and 2?')\n",
    "print(df[df.y_pred.isin(['2','2?'])].discr_reg2_abs.describe())\n",
    "print('')\n",
    "print('Absolute discrepancy reg2, for predicted 2')\n",
    "print(df[df.y_pred.isin(['2'])].discr_reg2_abs.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874f3a40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:40:21.003471Z",
     "start_time": "2021-05-14T20:40:20.851272Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Discrepancy reg3, for predicted 3 and 3?')\n",
    "print(df[df.y_pred.isin(['3','3?'])].discr_reg3.describe())\n",
    "print('')\n",
    "print('Discrepancy reg3, for predicted 3')\n",
    "print(df[df.y_pred.isin(['3'])].discr_reg3.describe())\n",
    "print('')\n",
    "print('Absolute discrepancy reg3, for predicted 3 and 3?')\n",
    "print(df[df.y_pred.isin(['3','3?'])].discr_reg3_abs.describe())\n",
    "print('')\n",
    "print('Absolute discrepancy reg3, for predicted 3')\n",
    "print(df[df.y_pred.isin(['3'])].discr_reg3_abs.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdc9eec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:40:24.854393Z",
     "start_time": "2021-05-14T20:40:23.434970Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(df[(df.y_pred.isin(['2']))].discr_reg2, bins=100)\n",
    "plt.show()\n",
    "plt.hist(df[(df.y_pred.isin(['2','2?']))].discr_reg2, bins=100)\n",
    "plt.show()\n",
    "\n",
    "plt.hist(df[(df.y_pred.isin(['2']))&(df.discr_reg2_abs<=100)].discr_reg2, bins=100)\n",
    "plt.show()\n",
    "plt.hist(df[(df.y_pred.isin(['2','2?']))&(df.discr_reg2_abs<=100)].discr_reg2, bins=100)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0ed408",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:40:28.349266Z",
     "start_time": "2021-05-14T20:40:27.001253Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(df[(df.y_pred.isin(['2']))].discr_reg2_logit, bins=100)\n",
    "plt.show()\n",
    "plt.hist(df[(df.y_pred.isin(['2','2?']))].discr_reg2_logit, bins=100)\n",
    "plt.show()\n",
    "\n",
    "plt.hist(df[(df.y_pred.isin(['2']))&(df.discr_reg2_logit_abs<=100)].discr_reg2_logit, bins=100)\n",
    "plt.show()\n",
    "plt.hist(df[(df.y_pred.isin(['2','2?']))&(df.discr_reg2_logit_abs<=100)].discr_reg2_logit, bins=100)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc99467",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:40:29.814384Z",
     "start_time": "2021-05-14T20:40:28.349266Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(df[(df.y_pred.isin(['3']))].discr_reg3, bins=100)\n",
    "plt.show()\n",
    "plt.hist(df[(df.y_pred.isin(['3', '3?']))].discr_reg3, bins=100)\n",
    "plt.show()\n",
    "\n",
    "plt.hist(df[(df.y_pred.isin(['3']))&(df.discr_reg3_abs<=150)].discr_reg3, bins=100)\n",
    "plt.show()\n",
    "plt.hist(df[(df.y_pred.isin(['3','3?']))&(df.discr_reg3_abs<=150)].discr_reg3, bins=100)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa66ed6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:40:31.369382Z",
     "start_time": "2021-05-14T20:40:29.816411Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(df[(df.y_pred.isin(['3']))].discr_reg3_logit, bins=100)\n",
    "plt.show()\n",
    "plt.hist(df[(df.y_pred.isin(['3', '3?']))].discr_reg3_logit, bins=100)\n",
    "plt.show()\n",
    "\n",
    "plt.hist(df[(df.y_pred.isin(['3']))&(df.discr_reg3_logit_abs<=150)].discr_reg3_logit, bins=100)\n",
    "plt.show()\n",
    "plt.hist(df[(df.y_pred.isin(['3','3?']))&(df.discr_reg3_logit_abs<=150)].discr_reg3_logit, bins=100)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00a5af1",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0738704a",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed304098",
   "metadata": {},
   "source": [
    "### CLF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a25bc90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:41:59.795330Z",
     "start_time": "2021-05-14T20:41:26.629583Z"
    }
   },
   "outputs": [],
   "source": [
    "explainer_clf1 = shap.TreeExplainer(clf1)\n",
    "shap_values_clf1 = explainer_clf1.shap_values(df[all_features])\n",
    "shap.summary_plot(shap_values_clf1, features=df[all_features], feature_names=df[all_features].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea19718c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:42:00.292465Z",
     "start_time": "2021-05-14T20:41:59.797347Z"
    }
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_clf1, features=df[all_features], feature_names=df[all_features].columns, plot_type='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa66f3fc",
   "metadata": {},
   "source": [
    "### Feature importance CLF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755f1423",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:42:18.520728Z",
     "start_time": "2021-05-14T20:42:00.294487Z"
    }
   },
   "outputs": [],
   "source": [
    "explainer_clf2 = shap.TreeExplainer(clf2)\n",
    "shap_values_clf2 = explainer_clf2.shap_values(df[~df.y_clf2.isnull()][all_features])\n",
    "shap.summary_plot(shap_values_clf2, features=df[~df.y_clf2.isnull()][all_features], feature_names=df[all_features].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33953e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:42:19.095060Z",
     "start_time": "2021-05-14T20:42:18.522747Z"
    }
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_clf2, features=df[~df.y_clf2.isnull()][all_features], feature_names=df[all_features].columns, plot_type='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2ddc0b",
   "metadata": {},
   "source": [
    "### Feature Importance Reg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f28dc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:42:47.537922Z",
     "start_time": "2021-05-14T20:42:19.097079Z"
    }
   },
   "outputs": [],
   "source": [
    "explainer_reg2 = shap.TreeExplainer(reg2)\n",
    "shap_values_reg2 = explainer_reg2.shap_values(df[df.amount_segment==2][all_features])\n",
    "shap.summary_plot(shap_values_reg2, features=df[df.amount_segment==2][all_features], feature_names=df[all_features].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3360af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:42:48.005661Z",
     "start_time": "2021-05-14T20:42:47.539971Z"
    }
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_reg2, features=df[df.amount_segment==2][all_features], feature_names=df[all_features].columns, plot_type='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6227af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:43:22.761567Z",
     "start_time": "2021-05-14T20:42:48.005661Z"
    }
   },
   "outputs": [],
   "source": [
    "explainer_reg2_logit = shap.TreeExplainer(reg2_logit)\n",
    "shap_values_reg2_logit = explainer_reg2_logit.shap_values(df[df.amount_segment==2][all_features])\n",
    "shap.summary_plot(shap_values_reg2_logit, features=df[df.amount_segment==2][all_features], feature_names=df[all_features].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67036edd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:43:23.405674Z",
     "start_time": "2021-05-14T20:43:22.763618Z"
    }
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_reg2_logit, features=df[df.amount_segment==2][all_features], feature_names=df[all_features].columns, plot_type='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d0411b",
   "metadata": {},
   "source": [
    "### Feature Importance reg3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a4837e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:43:37.515160Z",
     "start_time": "2021-05-14T20:43:23.407690Z"
    }
   },
   "outputs": [],
   "source": [
    "explainer_reg3 = shap.TreeExplainer(reg3)\n",
    "shap_values_reg3 = explainer_reg3.shap_values(df[df.amount_segment==3][all_features])\n",
    "shap.summary_plot(shap_values_reg3, features=df[df.amount_segment==3][all_features], feature_names=df[all_features].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9362d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:43:38.078943Z",
     "start_time": "2021-05-14T20:43:37.515160Z"
    }
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_reg3, features=df[df.amount_segment==3][all_features], feature_names=df[all_features].columns, plot_type='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727842ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:43:50.573213Z",
     "start_time": "2021-05-14T20:43:38.078943Z"
    }
   },
   "outputs": [],
   "source": [
    "explainer_reg3_logit = shap.TreeExplainer(reg3_logit)\n",
    "shap_values_reg3_logit = explainer_reg3_logit.shap_values(df[df.amount_segment==3][all_features])\n",
    "shap.summary_plot(shap_values_reg3_logit, features=df[df.amount_segment==3][all_features], feature_names=df[all_features].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00278974",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:43:51.560210Z",
     "start_time": "2021-05-14T20:43:50.573213Z"
    }
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_reg3_logit, features=df[df.amount_segment==3][all_features], feature_names=df[all_features].columns, plot_type='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f6de59",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5bfc96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T10:54:34.367423Z",
     "start_time": "2021-05-11T10:54:33.571871Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_predict(df_features, clf1, clf2, reg1, reg2, t_neg1, t_neg2, t_pos2):\n",
    "    prediction_result=pd.DataFrame(columns=['y_pred_proba_clf1',\n",
    "                                           'y_pred_proba_clf2',\n",
    "                                           'y_pred_clf1',\n",
    "                                           'y_pred_clf2',\n",
    "                                            'y_pred_reg2',\n",
    "                                            'y_pred_reg3',\n",
    "                                           'segment',\n",
    "                                           'amount'])\n",
    "\n",
    "    y_pred_proba_clf1 = clf1.predict_proba(df_features)[:, 1]\n",
    "    y_pred_proba_clf2 = clf2.predict_proba(df_features)[:, 1]\n",
    "    y_pred_reg2_logit=reg2.predict(df_features)\n",
    "    y_pred_reg3_logit=reg3.predict(df_features)\n",
    "    \n",
    "    prediction_result.y_pred_proba_clf1=y_pred_proba_clf1\n",
    "    prediction_result.y_pred_proba_clf2=y_pred_proba_clf2\n",
    "    prediction_result.y_pred_reg2 = pd.Series(y_pred_reg2_logit).apply(expit)*50\n",
    "    prediction_result.y_pred_reg3 = pd.Series(y_pred_reg3_logit).apply(expit)*50+50\n",
    "    \n",
    "    prediction_result['y_pred_proba_clf1']=prediction_result['y_pred_proba_clf1'].apply(lambda x: format(float(x),\".8f\")).astype(float)\n",
    "    prediction_result['y_pred_proba_clf2']=prediction_result['y_pred_proba_clf2'].apply(lambda x: format(float(x),\".8f\")).astype(float)\n",
    "    \n",
    "    prediction_result['y_pred_clf1']=prediction_result['y_pred_proba_clf1'].apply(lambda x: '2 or 3' if x <= t_neg_clf1 else '4+')\n",
    "    prediction_result['y_pred_clf2']=prediction_result['y_pred_proba_clf2'].apply(lambda x: '2' if x<=t_neg_clf2 else \n",
    "                                                ('2?' if x<=0.5 else \n",
    "                                                 ('3?' if x<=t_pos_clf2 else '3')))\n",
    "    \n",
    "    prediction_result['segment']=prediction_result.apply(lambda x: x['y_pred_clf1'] if x['y_pred_clf1']=='4+' else x['y_pred_clf2'], axis=1)\n",
    "\n",
    "    prediction_result['amount']=prediction_result.apply(lambda x: \n",
    "                                                        x['y_pred_reg2'] if '2' in x.segment\n",
    "                                                        else (x['y_pred_reg3'] if '3' in x.segment \n",
    "                                                        else np.nan), axis=1)\n",
    "    \n",
    "    prediction_result=prediction_result[['segment','amount']].copy()\n",
    "    \n",
    "    return prediction_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3649c347",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T10:54:38.195482Z",
     "start_time": "2021-05-11T10:54:34.367423Z"
    }
   },
   "outputs": [],
   "source": [
    "results=make_predict(df[all_features],clf1, clf2, reg2_logit, reg3_logit, \n",
    "             classifier_thresholds_clf1['t_neg_clf1'],\n",
    "             classifier_thresholds_clf2['t_neg_clf2'],\n",
    "             classifier_thresholds_clf2['t_pos_clf2'])\n",
    "print(results.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997ea8e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T10:55:40.106378Z",
     "start_time": "2021-05-11T10:55:39.974735Z"
    }
   },
   "outputs": [],
   "source": [
    "results[results.segment.isin(['2', '2?'])].amount.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19538b11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T10:55:52.081084Z",
     "start_time": "2021-05-11T10:55:51.958607Z"
    }
   },
   "outputs": [],
   "source": [
    "results[results.segment.isin(['3','3?'])].amount.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bce2e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "125624270022b3522b0d9fce357b71cad5d467865128e7f38888e7b3f5116099"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
