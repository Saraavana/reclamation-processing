{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "import os, sys; \n",
    "column_path = os.path.dirname(os.path.realpath('C:/Users/sgopalakrish/Downloads/intellizenz-model-training/Neuro-symbolic-AI/column.py'))\n",
    "if sys.path.__contains__(column_path)==False:\n",
    "    sys.path.append(column_path)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pytorch_tabnet.augmentations import ClassificationSMOTE\n",
    "\n",
    "import column\n",
    "import wandb\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==============================================="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"veranst_segment\"\n",
    "# feature_columns = column.features_v5  #143 features\n",
    "feature_columns = column.features_v6  #80 features\n",
    "\n",
    "data_path = column.data_path_2016_2020_v3\n",
    "df = pd.read_parquet(data_path)\n",
    "\n",
    "\n",
    "# class_frequency = df.groupby('veranst_segment')['veranst_segment'].transform('count')\n",
    "# df_sampled = df.sample(n=70000, weights=class_frequency, random_state=2)\n",
    "\n",
    "# df_sampled = df_sampled[feature_columns]\n",
    "df_sampled = df[feature_columns]\n",
    "\n",
    "df_sampled = df_sampled.loc[:,~df_sampled.columns.isin(['vg_inkasso', 'tarif_bez'])] #141 features\n",
    "df_sampled = df_sampled.fillna(-1) # Fill the Empty NaN values in all the cells with -1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "\n",
    "target = \"veranst_segment\"\n",
    "feature_columns = column.features_v7  #77 features # without tarif\n",
    "\n",
    "data_path = column.data_path_2016_2020_v4\n",
    "df = pd.read_parquet(data_path)\n",
    "\n",
    "# class_frequency = df.groupby('veranst_segment')['veranst_segment'].transform('count')\n",
    "# df_sampled = df.sample(n=70000, weights=class_frequency, random_state=2)\n",
    "# df_sampled = df.sample(n=300000, weights=class_frequency, random_state=2)\n",
    "df_sampled = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####with Tarif\n",
    "target = \"veranst_segment\"\n",
    "\n",
    "feature_columns = column.features_v8 #78 features including tarif_bez\n",
    "data_path = column.data_path_2016_2020_v5 # un-encoded tarif-bez\n",
    "df = pd.read_parquet(data_path)\n",
    "\n",
    "# feature_columns = column.features_v2 #140 features # doesn't include tarif_bez\n",
    "# data_path = column.data_path_2016_2020_v3\n",
    "# df = pd.read_parquet(data_path)\n",
    "# all_columns = column.features_v2 + [target]\n",
    "# df = df[all_columns]\n",
    "\n",
    "class_frequency = df.groupby('veranst_segment')['veranst_segment'].transform('count')\n",
    "# df_sampled = df.sample(n=70000, weights=class_frequency, random_state=2)\n",
    "df_sampled = df.sample(n=300000, weights=class_frequency, random_state=2)\n",
    "\n",
    "# df_sampled = df_sampled[feature_columns]\n",
    "# df_sampled = df[feature_columns]\n",
    "# df_sampled = df.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Categorical features for categorical embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nunique = df_sampled.nunique()\n",
    "types = df_sampled.dtypes\n",
    "\n",
    "categorical_columns = []\n",
    "categorical_dims =  {}\n",
    "for col in df_sampled.columns:\n",
    "    if types[col] == 'object' or nunique[col] < 200:\n",
    "        print(col, df_sampled[col].nunique())\n",
    "        l_enc = LabelEncoder()\n",
    "        df_sampled[col] = l_enc.fit_transform(df_sampled[col].values)\n",
    "        categorical_columns.append(col)\n",
    "        categorical_dims[col] = len(l_enc.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_idxs = [ i for i, f in enumerate(feature_columns) if f in categorical_columns]\n",
    "cat_dims = [ categorical_dims[f] for i, f in enumerate(feature_columns) if f in categorical_columns]\n",
    "print(cat_idxs)\n",
    "print(cat_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total = len(df_sampled)\n",
    "\n",
    "# Train, val and test split follows\n",
    "# Rory Mitchell, Andrey Adinets, Thejaswi Rao, and Eibe Frank.\n",
    "# Xgboost: Scalable GPU accelerated learning. arXiv:1806.11248, 2018.\n",
    "\n",
    "# #Train set = 53%, test set = 20%, valid set = 26%\n",
    "# train_val_indices, test_indices = train_test_split(\n",
    "#     range(n_total), test_size=0.2, random_state=0)\n",
    "# train_indices, valid_indices = train_test_split(\n",
    "#     train_val_indices, test_size=0.2 / 0.6, random_state=0) #valid split = 33%(0.2/0.6)\n",
    "    \n",
    "# 0.1, 0.1 / 0.8 - Train set = 78%, test set = 10%, valid set = 11%\n",
    "train_val_indices, test_indices = train_test_split(\n",
    "    range(n_total), test_size=0.1, random_state=0)\n",
    "train_indices, valid_indices = train_test_split(\n",
    "    train_val_indices, test_size=0.1 / 0.8, random_state=0) #valid split = 11%(0.1/0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = TabNetClassifier(\n",
    "#     n_d=64, n_a=64, n_steps=5,\n",
    "#     gamma=1.5, n_independent=2, n_shared=2,\n",
    "#     cat_emb_dim=1,\n",
    "#     lambda_sparse=1e-4, momentum=0.3, clip_value=2.,\n",
    "#     optimizer_fn=torch.optim.Adam,\n",
    "#     optimizer_params=dict(lr=2e-2),\n",
    "#     scheduler_params = {\"gamma\": 0.95,\n",
    "#                      \"step_size\": 20},\n",
    "#     scheduler_fn=torch.optim.lr_scheduler.StepLR, epsilon=1e-15\n",
    "# )\n",
    "\n",
    "clf = TabNetClassifier(\n",
    "    n_d=64, n_a=64, n_steps=5,\n",
    "    gamma=1.5, n_independent=2, n_shared=2,\n",
    "    cat_idxs=cat_idxs,\n",
    "    cat_dims=cat_dims,\n",
    "    cat_emb_dim=2,\n",
    "    lambda_sparse=1e-4, momentum=0.3, clip_value=2.,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    scheduler_params = {\"gamma\": 0.95,\n",
    "                     \"step_size\": 20},\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR, epsilon=1e-15,\n",
    "    mask_type='entmax' # sparsemax\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.getenv(\"CI\", False):\n",
    "# Take only a subsample to run CI\n",
    "    X_train = df_sampled[feature_columns].values[train_indices][:1000,:]\n",
    "    y_train = df_sampled[target].values[train_indices][:1000]\n",
    "else:\n",
    "    X_train = df_sampled[feature_columns].values[train_indices]\n",
    "    y_train = df_sampled[target].values[train_indices]\n",
    "\n",
    "X_valid = df_sampled[feature_columns].values[valid_indices]\n",
    "y_valid = df_sampled[target].values[valid_indices]\n",
    "\n",
    "X_test = df_sampled[feature_columns].values[test_indices]\n",
    "y_test = df_sampled[target].values[test_indices]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ======================================"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Train & Test data frame\n",
    "## Use Features obtained from Leave-one-out-target encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"veranst_segment\"\n",
    "# feature_columns = column.features_v6  #80 features\n",
    "feature_columns = column.features_v7  #77 features\n",
    "\n",
    "df_train = pd.read_parquet(column.train_data_path)\n",
    "df_test = pd.read_parquet(column.test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "df_train['tarif_bez'] = le.fit_transform(df_train['tarif_bez'])\n",
    "df_test['tarif_bez'] = le.fit_transform(df_test['tarif_bez'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train set = 53%, test set = 20%, valid set = 26%\n",
    "df_train, df_valid = train_test_split(\n",
    "    df_train, test_size=0.2 / 0.6, random_state=0) #valid split = 33%(0.2/0.6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input featuers - 78;\n",
    "### Target feature - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not_input_features = ['veranst_segment','vg_inkasso']\n",
    "\n",
    "# X_train = df_train.loc[:,~df_train.columns.isin(not_input_features)].values\n",
    "# y_train = df_train[target].values\n",
    "\n",
    "# X_valid = df_valid.loc[:,~df_valid.columns.isin(not_input_features)].values\n",
    "# y_valid = df_valid[target].values\n",
    "\n",
    "# X_test = df_test.loc[:,~df_test.columns.isin(not_input_features)].values\n",
    "# y_test = df_test[target].values\n",
    "\n",
    "## 77 input features - without tarif\n",
    "X_train = df_train[feature_columns].values\n",
    "y_train = df_train[target].values\n",
    "\n",
    "X_valid = df_valid[feature_columns].values\n",
    "y_valid = df_valid[target].values\n",
    "\n",
    "X_test = df_test[feature_columns].values\n",
    "y_test = df_test[target].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = TabNetClassifier(\n",
    "    n_d=64, n_a=64, n_steps=5,\n",
    "    gamma=1.5, n_independent=2, n_shared=2,\n",
    "    cat_emb_dim=1,\n",
    "    lambda_sparse=1e-4, momentum=0.3, clip_value=2.,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    scheduler_params = {\"gamma\": 0.95,\n",
    "                     \"step_size\": 20},\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR, epsilon=1e-15\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ======================================"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==============================================="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Train & Test data frame\n",
    "## Use Features obtained from Leave-one-out-target encoding\n",
    "## Perform 2 binary classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep only features + 'veranst_segment' columns.\n",
    "\n",
    "Encode segments for Classifier 1:\n",
    "* Variable y_clf1 \n",
    "* Positive class (seg 4+) y_clf1=1\n",
    "* Negative class (seg 2 or 3) y_clf1=0\n",
    "\n",
    "Encode segments for Classifier 2: \n",
    "* Variable y_clf2\n",
    "* Positive class (seg 3) y_clf2=1\n",
    "* Negative class (seg 2) y_clf2=0\n",
    "* Segments >3 are \"encoded\" as np.NaN. These NaN values will be dropped before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = \"veranst_segment\"\n",
    "# feature_columns = column.features_v7  #77 features\n",
    "\n",
    "# df_train = pd.read_parquet(column.train_data_path)\n",
    "# df_test = pd.read_parquet(column.test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Train set = 53%, test set = 20%, valid set = 26%\n",
    "# df_train, df_valid = train_test_split(\n",
    "#     df_train, test_size=0.2 / 0.6, random_state=0) #valid split = 33%(0.2/0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"veranst_segment\"\n",
    "feature_columns = column.features_v7  #77 features # without tarif\n",
    "\n",
    "data_path = column.data_path_2016_2020_v4\n",
    "df = pd.read_parquet(data_path)\n",
    "\n",
    "class_frequency = df.groupby('veranst_segment')['veranst_segment'].transform('count')\n",
    "# df_sampled = df.sample(n=70000, weights=class_frequency, random_state=2)\n",
    "df_sampled = df.sample(n=300000, weights=class_frequency, random_state=2)\n",
    "\n",
    "# df_sampled = df_sampled[feature_columns]\n",
    "# df_sampled = df[feature_columns]\n",
    "# df_sampled = df.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Categorical features for categorical embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nunique = df_sampled.nunique()\n",
    "types = df_sampled.dtypes\n",
    "\n",
    "categorical_columns = []\n",
    "categorical_dims =  {}\n",
    "for col in df_sampled.columns:\n",
    "    if types[col] == 'object' or nunique[col] < 200:\n",
    "        print(col, df_sampled[col].nunique())\n",
    "        l_enc = LabelEncoder()\n",
    "        df_sampled[col] = l_enc.fit_transform(df_sampled[col].values)\n",
    "        categorical_columns.append(col)\n",
    "        categorical_dims[col] = len(l_enc.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_idxs = [ i for i, f in enumerate(feature_columns) if f in categorical_columns]\n",
    "cat_dims = [ categorical_dims[f] for i, f in enumerate(feature_columns) if f in categorical_columns]\n",
    "print(cat_idxs)\n",
    "print(cat_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled.loc[:,'y_clf1']=(df_sampled.veranst_segment.astype(int) > 1).values.astype(int)\n",
    "df_sampled.loc[:,'y_clf2']=df_sampled.veranst_segment.apply(lambda x: 1 if x==1 else (0 if x==0 else np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total = len(df_sampled)\n",
    "\n",
    "# Train, val and test split follows\n",
    "# Rory Mitchell, Andrey Adinets, Thejaswi Rao, and Eibe Frank.\n",
    "# Xgboost: Scalable GPU accelerated learning. arXiv:1806.11248, 2018.\n",
    "\n",
    "# #Train set = 53%, test set = 20%, valid set = 26%\n",
    "# train_val_indices, test_indices = train_test_split(\n",
    "#     range(n_total), test_size=0.2, random_state=0)\n",
    "# train_indices, valid_indices = train_test_split(\n",
    "#     train_val_indices, test_size=0.2 / 0.6, random_state=0) #valid split = 33%(0.2/0.6)\n",
    "\n",
    "# 0.1, 0.1 / 0.8 - Train set = 78%, test set = 10%, valid set = 11%\n",
    "train_val_indices, test_indices = train_test_split(\n",
    "    range(n_total), test_size=0.1, random_state=0)\n",
    "train_indices, valid_indices = train_test_split(\n",
    "    train_val_indices, test_size=0.1 / 0.8, random_state=0) #valid split = 11%(0.1/0.8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Losses and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accuracy(model):\n",
    "    # plot losses\n",
    "    plt.plot(model.history['loss'])\n",
    "\n",
    "    # plot accuracy\n",
    "    plt.plot(model.history['train_auc'])\n",
    "    plt.plot(model.history['valid_auc'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(model, input, target):\n",
    "        # To get final results you may need to use a mapping for classes \n",
    "        # as you are allowed to use targets like [\"yes\", \"no\", \"maybe\", \"I don't know\"]\n",
    "\n",
    "        dataset_name = 'Intellizenz'\n",
    "        # preds_mapper = { idx : class_name for idx, class_name in enumerate(model.classes_)}\n",
    "\n",
    "        preds = model.predict_proba(input)\n",
    "\n",
    "        # y_pred = np.vectorize(preds_mapper.get)(np.argmax(preds, axis=1))\n",
    "        y_pred = (np.argmax(preds, axis=1))\n",
    "\n",
    "        test_acc = accuracy_score(y_pred=y_pred, y_true=target)\n",
    "\n",
    "        # print(f\"BEST VALID SCORE FOR {dataset_name} : {model.best_cost}\")\n",
    "        print(f\"FINAL TEST SCORE FOR {dataset_name} : {test_acc}\")\n",
    "\n",
    "        # or you can simply use the predict method\n",
    "        y_pred = model.predict(input)\n",
    "        test_acc = accuracy_score(y_pred=y_pred, y_true=target)\n",
    "        print(f\"FINAL TEST SCORE FOR {dataset_name} : {test_acc}\")\n",
    "\n",
    "        return preds, y_pred, test_acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    # save state dict\n",
    "    # saving_path_name = \"./baseline_tabnet_model_test_1\"\n",
    "    saved_filename = model.save_model(path)\n",
    "\n",
    "def load_model(path):\n",
    "    # define new model and load save parameters\n",
    "    loaded_clf_model = TabNetClassifier()\n",
    "    loaded_clf_model.load_model(path)\n",
    "    return loaded_clf_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wandb Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wand_log(model, pred_probas, y_test, y_pred, test_accuracy):\n",
    "    wandb.init(project=\"Intellizenz\", entity=\"elsaravana\")\n",
    "    wandb.config = {\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"epochs\": 200,\n",
    "        \"batch_size\": 16384\n",
    "    }\n",
    "\n",
    "    wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
    "                        preds=y_pred, y_true=y_test,\n",
    "                        class_names=[0, 1])})\n",
    "    wandb.log({\"pr\" : wandb.plot.pr_curve(y_true=y_test, y_probas=pred_probas,\n",
    "                labels=['Segment 0-50€ or 50-100€', 'Segment >100€'], classes_to_plot=[0, 1])})\n",
    "    wandb.log({\"roc\" : wandb.plot.roc_curve(y_true=y_test, y_probas=pred_probas,\n",
    "                    labels=['Segment 0-50€ or 50-100€', 'Segment >100€'], classes_to_plot=[0, 1])})\n",
    "\n",
    "    train_loss = model.history['loss']\n",
    "    # train_accuracy = model.history['train_accuracy']\n",
    "    # validation_accuracy = model.history['valid_accuracy']\n",
    "\n",
    "    train_accuracy = model.history['train_auc']\n",
    "    validation_accuracy = model.history['valid_auc']\n",
    "    \n",
    "    for i,loss in enumerate(train_loss):\n",
    "        wandb.log({\"train_loss\": loss, \n",
    "            # \"train_accuracy\": train_accuracy[i],\n",
    "            # \"validation_accuracy\": validation_accuracy[i]})\n",
    "            \"train_auc\": train_accuracy[i],\n",
    "            \"validation_auc\": validation_accuracy[i]})\n",
    "\n",
    "    wandb.log({\"test_accuracy\": test_accuracy})\n",
    "    # exit_code 0, to finish a successful run\n",
    "    wandb.finish(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 77 input features - without tarif\n",
    "# X_train_clf1 = df_train[feature_columns].values\n",
    "# y_train_clf1 = df_train.y_clf1\n",
    "\n",
    "# X_valid_clf1 = df_valid[feature_columns].values\n",
    "# y_valid_clf1 = df_valid.y_clf1\n",
    "\n",
    "# X_test_clf1 = df_test[feature_columns].values\n",
    "# y_test_clf1 = df_test.y_clf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 77 input features - without tarif\n",
    "X_train_clf1 = df_sampled[feature_columns].values[train_indices]\n",
    "y_train_clf1 = df_sampled['y_clf1'].values[train_indices]\n",
    "\n",
    "X_valid_clf1 = df_sampled[feature_columns].values[valid_indices]\n",
    "y_valid_clf1 = df_sampled['y_clf1'].values[valid_indices]\n",
    "\n",
    "X_test_clf1 = df_sampled[feature_columns].values[test_indices]\n",
    "y_test_clf1 = df_sampled['y_clf1'].values[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_clf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Network parameters\n",
    "clf1 = TabNetClassifier(\n",
    "    n_d=64, n_a=64, n_steps=5,\n",
    "    gamma=1.5, n_independent=2, n_shared=2,\n",
    "    cat_emb_dim=1,\n",
    "    lambda_sparse=1e-4, momentum=0.3, clip_value=2.,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    scheduler_params = {\"gamma\": 0.95,\n",
    "                     \"step_size\": 20},\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR, epsilon=1e-15\n",
    ")\n",
    "\n",
    "# clf1 = TabNetClassifier(\n",
    "#     n_d=64, n_a=64, n_steps=5,\n",
    "#     gamma=1.5, n_independent=2, n_shared=2,\n",
    "#     cat_idxs=cat_idxs,\n",
    "#     cat_dims=cat_dims,\n",
    "#     cat_emb_dim=2,\n",
    "#     lambda_sparse=1e-4, momentum=0.3, clip_value=2.,\n",
    "#     optimizer_fn=torch.optim.Adam,\n",
    "#     optimizer_params=dict(lr=2e-2),\n",
    "#     scheduler_params = {\"gamma\": 0.95,\n",
    "#                      \"step_size\": 20},\n",
    "#     scheduler_fn=torch.optim.lr_scheduler.StepLR, epsilon=1e-15,\n",
    "#     mask_type='entmax' # sparsemax\n",
    "# )\n",
    "\n",
    "max_epochs = 200 if not os.getenv(\"CI\", False) else 2\n",
    "\n",
    "aug = ClassificationSMOTE(p=0.2)\n",
    "#SMOTE - Synthetic Minority Oversampling Technique\n",
    "clf1.fit(\n",
    "    X_train=X_train_clf1, y_train=y_train_clf1,\n",
    "    eval_set=[(X_valid_clf1, y_valid_clf1), (X_valid_clf1, y_valid_clf1)],\n",
    "    eval_name=['train', 'valid'],\n",
    "    max_epochs=max_epochs, patience=100,\n",
    "    batch_size=16384, virtual_batch_size=256, \n",
    "    #batch_size can be 1-10% of whole training dataset size\n",
    "    #Training - 1.3M, 1% - 13,000, 10% - 130,000 \n",
    "    augmentations=aug\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification 1 save model, plot, visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(clf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probas, y_pred, test_acc = test_prediction(clf1, X_test_clf1, y_test_clf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wand_log(model=clf1,pred_probas=pred_probas, y_test= y_test_clf1, y_pred=y_pred, test_accuracy=test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(clf1,path=\"./baseline_tabnet_model_test_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_1 = load_model(path=\"./baseline_tabnet_model_test_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(clf1)\n",
    "pred_probas, y_pred, test_acc = test_prediction(clf1, X_test_clf1, y_test_clf1)\n",
    "\n",
    "wand_log(model=clf1,pred_probas=pred_probas, y_test= y_test_clf1, y_pred=y_pred, test_accuracy=test_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For Classifier2 (clf2) we use data from segments 2 and 3 (so we drop records where y_clf2 is NaN)\n",
    "# df_train_clf2 = df_train.dropna(subset=['y_clf2'])\n",
    "# df_valid_clf2 = df_valid.dropna(subset=['y_clf2'])\n",
    "# df_test_clf2 = df_test.dropna(subset=['y_clf2'])\n",
    "\n",
    "\n",
    "# ## 77 input features - without tarif\n",
    "# X_train_clf2 = df_train_clf2[feature_columns].values\n",
    "# y_train_clf2 = df_train_clf2.y_clf2\n",
    "\n",
    "# X_valid_clf2 = df_valid_clf2[feature_columns].values\n",
    "# y_valid_clf2 = df_valid_clf2.y_clf2\n",
    "\n",
    "# X_test_clf2 = df_test_clf2[feature_columns].values\n",
    "# y_test_clf2 = df_test_clf2.y_clf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Classifier2 (clf2) we use data from segments 2 and 3 (so we drop records where y_clf2 is NaN)\n",
    "df_sampled_clf2 = df_sampled.dropna(subset=['y_clf2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nunique_clf2 = df_sampled_clf2.nunique()\n",
    "types_clf2 = df_sampled_clf2.dtypes\n",
    "\n",
    "categorical_columns_clf2 = []\n",
    "categorical_dims_clf2 =  {}\n",
    "for col in df_sampled_clf2.columns:\n",
    "    if types_clf2[col] == 'object' or nunique_clf2[col] < 200:\n",
    "        print(col, df_sampled_clf2[col].nunique())\n",
    "        l_enc = LabelEncoder()\n",
    "        df_sampled_clf2[col] = l_enc.fit_transform(df_sampled_clf2[col].values)\n",
    "        categorical_columns_clf2.append(col)\n",
    "        categorical_dims_clf2[col] = len(l_enc.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_idxs_clf2 = [ i for i, f in enumerate(feature_columns) if f in categorical_columns_clf2]\n",
    "cat_dims_clf2 = [ categorical_dims_clf2[f] for i, f in enumerate(feature_columns) if f in categorical_columns_clf2]\n",
    "print(cat_idxs_clf2)\n",
    "print(cat_dims_clf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total_clf2 = len(df_sampled_clf2)\n",
    "\n",
    "# Train, val and test split follows\n",
    "# Rory Mitchell, Andrey Adinets, Thejaswi Rao, and Eibe Frank.\n",
    "# Xgboost: Scalable GPU accelerated learning. arXiv:1806.11248, 2018.\n",
    "\n",
    "# #Train set = 53%, test set = 20%, valid set = 26%\n",
    "# train_val_indices_clf2, test_indices_clf2 = train_test_split(\n",
    "#     range(n_total_clf2), test_size=0.2, random_state=0)\n",
    "# train_indices_clf2, valid_indices_clf2 = train_test_split(\n",
    "#     train_val_indices_clf2, test_size=0.2 / 0.6, random_state=0) #valid split = 33%(0.2/0.6)\n",
    "\n",
    "# 0.1, 0.1 / 0.8 - Train set = 78%, test set = 10%, valid set = 11%\n",
    "train_val_indices_clf2, test_indices_clf2 = train_test_split(\n",
    "    range(n_total_clf2), test_size=0.1, random_state=0)\n",
    "train_indices_clf2, valid_indices_clf2 = train_test_split(\n",
    "    train_val_indices_clf2, test_size=0.1 / 0.8, random_state=0) #valid split = 11%(0.1/0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clf2 = df_sampled_clf2[feature_columns].values[train_indices_clf2]\n",
    "y_train_clf2 = df_sampled_clf2['y_clf2'].values[train_indices_clf2]\n",
    "\n",
    "X_valid_clf2 = df_sampled_clf2[feature_columns].values[valid_indices_clf2]\n",
    "y_valid_clf2 = df_sampled_clf2['y_clf2'].values[valid_indices_clf2]\n",
    "\n",
    "X_test_clf2 = df_sampled_clf2[feature_columns].values[test_indices_clf2]\n",
    "y_test_clf2 = df_sampled_clf2['y_clf2'].values[test_indices_clf2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Network parameters\n",
    "# clf2 = TabNetClassifier(\n",
    "#     n_d=64, n_a=64, n_steps=5,\n",
    "#     gamma=1.5, n_independent=2, n_shared=2,\n",
    "#     cat_emb_dim=1,\n",
    "#     lambda_sparse=1e-4, momentum=0.3, clip_value=2.,\n",
    "#     optimizer_fn=torch.optim.Adam,\n",
    "#     optimizer_params=dict(lr=2e-2),\n",
    "#     scheduler_params = {\"gamma\": 0.95,\n",
    "#                      \"step_size\": 20},\n",
    "#     scheduler_fn=torch.optim.lr_scheduler.StepLR, epsilon=1e-15\n",
    "# )\n",
    "\n",
    "clf2 = TabNetClassifier(\n",
    "     n_d=64, n_a=64, n_steps=5,\n",
    "     gamma=1.5, n_independent=2, n_shared=2,\n",
    "     cat_idxs=cat_idxs_clf2,\n",
    "     cat_dims=cat_dims_clf2,\n",
    "     cat_emb_dim=2,\n",
    "     lambda_sparse=1e-4, momentum=0.3, clip_value=2.,\n",
    "     optimizer_fn=torch.optim.Adam,\n",
    "     optimizer_params=dict(lr=2e-2),\n",
    "     scheduler_params = {\"gamma\": 0.95,\n",
    "                      \"step_size\": 20},\n",
    "     scheduler_fn=torch.optim.lr_scheduler.StepLR, epsilon=1e-15,\n",
    "     mask_type='entmax' # sparsemax\n",
    ")\n",
    "\n",
    "max_epochs = 200 if not os.getenv(\"CI\", False) else 2\n",
    "\n",
    "aug = ClassificationSMOTE(p=0.2)\n",
    "#SMOTE - Synthetic Minority Oversampling Technique\n",
    "clf2.fit(\n",
    "    X_train=X_train_clf2, y_train=y_train_clf2,\n",
    "    eval_set=[(X_valid_clf2, y_valid_clf2), (X_valid_clf2, y_valid_clf2)],\n",
    "    eval_name=['train', 'valid'],\n",
    "    max_epochs=max_epochs, patience=100,\n",
    "    batch_size=16384, virtual_batch_size=256,\n",
    "    augmentations=aug\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probas_clf2, y_pred_clf2, test_acc_clf2 = test_prediction(clf2, X_test_clf2, y_test_clf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wand_log(model=clf2,pred_probas=pred_probas_clf2, y_test= y_test_clf2, y_pred=y_pred_clf2, test_accuracy=test_acc_clf2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification 2 save model, plot, visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(clf2,path=\"./baseline_tabnet_model_test_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = load_model(path=\"./baseline_tabnet_model_test_1.zip\")\n",
    "\n",
    "# C:/Users/sgopalakrish/Downloads/intellizenz-model-training/Neuro-symbolic-AI/SLASH/TabNet/baseline_tabnet_model_test_1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_loss_accuracy(clf2)\n",
    "pred_probas, y_pred, test_acc = test_prediction(clf2, X_test_clf2, y_test_clf2)\n",
    "\n",
    "# wand_log(model=clf2,pred_probas=pred_probas, y_test= y_test_clf2, y_pred=y_pred, test_accuracy=test_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==============================================="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ============================================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 200 if not os.getenv(\"CI\", False) else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.Tensor([-5])\n",
    "print((-x1).pow(2))\n",
    "print(-(x1).pow(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = ClassificationSMOTE(p=0.2)\n",
    "#SMOTE - Synthetic Minority Oversampling Technique\n",
    "clf.fit(\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "    eval_name=['train', 'valid'],\n",
    "    # max_epochs=max_epochs, patience=100,\n",
    "    max_epochs=max_epochs, patience=100,\n",
    "    batch_size=16384, virtual_batch_size=256,\n",
    "    augmentations=aug\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(clf.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy\n",
    "plt.plot(clf.history['train_accuracy'])\n",
    "plt.plot(clf.history['valid_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'Intellizenz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get final results you may need to use a mapping for classes \n",
    "# as you are allowed to use targets like [\"yes\", \"no\", \"maybe\", \"I don't know\"]\n",
    "\n",
    "preds_mapper = { idx : class_name for idx, class_name in enumerate(clf.classes_)}\n",
    "\n",
    "preds = clf.predict_proba(X_test)\n",
    "\n",
    "y_pred = np.vectorize(preds_mapper.get)(np.argmax(preds, axis=1))\n",
    "\n",
    "test_acc = accuracy_score(y_pred=y_pred, y_true=y_test)\n",
    "\n",
    "print(f\"BEST VALID SCORE FOR {dataset_name} : {clf.best_cost}\")\n",
    "print(f\"FINAL TEST SCORE FOR {dataset_name} : {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or you can simply use the predict method\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "test_acc = accuracy_score(y_pred=y_pred, y_true=y_test)\n",
    "print(f\"FINAL TEST SCORE FOR {dataset_name} : {test_acc}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, labels=[0, 1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classification_report(d_true, d_pred, labels=[0, 1, 2]))\n",
    "## Classification report of tabnet model training on 300k data with 78 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save state dict\n",
    "# saving_path_name = \"./baseline_tabnet_model_test_1\"\n",
    "\n",
    "saving_path_name = \"./baseline_tabnet_model_clf_d300k_140feat_lr_0.02_ep200\"\n",
    "saved_filename = clf.save_model(saving_path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_path_name = 'baseline_tabnet_model_clf_d300k_78feat_lr_0.02_ep200'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define new model and load save parameters\n",
    "loaded_clf = TabNetClassifier()\n",
    "loaded_clf.load_model(saved_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = loaded_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_preds = loaded_clf.predict_proba(X_test)\n",
    "loaded_y_pred = np.vectorize(preds_mapper.get)(np.argmax(loaded_preds, axis=1))\n",
    "\n",
    "loaded_test_acc = accuracy_score(y_pred=loaded_y_pred, y_true=y_test)\n",
    "\n",
    "print(f\"FINAL TEST SCORE FOR {dataset_name} : {loaded_test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"Intellizenz\", entity=\"elsaravana\")\n",
    "wandb.config = {\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"epochs\": 200,\n",
    "    \"batch_size\": 16384\n",
    "}\n",
    "\n",
    "wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
    "                    preds=loaded_y_pred, y_true=y_test,\n",
    "                    class_names=[0, 1, 2])})\n",
    "wandb.log({\"pr\" : wandb.plot.pr_curve(y_true=y_test, y_probas=loaded_preds,\n",
    "             labels=['Segment 0-50€', 'Segment 50-100€', 'Segment >100€'], classes_to_plot=[0, 1, 2])})\n",
    "wandb.log({\"roc\" : wandb.plot.roc_curve(y_true=y_test, y_probas=loaded_preds,\n",
    "                labels=['Segment 0-50€', 'Segment 50-100€', 'Segment >100€'], classes_to_plot=[0, 1, 2])})\n",
    "\n",
    "train_loss = clf.history['loss']\n",
    "train_accuracy = clf.history['train_accuracy']\n",
    "validation_accuracy = clf.history['valid_accuracy']\n",
    "for i,loss in enumerate(train_loss):\n",
    "    wandb.log({\"train_loss\": loss, \n",
    "        \"train_accuracy\": train_accuracy[i],\n",
    "        \"validation_accuracy\": validation_accuracy[i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.summary[\"test_accuracy\"] = loaded_test_acc\n",
    "wandb.log({\"test_accuracy\": loaded_test_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(test_acc == loaded_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exit_code 0, to finish a successful run\n",
    "wandb.finish(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global explainability: feature importance summing to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local explainability and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_matrix, masks = clf.explain(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 5, figsize=(20,20))\n",
    "\n",
    "for i in range(5):\n",
    "    axs[i].imshow(masks[i][:50])\n",
    "    axs[i].set_title(f\"mask {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot PR curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import precision_recall_curve,roc_curve\n",
    "\n",
    "n_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the True labels and Prediction to One hot encoded representation such as: if true - 0, then [1 0 0], \n",
    "# if true - 1, then [0 1 0]\n",
    "y_true_binarize = label_binarize(y_test, classes=[*range(n_classes)])\n",
    "y_pred_binarize = label_binarize(y_pred, classes=[*range(n_classes)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision recall curve\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "for i in range(n_classes):\n",
    "    label = ''\n",
    "    if i == 0:\n",
    "        label = 'Class 0(0-50€)'\n",
    "    elif i == 1:\n",
    "        label = 'Class 1(50-100€)'\n",
    "    else:\n",
    "        label = 'Class 2(>100€)'\n",
    "\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_true_binarize[:, i],\n",
    "                                                        y_pred_binarize[:, i])\n",
    "    plt.plot(recall[i], precision[i], lw=2, label=label)\n",
    "    \n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend(loc=\"best\")\n",
    "# plt.title(\"Precision vs recall curve\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "for i in range(n_classes):\n",
    "    label = ''\n",
    "    if i == 0:\n",
    "        label = 'Class 0(0-50€)'\n",
    "    elif i == 1:\n",
    "        label = 'Class 1(50-100€)'\n",
    "    else:\n",
    "        label = 'Class 2(>100€)'\n",
    "\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true_binarize[:, i],\n",
    "                                  y_pred_binarize[:, i])\n",
    "    plt.plot(fpr[i], tpr[i], lw=2, label=label)\n",
    "    \n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.legend(loc=\"best\")\n",
    "# plt.title(\"Precision vs recall curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix\n",
    "\n",
    "# mul_c = multilabel_confusion_matrix(y_true_binarize, y_pred_binarize, labels=['Class 0(0-50€)', 'Class 1(50-100€)', 'Class 2(>100€)'])\n",
    "# mul_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_df = pd.DataFrame(cm, index = ['Class 0(0-50€)', 'Class 1(50-100€)', 'Class 2(>100€)'], columns= ['Class 0(0-50€)', 'Class 1(50-100€)', 'Class 2(>100€)'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm_df, annot=True)\n",
    "plt.xlabel('Actual values')\n",
    "plt.ylabel('Predicted values')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLASH Plots\n",
    "### Tabnet + SLASH - 140 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report of SLASH +TabNet model trained on 300,000 data with 140 features\n",
    "conf_mat_df = pd.read_csv('140_features_slash_tabnet_conf_matr.csv')\n",
    "\n",
    "actual = conf_mat_df['Actual']\n",
    "pred = conf_mat_df['Predicted']\n",
    "count = conf_mat_df['nPredictions']\n",
    "\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "for id, label in enumerate(actual):\n",
    "    each_true_lbl = label\n",
    "    each_pred_lbl = pred[id]\n",
    "    for i in range(count[id]):\n",
    "        true_labels.append(each_true_lbl)\n",
    "        pred_labels.append(each_pred_lbl)\n",
    "\n",
    "print(classification_report(true_labels, pred_labels, labels=[0, 1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.27      0.34        95\n",
      "           1       0.34      0.77      0.47        92\n",
      "           2       0.55      0.06      0.12        93\n",
      "\n",
      "    accuracy                           0.37       280\n",
      "   macro avg       0.44      0.37      0.31       280\n",
      "weighted avg       0.44      0.37      0.31       280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report of SLASH +TabNet model trained on 300,000 data with 78 features\n",
    "# conf_mat_df = pd.read_csv('78_features_slash_tabnet_conf_matr.csv')\n",
    "\n",
    "# Classification report of SLASH +TabNet model trained on normalized 78 features\n",
    "# conf_mat_df = pd.read_csv('78_norm_features_slash_tabnet_d5k_ep30_conf_matr.csv')\n",
    "conf_mat_df = pd.read_csv('78_norm_features_slash_tabnet_d1_5k_ep30_conf_matr.csv')\n",
    "\n",
    "actual = conf_mat_df['Actual']\n",
    "pred = conf_mat_df['Predicted']\n",
    "count = conf_mat_df['nPredictions']\n",
    "\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "for id, label in enumerate(actual):\n",
    "    each_true_lbl = label\n",
    "    each_pred_lbl = pred[id]\n",
    "    for i in range(count[id]):\n",
    "        true_labels.append(each_true_lbl)\n",
    "        pred_labels.append(each_pred_lbl)\n",
    "\n",
    "print(classification_report(true_labels, pred_labels, labels=[0, 1, 2]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeurASP Plots\n",
    "### 2 Hidden Layer MLP + NeurASP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.78      0.78     20258\n",
      "           1       0.69      0.63      0.66     19959\n",
      "           2       0.77      0.85      0.81     19783\n",
      "\n",
      "    accuracy                           0.75     60000\n",
      "   macro avg       0.75      0.75      0.75     60000\n",
      "weighted avg       0.75      0.75      0.75     60000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report of 2 Hidden Layer + NeurASP model trained on 300,000 data with 78 features\n",
    "# conf_mat_df = pd.read_csv('78_features_neurasp_2hl_mlp_conf_matr.csv')\n",
    "conf_mat_df = pd.read_csv('78_norm_features_neurasp_2hl_mlp_event_tarif_rule_conf_matr.csv')\n",
    "\n",
    "\n",
    "actual = conf_mat_df['Actual']\n",
    "pred = conf_mat_df['Predicted']\n",
    "count = conf_mat_df['nPredictions']\n",
    "\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "for id, label in enumerate(actual):\n",
    "    each_true_lbl = label\n",
    "    each_pred_lbl = pred[id]\n",
    "    for i in range(count[id]):\n",
    "        true_labels.append(each_true_lbl)\n",
    "        pred_labels.append(each_pred_lbl)\n",
    "\n",
    "print(classification_report(true_labels, pred_labels, labels=[0, 1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report of 2 Hidden Layer + NeurASP model trained on 300,000 data with 140 features\n",
    "conf_mat_df = pd.read_csv('140_features_neurasp_2hl_mlp_conf_matr.csv')\n",
    "\n",
    "actual = conf_mat_df['Actual']\n",
    "pred = conf_mat_df['Predicted']\n",
    "count = conf_mat_df['nPredictions']\n",
    "\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "for id, label in enumerate(actual):\n",
    "    each_true_lbl = label\n",
    "    each_pred_lbl = pred[id]\n",
    "    for i in range(count[id]):\n",
    "        true_labels.append(each_true_lbl)\n",
    "        pred_labels.append(each_pred_lbl)\n",
    "\n",
    "print(classification_report(true_labels, pred_labels, labels=[0, 1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intellizenz-model-training-zEgBkwlM-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "abd75e625403f7dee3f9702b96e52af8e2fb071588893a9b089d1a7fbc771407"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
