{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models_node import *\n",
    "from utils.datautils import *\n",
    "from utils.train_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training ---Intellizenz ---dataset\n",
      "\n",
      "\n",
      "X Train Data shape:\n",
      " (195254, 152)\n",
      "\n",
      "X Test Data shape:\n",
      " (151287, 152)\n",
      "\n",
      "Y Train Data shape:\n",
      " (195254,)\n",
      "\n",
      "Y Test Data shape:\n",
      " (151287,)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument tensors in method wrapper_cat)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Saravana\\Projects\\Intellizenz\\intellizenz-model-training\\Neuro-symbolic-AI\\NSTSC\\Codes\\evaluation.ipynb Zelle 2\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Saravana/Projects/Intellizenz/intellizenz-model-training/Neuro-symbolic-AI/NSTSC/Codes/evaluation.ipynb#W1sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m Xtrain, Xval, Xtest \u001b[39m=\u001b[39m Multi_view(Xtrain_raw, Xval_raw, Xtest_raw)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Saravana/Projects/Intellizenz/intellizenz-model-training/Neuro-symbolic-AI/NSTSC/Codes/evaluation.ipynb#W1sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m N, T \u001b[39m=\u001b[39m calculate_dataset_metrics(Xtrain)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Saravana/Projects/Intellizenz/intellizenz-model-training/Neuro-symbolic-AI/NSTSC/Codes/evaluation.ipynb#W1sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m Tree \u001b[39m=\u001b[39m Train_model(Xtrain, Xval, ytrain_raw, yval_raw, epochs \u001b[39m=\u001b[39;49m Max_epoch,\\\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Saravana/Projects/Intellizenz/intellizenz-model-training/Neuro-symbolic-AI/NSTSC/Codes/evaluation.ipynb#W1sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m                     normalize_timeseries \u001b[39m=\u001b[39;49m normalize_dataset)\n",
      "File \u001b[1;32mc:\\Saravana\\Projects\\Intellizenz\\intellizenz-model-training\\Neuro-symbolic-AI\\NSTSC\\Codes\\utils\\train_utils.py:37\u001b[0m, in \u001b[0;36mTrain_model\u001b[1;34m(Xtrain_raw, Xval_raw, ytrain_raw, yval_raw, epochs, normalize_timeseries, lr)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mTrain_model\u001b[39m(Xtrain_raw, Xval_raw, ytrain_raw, yval_raw, \\\n\u001b[0;32m     35\u001b[0m                 epochs \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m, normalize_timeseries \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, lr \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m):\n\u001b[0;32m     36\u001b[0m     classnum \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39mmax(ytrain_raw) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m     Tree \u001b[39m=\u001b[39m Build_tree(Xtrain_raw, Xval_raw, ytrain_raw, yval_raw, epochs, classnum, \\\n\u001b[0;32m     38\u001b[0m                    learnrate \u001b[39m=\u001b[39;49m lr, savepath \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mC:/Saravana/Projects/Intellizenz/intellizenz-model-training/Neuro-symbolic-AI/NSTSC/Codes/utils/\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     39\u001b[0m     Tree \u001b[39m=\u001b[39m Prune_tree(Tree, Xval_raw, yval_raw)\n\u001b[0;32m     41\u001b[0m     \u001b[39mreturn\u001b[39;00m Tree\n",
      "File \u001b[1;32mc:\\Saravana\\Projects\\Intellizenz\\intellizenz-model-training\\Neuro-symbolic-AI\\NSTSC\\Codes\\utils\\train_utils.py:59\u001b[0m, in \u001b[0;36mBuild_tree\u001b[1;34m(Xtrain, Xval, ytrain_raw, yval_raw, Epoch, classnum, learnrate, savepath)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mwhile\u001b[39;00m pronodenum \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m maxnodenum:\n\u001b[0;32m     57\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m Tree[pronodenum]\u001b[39m.\u001b[39mstoptrain:\n\u001b[0;32m     58\u001b[0m         Tree, trueidx, falseidx, trueidxt,\\\n\u001b[1;32m---> 59\u001b[0m         falseidxt, \u001b[39m=\u001b[39m Trainnode(Tree, pronodenum, Epoch, learnrate,\\\n\u001b[0;32m     60\u001b[0m         Xtrain, ytrain_raw, Modelnum, savepath, classnum,\\\n\u001b[0;32m     61\u001b[0m             Xval, yval_raw)\n\u001b[0;32m     63\u001b[0m         \u001b[39mif\u001b[39;00m maxnodenum \u001b[39m<\u001b[39m \u001b[39m128\u001b[39m:\n\u001b[0;32m     64\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(Tree[pronodenum]\u001b[39m.\u001b[39mtrueidx) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Saravana\\Projects\\Intellizenz\\intellizenz-model-training\\Neuro-symbolic-AI\\NSTSC\\Codes\\utils\\train_utils.py:129\u001b[0m, in \u001b[0;36mTrainnode\u001b[1;34m(Nodes, pronum, Epoch, lrt, X, y, Mdlnum, mdlpath, clsnum, Xt, yt)\u001b[0m\n\u001b[0;32m    127\u001b[0m w_batch \u001b[39m=\u001b[39m IR \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m\u001b[39m-\u001b[39my_batch) \n\u001b[0;32m    128\u001b[0m w_batch[w_batch\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 129\u001b[0m X_rns[Ci] \u001b[39m=\u001b[39m tlnns[Ci](X_batch[:,:T], X_batch[:,T:\u001b[39m2\u001b[39;49m\u001b[39m*\u001b[39;49mT],\\\n\u001b[0;32m    130\u001b[0m                       X_batch[:,\u001b[39m2\u001b[39;49m\u001b[39m*\u001b[39;49mT:])\n\u001b[0;32m    131\u001b[0m Losses[Ci] \u001b[39m=\u001b[39m  torch\u001b[39m.\u001b[39msum(w_batch \u001b[39m*\u001b[39m (\u001b[39m-\u001b[39my_batch \u001b[39m*\u001b[39m \\\n\u001b[0;32m    132\u001b[0m               torch\u001b[39m.\u001b[39mlog(X_rns[Ci] \u001b[39m+\u001b[39m \u001b[39m1e-9\u001b[39m) \u001b[39m-\u001b[39m (\u001b[39m1\u001b[39m\u001b[39m-\u001b[39my_batch) \u001b[39m*\u001b[39m \\\n\u001b[0;32m    133\u001b[0m               torch\u001b[39m.\u001b[39mlog(\u001b[39m1\u001b[39m\u001b[39m-\u001b[39mX_rns[Ci] \u001b[39m+\u001b[39m \u001b[39m1e-9\u001b[39m)))\n\u001b[0;32m    135\u001b[0m optimizers[Ci]\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Saravana\\Projects\\Intellizenz\\intellizenz-model-training\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Saravana\\Projects\\Intellizenz\\intellizenz-model-training\\Neuro-symbolic-AI\\NSTSC\\Codes\\Models_node.py:270\u001b[0m, in \u001b[0;36mTL_NN5.forward\u001b[1;34m(self, x1, x2, x3)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x1, x2, x3):\n\u001b[0;32m    269\u001b[0m     \u001b[39m# first layer for eventually\u001b[39;00m\n\u001b[1;32m--> 270\u001b[0m      x1 \u001b[39m=\u001b[39m Preprocess(x1, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mT)\n\u001b[0;32m    271\u001b[0m      r_a1_1 \u001b[39m=\u001b[39m  (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb1 \u001b[39m-\u001b[39m x1 \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt1)\n\u001b[0;32m    272\u001b[0m      b1pos \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mwhere(r_a1_1 \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb1)\n",
      "File \u001b[1;32mc:\\Saravana\\Projects\\Intellizenz\\intellizenz-model-training\\Neuro-symbolic-AI\\NSTSC\\Codes\\Models_node.py:429\u001b[0m, in \u001b[0;36mPreprocess\u001b[1;34m(x, T1, T2)\u001b[0m\n\u001b[0;32m    427\u001b[0m xnew \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros([x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], T1, T2])\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m    428\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(T1):\n\u001b[1;32m--> 429\u001b[0m     xnew[:,i,:] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat((x[:, i:], torch\u001b[39m.\u001b[39;49mzeros([x\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m],i])\u001b[39m.\u001b[39;49mto(device)),\u001b[39m1\u001b[39;49m)\n\u001b[0;32m    430\u001b[0m \u001b[39mreturn\u001b[39;00m xnew\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument tensors in method wrapper_cat)"
     ]
    }
   ],
   "source": [
    "Dataset_name = \"Intellizenz\"\n",
    "print('Start Training ---' + str(Dataset_name) + ' ---dataset\\n')\n",
    "dataset_path_ = 'C:/Saravana/Projects/Intellizenz/intellizenz-model-training/data'\n",
    "\n",
    "normalize_dataset = True\n",
    "Max_epoch = 100\n",
    "# model training\n",
    "Xtrain_raw, ytrain_raw, Xval_raw, yval_raw, Xtest_raw, ytest_raw \\\n",
    "    = Readdataset(dataset_path_, Dataset_name)\n",
    "Xtrain, Xval, Xtest = Multi_view(Xtrain_raw, Xval_raw, Xtest_raw)\n",
    "N, T = calculate_dataset_metrics(Xtrain)\n",
    "Tree = Train_model(Xtrain, Xval, ytrain_raw, yval_raw, epochs = Max_epoch,\\\n",
    "                    normalize_timeseries = normalize_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model testing\n",
    "testaccu = Evaluate_model(Tree, Xtest, ytest_raw)\n",
    "print(\"Test accuracy for dataset {} is --- {}\".format(Dataset_name, testaccu))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "125624270022b3522b0d9fce357b71cad5d467865128e7f38888e7b3f5116099"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
